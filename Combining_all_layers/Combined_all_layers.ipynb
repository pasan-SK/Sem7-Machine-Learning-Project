{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np          # For mathematical calculations\n",
    "import matplotlib.pyplot as plt  # For plotting graphs\n",
    "# import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "layer_7_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\train.csv\"\n",
    "layer_7_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\valid.csv\"\n",
    "layer_7_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\test.csv\"\n",
    "\n",
    "layer_8_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer8\\\\train.csv\"\n",
    "layer_8_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer8\\\\valid.csv\"\n",
    "layer_8_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer8\\\\test.csv\"\n",
    "\n",
    "layer_9_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer9\\\\train.csv\"\n",
    "layer_9_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer9\\\\valid.csv\"\n",
    "layer_9_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer9\\\\test.csv\"\n",
    "\n",
    "layer_10_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer10\\\\train.csv\"\n",
    "layer_10_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer10\\\\valid.csv\"\n",
    "layer_10_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer10\\\\test.csv\"\n",
    "\n",
    "layer_11_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer11\\\\train.csv\"\n",
    "layer_11_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer11\\\\valid.csv\"\n",
    "layer_11_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer11\\\\test.csv\"\n",
    "\n",
    "layer_12_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer12\\\\train.csv\"\n",
    "layer_12_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer12\\\\valid.csv\"\n",
    "layer_12_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer12\\\\test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ID_preds_confidences = {}\n",
    "speaker_age_preds_confidences = {}\n",
    "speaker_gender_preds_confidences = {}\n",
    "speaker_accent_preds_confidences = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187868</td>\n",
       "      <td>-0.009268</td>\n",
       "      <td>0.039846</td>\n",
       "      <td>-0.102515</td>\n",
       "      <td>-0.109121</td>\n",
       "      <td>-0.079769</td>\n",
       "      <td>0.061295</td>\n",
       "      <td>-0.042890</td>\n",
       "      <td>0.099860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043647</td>\n",
       "      <td>0.061174</td>\n",
       "      <td>0.026771</td>\n",
       "      <td>-0.092734</td>\n",
       "      <td>-0.052383</td>\n",
       "      <td>-0.055526</td>\n",
       "      <td>0.029101</td>\n",
       "      <td>0.041857</td>\n",
       "      <td>-0.027872</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.122119</td>\n",
       "      <td>0.019015</td>\n",
       "      <td>0.134483</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>-0.157884</td>\n",
       "      <td>-0.033332</td>\n",
       "      <td>0.044334</td>\n",
       "      <td>-0.016869</td>\n",
       "      <td>-0.014088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179798</td>\n",
       "      <td>-0.027611</td>\n",
       "      <td>0.240023</td>\n",
       "      <td>-0.005648</td>\n",
       "      <td>-0.057537</td>\n",
       "      <td>0.023922</td>\n",
       "      <td>-0.038126</td>\n",
       "      <td>-0.015171</td>\n",
       "      <td>-0.006270</td>\n",
       "      <td>-0.003790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.079187</td>\n",
       "      <td>0.146195</td>\n",
       "      <td>-0.001316</td>\n",
       "      <td>-0.085596</td>\n",
       "      <td>-0.261093</td>\n",
       "      <td>-0.021620</td>\n",
       "      <td>0.268898</td>\n",
       "      <td>-0.103950</td>\n",
       "      <td>0.068976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075460</td>\n",
       "      <td>0.037346</td>\n",
       "      <td>0.124254</td>\n",
       "      <td>-0.369695</td>\n",
       "      <td>-0.056773</td>\n",
       "      <td>-0.028471</td>\n",
       "      <td>0.019971</td>\n",
       "      <td>-0.065683</td>\n",
       "      <td>-0.047366</td>\n",
       "      <td>-0.121744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.231196</td>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.215534</td>\n",
       "      <td>-0.210258</td>\n",
       "      <td>-0.158189</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.240254</td>\n",
       "      <td>-0.068112</td>\n",
       "      <td>-0.017550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0.141642</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>0.031547</td>\n",
       "      <td>-0.010762</td>\n",
       "      <td>-0.169036</td>\n",
       "      <td>-0.030963</td>\n",
       "      <td>0.086698</td>\n",
       "      <td>0.114194</td>\n",
       "      <td>-0.036775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.020412</td>\n",
       "      <td>0.063732</td>\n",
       "      <td>-0.065506</td>\n",
       "      <td>-0.089598</td>\n",
       "      <td>-0.130788</td>\n",
       "      <td>-0.018809</td>\n",
       "      <td>0.119304</td>\n",
       "      <td>-0.057494</td>\n",
       "      <td>0.094714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001969</td>\n",
       "      <td>-0.003025</td>\n",
       "      <td>-0.021156</td>\n",
       "      <td>-0.136993</td>\n",
       "      <td>-0.003615</td>\n",
       "      <td>0.036371</td>\n",
       "      <td>0.048715</td>\n",
       "      <td>-0.002688</td>\n",
       "      <td>-0.016957</td>\n",
       "      <td>-0.004331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   1   0.187868  -0.009268   0.039846  -0.102515  -0.109121  -0.079769   \n",
       "1   2   0.122119   0.019015   0.134483   0.007949  -0.157884  -0.033332   \n",
       "2   3   0.079187   0.146195  -0.001316  -0.085596  -0.261093  -0.021620   \n",
       "3   4   0.231196   0.021558   0.215534  -0.210258  -0.158189   0.041621   \n",
       "4   5   0.020412   0.063732  -0.065506  -0.089598  -0.130788  -0.018809   \n",
       "\n",
       "   feature_7  feature_8  feature_9  ...  feature_759  feature_760  \\\n",
       "0   0.061295  -0.042890   0.099860  ...     0.043647     0.061174   \n",
       "1   0.044334  -0.016869  -0.014088  ...    -0.179798    -0.027611   \n",
       "2   0.268898  -0.103950   0.068976  ...    -0.075460     0.037346   \n",
       "3   0.240254  -0.068112  -0.017550  ...     0.083334     0.141642   \n",
       "4   0.119304  -0.057494   0.094714  ...    -0.001969    -0.003025   \n",
       "\n",
       "   feature_761  feature_762  feature_763  feature_764  feature_765  \\\n",
       "0     0.026771    -0.092734    -0.052383    -0.055526     0.029101   \n",
       "1     0.240023    -0.005648    -0.057537     0.023922    -0.038126   \n",
       "2     0.124254    -0.369695    -0.056773    -0.028471     0.019971   \n",
       "3    -0.007219     0.031547    -0.010762    -0.169036    -0.030963   \n",
       "4    -0.021156    -0.136993    -0.003615     0.036371     0.048715   \n",
       "\n",
       "   feature_766  feature_767  feature_768  \n",
       "0     0.041857    -0.027872     0.099500  \n",
       "1    -0.015171    -0.006270    -0.003790  \n",
       "2    -0.065683    -0.047366    -0.121744  \n",
       "3     0.086698     0.114194    -0.036775  \n",
       "4    -0.002688    -0.016957    -0.004331  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(layer_7_test_csv_file_path)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID               int64\n",
       "feature_1      float64\n",
       "feature_2      float64\n",
       "feature_3      float64\n",
       "feature_4      float64\n",
       "                ...   \n",
       "feature_764    float64\n",
       "feature_765    float64\n",
       "feature_766    float64\n",
       "feature_767    float64\n",
       "feature_768    float64\n",
       "Length: 769, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get type of each column\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             False\n",
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "               ...  \n",
       "feature_764    False\n",
       "feature_765    False\n",
       "feature_766    False\n",
       "feature_767    False\n",
       "feature_768    False\n",
       "Length: 769, dtype: bool"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum().sum()\n",
    "\n",
    "# based on above output we can see that there are no missing values in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset shape: (744, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.187868</td>\n",
       "      <td>-0.009268</td>\n",
       "      <td>0.039846</td>\n",
       "      <td>-0.102515</td>\n",
       "      <td>-0.109121</td>\n",
       "      <td>-0.079769</td>\n",
       "      <td>0.061295</td>\n",
       "      <td>-0.042890</td>\n",
       "      <td>0.099860</td>\n",
       "      <td>-0.066319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043647</td>\n",
       "      <td>0.061174</td>\n",
       "      <td>0.026771</td>\n",
       "      <td>-0.092734</td>\n",
       "      <td>-0.052383</td>\n",
       "      <td>-0.055526</td>\n",
       "      <td>0.029101</td>\n",
       "      <td>0.041857</td>\n",
       "      <td>-0.027872</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122119</td>\n",
       "      <td>0.019015</td>\n",
       "      <td>0.134483</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>-0.157884</td>\n",
       "      <td>-0.033332</td>\n",
       "      <td>0.044334</td>\n",
       "      <td>-0.016869</td>\n",
       "      <td>-0.014088</td>\n",
       "      <td>-0.060542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179798</td>\n",
       "      <td>-0.027611</td>\n",
       "      <td>0.240023</td>\n",
       "      <td>-0.005648</td>\n",
       "      <td>-0.057537</td>\n",
       "      <td>0.023922</td>\n",
       "      <td>-0.038126</td>\n",
       "      <td>-0.015171</td>\n",
       "      <td>-0.006270</td>\n",
       "      <td>-0.003790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.079187</td>\n",
       "      <td>0.146195</td>\n",
       "      <td>-0.001316</td>\n",
       "      <td>-0.085596</td>\n",
       "      <td>-0.261093</td>\n",
       "      <td>-0.021620</td>\n",
       "      <td>0.268898</td>\n",
       "      <td>-0.103950</td>\n",
       "      <td>0.068976</td>\n",
       "      <td>-0.062388</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075460</td>\n",
       "      <td>0.037346</td>\n",
       "      <td>0.124254</td>\n",
       "      <td>-0.369695</td>\n",
       "      <td>-0.056773</td>\n",
       "      <td>-0.028471</td>\n",
       "      <td>0.019971</td>\n",
       "      <td>-0.065683</td>\n",
       "      <td>-0.047366</td>\n",
       "      <td>-0.121744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.231196</td>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.215534</td>\n",
       "      <td>-0.210258</td>\n",
       "      <td>-0.158189</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.240254</td>\n",
       "      <td>-0.068112</td>\n",
       "      <td>-0.017550</td>\n",
       "      <td>-0.233767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0.141642</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>0.031547</td>\n",
       "      <td>-0.010762</td>\n",
       "      <td>-0.169036</td>\n",
       "      <td>-0.030963</td>\n",
       "      <td>0.086698</td>\n",
       "      <td>0.114194</td>\n",
       "      <td>-0.036775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020412</td>\n",
       "      <td>0.063732</td>\n",
       "      <td>-0.065506</td>\n",
       "      <td>-0.089598</td>\n",
       "      <td>-0.130788</td>\n",
       "      <td>-0.018809</td>\n",
       "      <td>0.119304</td>\n",
       "      <td>-0.057494</td>\n",
       "      <td>0.094714</td>\n",
       "      <td>-0.047923</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001969</td>\n",
       "      <td>-0.003025</td>\n",
       "      <td>-0.021156</td>\n",
       "      <td>-0.136993</td>\n",
       "      <td>-0.003615</td>\n",
       "      <td>0.036371</td>\n",
       "      <td>0.048715</td>\n",
       "      <td>-0.002688</td>\n",
       "      <td>-0.016957</td>\n",
       "      <td>-0.004331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.187868  -0.009268   0.039846  -0.102515  -0.109121  -0.079769   \n",
       "1   0.122119   0.019015   0.134483   0.007949  -0.157884  -0.033332   \n",
       "2   0.079187   0.146195  -0.001316  -0.085596  -0.261093  -0.021620   \n",
       "3   0.231196   0.021558   0.215534  -0.210258  -0.158189   0.041621   \n",
       "4   0.020412   0.063732  -0.065506  -0.089598  -0.130788  -0.018809   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_759  feature_760  \\\n",
       "0   0.061295  -0.042890   0.099860   -0.066319  ...     0.043647     0.061174   \n",
       "1   0.044334  -0.016869  -0.014088   -0.060542  ...    -0.179798    -0.027611   \n",
       "2   0.268898  -0.103950   0.068976   -0.062388  ...    -0.075460     0.037346   \n",
       "3   0.240254  -0.068112  -0.017550   -0.233767  ...     0.083334     0.141642   \n",
       "4   0.119304  -0.057494   0.094714   -0.047923  ...    -0.001969    -0.003025   \n",
       "\n",
       "   feature_761  feature_762  feature_763  feature_764  feature_765  \\\n",
       "0     0.026771    -0.092734    -0.052383    -0.055526     0.029101   \n",
       "1     0.240023    -0.005648    -0.057537     0.023922    -0.038126   \n",
       "2     0.124254    -0.369695    -0.056773    -0.028471     0.019971   \n",
       "3    -0.007219     0.031547    -0.010762    -0.169036    -0.030963   \n",
       "4    -0.021156    -0.136993    -0.003615     0.036371     0.048715   \n",
       "\n",
       "   feature_766  feature_767  feature_768  \n",
       "0     0.041857    -0.027872     0.099500  \n",
       "1    -0.015171    -0.006270    -0.003790  \n",
       "2    -0.065683    -0.047366    -0.121744  \n",
       "3     0.086698     0.114194    -0.036775  \n",
       "4    -0.002688    -0.016957    -0.004331  \n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = test_df.drop([\"ID\"], axis=1)\n",
    "print(\"test dataset shape:\", test_X.shape)\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.186257</td>\n",
       "      <td>-0.058807</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>-0.163933</td>\n",
       "      <td>-0.146699</td>\n",
       "      <td>0.035889</td>\n",
       "      <td>0.111708</td>\n",
       "      <td>-0.162861</td>\n",
       "      <td>0.028249</td>\n",
       "      <td>-0.098063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055629</td>\n",
       "      <td>-0.010358</td>\n",
       "      <td>0.125754</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>0.079197</td>\n",
       "      <td>0.093215</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.063431</td>\n",
       "      <td>-0.023597</td>\n",
       "      <td>0.068057</td>\n",
       "      <td>-0.252915</td>\n",
       "      <td>-0.061094</td>\n",
       "      <td>-0.027316</td>\n",
       "      <td>0.135747</td>\n",
       "      <td>-0.168147</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>-0.078473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014893</td>\n",
       "      <td>0.071721</td>\n",
       "      <td>0.018918</td>\n",
       "      <td>0.100032</td>\n",
       "      <td>-0.083042</td>\n",
       "      <td>0.088615</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034962</td>\n",
       "      <td>0.035816</td>\n",
       "      <td>-0.029753</td>\n",
       "      <td>-0.094607</td>\n",
       "      <td>-0.017576</td>\n",
       "      <td>-0.053074</td>\n",
       "      <td>0.040121</td>\n",
       "      <td>-0.007932</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>-0.024042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>0.015215</td>\n",
       "      <td>0.083808</td>\n",
       "      <td>0.031312</td>\n",
       "      <td>-0.056277</td>\n",
       "      <td>0.064702</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033772</td>\n",
       "      <td>0.085612</td>\n",
       "      <td>0.067488</td>\n",
       "      <td>-0.073953</td>\n",
       "      <td>-0.180646</td>\n",
       "      <td>-0.024512</td>\n",
       "      <td>0.242879</td>\n",
       "      <td>-0.023374</td>\n",
       "      <td>-0.059999</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078246</td>\n",
       "      <td>-0.032903</td>\n",
       "      <td>0.082949</td>\n",
       "      <td>-0.020659</td>\n",
       "      <td>0.082274</td>\n",
       "      <td>-0.050164</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.134305</td>\n",
       "      <td>0.062096</td>\n",
       "      <td>0.106920</td>\n",
       "      <td>-0.089327</td>\n",
       "      <td>0.117093</td>\n",
       "      <td>-0.077107</td>\n",
       "      <td>0.152579</td>\n",
       "      <td>0.047529</td>\n",
       "      <td>-0.015998</td>\n",
       "      <td>-0.110657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094629</td>\n",
       "      <td>0.069718</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.048124</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>-0.016980</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.186257  -0.058807   0.024632  -0.163933  -0.146699   0.035889   \n",
       "1   0.063431  -0.023597   0.068057  -0.252915  -0.061094  -0.027316   \n",
       "2   0.034962   0.035816  -0.029753  -0.094607  -0.017576  -0.053074   \n",
       "3   0.033772   0.085612   0.067488  -0.073953  -0.180646  -0.024512   \n",
       "4   0.134305   0.062096   0.106920  -0.089327   0.117093  -0.077107   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0   0.111708  -0.162861   0.028249   -0.098063  ...     0.055629    -0.010358   \n",
       "1   0.135747  -0.168147   0.091236   -0.078473  ...    -0.014893     0.071721   \n",
       "2   0.040121  -0.007932   0.097872   -0.024042  ...     0.012415     0.015215   \n",
       "3   0.242879  -0.023374  -0.059999    0.002006  ...    -0.078246    -0.032903   \n",
       "4   0.152579   0.047529  -0.015998   -0.110657  ...    -0.094629     0.069718   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0     0.125754     0.011648     0.079197     0.093215       45      NaN   \n",
       "1     0.018918     0.100032    -0.083042     0.088615       45      NaN   \n",
       "2     0.083808     0.031312    -0.056277     0.064702       45      NaN   \n",
       "3     0.082949    -0.020659     0.082274    -0.050164       45      NaN   \n",
       "4     0.014379     0.048124     0.007586    -0.016980       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_train_df = pd.read_csv(layer_7_train_csv_file_path)\n",
    "l7_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l7_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l7_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l7_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "l7_train_X = l7_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)\n",
    "train_speaker_IDs = l7_train_df['label_1']\n",
    "train_speaker_ages = l7_train_df['label_2']\n",
    "train_speaker_genders = l7_train_df['label_3']\n",
    "train_speaker_accents = l7_train_df['label_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "l7_valid_df = pd.read_csv(layer_7_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l7_valid_df.shape)\n",
    "print(\"null values row count: \", l7_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l7_valid_df.isnull().sum().sum() / l7_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l7_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l7_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l7_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "l7_valid_X = l7_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)\n",
    "valid_speaker_IDs = l7_valid_df['label_1']\n",
    "valid_speaker_ages = l7_valid_df['label_2']\n",
    "valid_speaker_genders = l7_valid_df['label_3']\n",
    "valid_speaker_accents = l7_valid_df['label_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now develop pipelines to predict the speaker ID, speaker age, speaker gender, and speaker accent. \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "##visualize Pipeline\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       1.00      1.00      1.00        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      1.00      1.00        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      1.00      0.97        14\n",
      "          18       1.00      1.00      1.00        19\n",
      "          19       0.93      0.93      0.93        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       1.00      0.93      0.96        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       1.00      1.00      1.00        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      1.00      1.00        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.93      0.97        15\n",
      "          30       1.00      1.00      1.00         8\n",
      "          31       1.00      1.00      1.00        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       1.00      1.00      1.00         6\n",
      "          34       1.00      1.00      1.00        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      1.00      1.00        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       1.00      1.00      1.00        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       1.00      1.00      1.00        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       0.91      1.00      0.95        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       0.99      0.99      0.99       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PCA__n_components = 0.97\n",
    "clf__C = 10.0\n",
    "clf__kernel = 'rbf'\n",
    "\n",
    "layer_7_speaker_ID_pipe = Pipeline([\n",
    "    ('pca', PCA(n_components=PCA__n_components)),\n",
    "    ('clf', SVC(C=clf__C, kernel=clf__kernel))\n",
    "    ])\n",
    "\n",
    "layer_7_speaker_ID_pipe.fit(l7_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_7_speaker_ID_pipe.predict(l7_valid_X))\n",
    "print(report)\n",
    "\n",
    "# The above pipeline as a text\n",
    "# PCA(n_components=0.97) -> SVC(C=10.0, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_7_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.92      1.00      0.96        36\n",
      "        23.0       0.96      0.97      0.97        71\n",
      "        24.0       0.96      0.96      0.96        46\n",
      "        25.0       0.99      0.90      0.94        79\n",
      "        26.0       0.93      0.99      0.96       115\n",
      "        27.0       1.00      0.95      0.97        81\n",
      "        28.0       0.97      0.97      0.97        60\n",
      "        29.0       1.00      1.00      1.00        45\n",
      "        30.0       0.96      1.00      0.98        48\n",
      "        31.0       0.95      0.97      0.96        65\n",
      "        32.0       1.00      1.00      1.00        11\n",
      "        33.0       1.00      1.00      1.00        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.93      0.96        14\n",
      "        61.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.98      0.97      0.97       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca__n_components = 0.97\n",
    "clf__C = 10.0\n",
    "clf__kernel = 'rbf'\n",
    "\n",
    "# Pipleline with best params\n",
    "layer_7_speaker_age_pipe = Pipeline([\n",
    "    # ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=pca__n_components)),\n",
    "    ('clf', SVC(C=clf__C, kernel=clf__kernel))\n",
    "    ])\n",
    "\n",
    "layer_7_speaker_age_pipe.fit(l7_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_7_speaker_age_pipe.predict(l7_valid_X))\n",
    "print(report)\n",
    "\n",
    "# The above pipeline as a text\n",
    "# PCA(n_components=0.97) -> SVC(C=10.0, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_7_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With SVC classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      1.00      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_7_speaker_gender_pipe = Pipeline([ \n",
    "    ('clf', SVC(class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_7_speaker_gender_pipe.fit(l7_train_X, train_speaker_genders)\n",
    "\n",
    "# now let's check the results\n",
    "print(\"With SVC classifier:\")\n",
    "report = classification_report(valid_speaker_genders, layer_7_speaker_gender_pipe.predict(l7_valid_X)) \n",
    "print(report)\n",
    "\n",
    "# The above pipeline as a text\n",
    "# SVC(class_weight='balanced') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_gender_preds_confidences['layer_7_speaker_gender_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        21\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       1.00      1.00      1.00        27\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       1.00      0.93      0.97        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.99      1.00      0.99       532\n",
      "           7       1.00      0.97      0.98        32\n",
      "           8       1.00      0.95      0.97        19\n",
      "           9       1.00      1.00      1.00        17\n",
      "          10       1.00      1.00      1.00        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       1.00      1.00      1.00        26\n",
      "          13       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       1.00      0.97      0.98       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca__n_components = 0.97\n",
    "clf__C = 10.0\n",
    "clf__kernel = 'rbf'\n",
    "\n",
    "# Pipleline with best params\n",
    "layer_7_speaker_accent_pipe = Pipeline([\n",
    "    # ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=pca__n_components)),\n",
    "    ('clf', SVC(C=clf__C, kernel=clf__kernel))\n",
    "    ])\n",
    "\n",
    "layer_7_speaker_accent_pipe.fit(l7_train_X, train_speaker_accents)\n",
    "report = classification_report(valid_speaker_accents, layer_7_speaker_accent_pipe.predict(l7_valid_X))\n",
    "print(report)\n",
    "\n",
    "# The above pipeline as a text\n",
    "# PCA(n_components=0.97) -> SVC(C=10.0, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_accent_preds_confidences['layer_7_speaker_accent_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.071810</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>-0.022749</td>\n",
       "      <td>0.086143</td>\n",
       "      <td>0.026361</td>\n",
       "      <td>-0.028817</td>\n",
       "      <td>0.199237</td>\n",
       "      <td>-0.287368</td>\n",
       "      <td>-0.059560</td>\n",
       "      <td>-0.043694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004646</td>\n",
       "      <td>0.123011</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>-0.042152</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>0.072623</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.030930</td>\n",
       "      <td>0.024088</td>\n",
       "      <td>0.057811</td>\n",
       "      <td>-0.230877</td>\n",
       "      <td>-0.146281</td>\n",
       "      <td>0.102807</td>\n",
       "      <td>0.128767</td>\n",
       "      <td>-0.146269</td>\n",
       "      <td>0.053893</td>\n",
       "      <td>0.055378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077742</td>\n",
       "      <td>0.081691</td>\n",
       "      <td>-0.004778</td>\n",
       "      <td>0.171727</td>\n",
       "      <td>-0.026027</td>\n",
       "      <td>0.171089</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.044019</td>\n",
       "      <td>-0.004626</td>\n",
       "      <td>-0.029383</td>\n",
       "      <td>-0.165376</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>-0.028142</td>\n",
       "      <td>-0.009649</td>\n",
       "      <td>-0.082088</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>-0.046272</td>\n",
       "      <td>0.027831</td>\n",
       "      <td>0.028096</td>\n",
       "      <td>0.030994</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.086241</td>\n",
       "      <td>0.129585</td>\n",
       "      <td>-0.013893</td>\n",
       "      <td>0.089885</td>\n",
       "      <td>-0.100300</td>\n",
       "      <td>-0.035184</td>\n",
       "      <td>0.240980</td>\n",
       "      <td>-0.128362</td>\n",
       "      <td>-0.072328</td>\n",
       "      <td>-0.019385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006934</td>\n",
       "      <td>-0.049213</td>\n",
       "      <td>0.078852</td>\n",
       "      <td>0.088323</td>\n",
       "      <td>0.168815</td>\n",
       "      <td>-0.049188</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126416</td>\n",
       "      <td>0.088338</td>\n",
       "      <td>0.088307</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.174417</td>\n",
       "      <td>-0.030560</td>\n",
       "      <td>0.181163</td>\n",
       "      <td>-0.009382</td>\n",
       "      <td>0.085396</td>\n",
       "      <td>0.015823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028883</td>\n",
       "      <td>0.110844</td>\n",
       "      <td>-0.041875</td>\n",
       "      <td>0.025686</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>-0.104945</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.071810   0.068413  -0.022749   0.086143   0.026361  -0.028817   \n",
       "1   0.030930   0.024088   0.057811  -0.230877  -0.146281   0.102807   \n",
       "2  -0.044019  -0.004626  -0.029383  -0.165376  -0.026611  -0.028142   \n",
       "3  -0.086241   0.129585  -0.013893   0.089885  -0.100300  -0.035184   \n",
       "4   0.126416   0.088338   0.088307   0.020371   0.174417  -0.030560   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0   0.199237  -0.287368  -0.059560   -0.043694  ...     0.004646     0.123011   \n",
       "1   0.128767  -0.146269   0.053893    0.055378  ...     0.077742     0.081691   \n",
       "2  -0.009649  -0.082088   0.018933    0.006830  ...     0.076249    -0.046272   \n",
       "3   0.240980  -0.128362  -0.072328   -0.019385  ...     0.006934    -0.049213   \n",
       "4   0.181163  -0.009382   0.085396    0.015823  ...    -0.028883     0.110844   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0     0.043040    -0.042152     0.026225     0.072623       45      NaN   \n",
       "1    -0.004778     0.171727    -0.026027     0.171089       45      NaN   \n",
       "2     0.027831     0.028096     0.030994     0.009709       45      NaN   \n",
       "3     0.078852     0.088323     0.168815    -0.049188       45      NaN   \n",
       "4    -0.041875     0.025686     0.003534    -0.104945       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_train_df = pd.read_csv(layer_8_train_csv_file_path)\n",
    "l8_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l8_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l8_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l8_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "l8_train_X = l8_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_valid_df = pd.read_csv(layer_8_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l8_valid_df.shape)\n",
    "print(\"null values row count: \", l8_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l8_valid_df.isnull().sum().sum() / l8_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l8_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l8_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l8_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_valid_X = l8_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.92      0.92        13\n",
      "           2       1.00      0.78      0.88         9\n",
      "           3       0.86      1.00      0.92        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       0.84      0.89      0.86        18\n",
      "           6       0.90      1.00      0.95         9\n",
      "           7       0.89      0.94      0.91        17\n",
      "           8       0.91      0.71      0.80        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      0.89      0.94        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.82      0.90        11\n",
      "          14       0.93      0.87      0.90        15\n",
      "          15       1.00      0.88      0.94        17\n",
      "          16       1.00      0.93      0.96        14\n",
      "          17       1.00      0.93      0.96        14\n",
      "          18       0.95      0.95      0.95        19\n",
      "          19       0.74      0.93      0.82        15\n",
      "          20       1.00      0.82      0.90        11\n",
      "          21       0.87      0.93      0.90        14\n",
      "          22       0.91      0.91      0.91        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.65      0.92      0.76        12\n",
      "          25       0.91      0.91      0.91        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.94      0.94      0.94        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.87      0.93        15\n",
      "          30       0.88      0.88      0.88         8\n",
      "          31       0.92      0.92      0.92        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.77      0.87        13\n",
      "          35       0.89      1.00      0.94         8\n",
      "          36       0.85      0.94      0.89        18\n",
      "          37       1.00      0.81      0.90        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       0.92      0.92      0.92        12\n",
      "          41       0.83      1.00      0.91        10\n",
      "          42       0.92      1.00      0.96        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       0.90      0.95      0.92        19\n",
      "          45       0.93      0.93      0.93        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       0.70      0.88      0.78         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       1.00      0.85      0.92        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       0.80      1.00      0.89         8\n",
      "          52       0.85      1.00      0.92        11\n",
      "          53       0.93      0.93      0.93        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       0.88      0.88      0.88         8\n",
      "          56       1.00      0.90      0.95        10\n",
      "          57       1.00      0.94      0.97        18\n",
      "          58       1.00      0.95      0.97        20\n",
      "          59       0.90      0.90      0.90        10\n",
      "          60       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.93       750\n",
      "   macro avg       0.93      0.93      0.93       750\n",
      "weighted avg       0.93      0.93      0.93       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_8_speaker_ID_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.85)),\n",
    "    ('clf', SVC(C=10, gamma=0.001, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_8_speaker_ID_pipe.fit(l8_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_8_speaker_ID_pipe.predict(l8_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_8_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.97      0.83      0.90        36\n",
      "        23.0       0.96      0.90      0.93        71\n",
      "        24.0       0.85      0.89      0.87        46\n",
      "        25.0       0.95      0.90      0.92        79\n",
      "        26.0       0.83      0.95      0.88       115\n",
      "        27.0       0.94      0.89      0.91        81\n",
      "        28.0       0.86      0.92      0.89        60\n",
      "        29.0       0.93      0.96      0.95        45\n",
      "        30.0       0.98      0.92      0.95        48\n",
      "        31.0       0.89      0.98      0.93        65\n",
      "        32.0       1.00      0.82      0.90        11\n",
      "        33.0       1.00      0.93      0.97        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.73      0.84        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.93      0.96        14\n",
      "        61.0       1.00      0.89      0.94        19\n",
      "\n",
      "    accuracy                           0.91       750\n",
      "   macro avg       0.95      0.90      0.92       750\n",
      "weighted avg       0.92      0.91      0.92       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_8_age_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.85)),\n",
    "    ('clf', SVC(C=100, class_weight='balanced', gamma=0.01, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_8_age_pipe.fit(l8_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_8_age_pipe.predict(l8_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_8_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       0.99      1.00      0.99       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_8_gender_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.85)),\n",
    "    ('clf', SVC(C=10, class_weight='balanced', gamma=0.001, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_8_gender_pipe.fit(l8_train_X, train_speaker_genders)\n",
    "report = classification_report(valid_speaker_genders, layer_8_gender_pipe.predict(l8_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_gender_preds_confidences['layer_8_speaker_gender_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0, 'layer_8_speaker_gender_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        21\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       0.60      0.93      0.72        27\n",
      "           3       0.89      1.00      0.94         8\n",
      "           4       0.76      0.87      0.81        15\n",
      "           5       0.90      0.82      0.86        11\n",
      "           6       0.96      0.94      0.95       532\n",
      "           7       0.83      0.78      0.81        32\n",
      "           8       1.00      0.84      0.91        19\n",
      "           9       1.00      0.88      0.94        17\n",
      "          10       1.00      0.90      0.95        10\n",
      "          11       0.91      0.91      0.91        11\n",
      "          12       0.76      0.85      0.80        26\n",
      "          13       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.92       750\n",
      "   macro avg       0.89      0.89      0.88       750\n",
      "weighted avg       0.93      0.92      0.92       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_8_accent_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.85)),\n",
    "    ('clf', SVC(C=100, class_weight='balanced', gamma=0.001, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_8_accent_pipe.fit(l8_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_8_accent_pipe.predict(l8_valid_X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_accent_preds_confidences['layer_8_speaker_accent_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99, 'layer_8_speaker_accent_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019301</td>\n",
       "      <td>0.059756</td>\n",
       "      <td>0.081375</td>\n",
       "      <td>0.057481</td>\n",
       "      <td>-0.068440</td>\n",
       "      <td>-0.165913</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>-0.091138</td>\n",
       "      <td>0.021688</td>\n",
       "      <td>0.057158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035576</td>\n",
       "      <td>0.127319</td>\n",
       "      <td>0.098128</td>\n",
       "      <td>-0.058787</td>\n",
       "      <td>0.100971</td>\n",
       "      <td>-0.047754</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049741</td>\n",
       "      <td>0.090030</td>\n",
       "      <td>0.035118</td>\n",
       "      <td>-0.013676</td>\n",
       "      <td>-0.194317</td>\n",
       "      <td>-0.101763</td>\n",
       "      <td>0.085875</td>\n",
       "      <td>-0.081317</td>\n",
       "      <td>0.112418</td>\n",
       "      <td>0.120523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020538</td>\n",
       "      <td>0.058968</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.111324</td>\n",
       "      <td>0.036727</td>\n",
       "      <td>0.031927</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019212</td>\n",
       "      <td>0.087779</td>\n",
       "      <td>0.093907</td>\n",
       "      <td>-0.033738</td>\n",
       "      <td>-0.141409</td>\n",
       "      <td>-0.062881</td>\n",
       "      <td>-0.071402</td>\n",
       "      <td>-0.006599</td>\n",
       "      <td>0.020372</td>\n",
       "      <td>-0.027777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119645</td>\n",
       "      <td>-0.040861</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>-0.061003</td>\n",
       "      <td>-0.042450</td>\n",
       "      <td>0.063340</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070283</td>\n",
       "      <td>0.049040</td>\n",
       "      <td>0.042126</td>\n",
       "      <td>0.122637</td>\n",
       "      <td>-0.056964</td>\n",
       "      <td>-0.113700</td>\n",
       "      <td>0.108454</td>\n",
       "      <td>0.051336</td>\n",
       "      <td>0.086610</td>\n",
       "      <td>0.141578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124494</td>\n",
       "      <td>-0.169225</td>\n",
       "      <td>-0.046391</td>\n",
       "      <td>0.148787</td>\n",
       "      <td>0.014616</td>\n",
       "      <td>-0.140644</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028864</td>\n",
       "      <td>0.165634</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>0.036117</td>\n",
       "      <td>-0.028871</td>\n",
       "      <td>-0.147748</td>\n",
       "      <td>0.053180</td>\n",
       "      <td>0.025071</td>\n",
       "      <td>-0.004200</td>\n",
       "      <td>-0.022183</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124862</td>\n",
       "      <td>0.044907</td>\n",
       "      <td>0.084005</td>\n",
       "      <td>-0.038450</td>\n",
       "      <td>0.084371</td>\n",
       "      <td>-0.072146</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.019301   0.059756   0.081375   0.057481  -0.068440  -0.165913   \n",
       "1   0.049741   0.090030   0.035118  -0.013676  -0.194317  -0.101763   \n",
       "2   0.019212   0.087779   0.093907  -0.033738  -0.141409  -0.062881   \n",
       "3   0.070283   0.049040   0.042126   0.122637  -0.056964  -0.113700   \n",
       "4   0.028864   0.165634   0.016302   0.036117  -0.028871  -0.147748   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0   0.035643  -0.091138   0.021688    0.057158  ...    -0.035576     0.127319   \n",
       "1   0.085875  -0.081317   0.112418    0.120523  ...     0.020538     0.058968   \n",
       "2  -0.071402  -0.006599   0.020372   -0.027777  ...     0.119645    -0.040861   \n",
       "3   0.108454   0.051336   0.086610    0.141578  ...    -0.124494    -0.169225   \n",
       "4   0.053180   0.025071  -0.004200   -0.022183  ...    -0.124862     0.044907   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0     0.098128    -0.058787     0.100971    -0.047754       45      NaN   \n",
       "1     0.029803     0.111324     0.036727     0.031927       45      NaN   \n",
       "2     0.000548    -0.061003    -0.042450     0.063340       45      NaN   \n",
       "3    -0.046391     0.148787     0.014616    -0.140644       45      NaN   \n",
       "4     0.084005    -0.038450     0.084371    -0.072146       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_train_df = pd.read_csv(layer_9_train_csv_file_path)\n",
    "l9_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l9_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean: 28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l9_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean:\", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l9_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_train_X = l9_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_valid_df = pd.read_csv(layer_9_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l9_valid_df.shape)\n",
    "print(\"null values row count: \", l9_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l9_valid_df.isnull().sum().sum() / l9_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l9_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l9_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l9_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_valid_X = l9_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       0.92      1.00      0.96        12\n",
      "           4       0.88      0.94      0.91        16\n",
      "           5       1.00      0.89      0.94        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       0.94      0.94      0.94        17\n",
      "           8       1.00      0.79      0.88        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       0.89      1.00      0.94         8\n",
      "          11       0.95      0.95      0.95        19\n",
      "          12       0.88      1.00      0.93         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.87      0.93        15\n",
      "          15       1.00      0.88      0.94        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       1.00      1.00      1.00        14\n",
      "          18       0.90      0.95      0.92        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      0.91      0.95        11\n",
      "          21       0.93      0.93      0.93        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.92      0.92      0.92        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.84      1.00      0.91        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      0.88      0.93         8\n",
      "          31       0.85      0.92      0.88        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.85      0.92        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       0.95      1.00      0.97        18\n",
      "          37       1.00      0.94      0.97        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       0.91      1.00      0.95        10\n",
      "          42       0.92      1.00      0.96        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       0.93      1.00      0.97        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       0.88      0.88      0.88         8\n",
      "          48       0.84      0.94      0.89        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       0.89      1.00      0.94         8\n",
      "          52       0.91      0.91      0.91        11\n",
      "          53       1.00      0.93      0.97        15\n",
      "          54       0.89      0.89      0.89         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       0.90      0.90      0.90        10\n",
      "          57       0.95      1.00      0.97        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.96       750\n",
      "   macro avg       0.96      0.96      0.96       750\n",
      "weighted avg       0.96      0.96      0.96       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_9_speaker_ID_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.94)),\n",
    "    ('clf', SVC(C=28, kernel='rbf'))\n",
    "    ])\n",
    "\n",
    "layer_9_speaker_ID_pipe.fit(l9_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_9_speaker_ID_pipe.predict(l9_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_9_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.92      1.00      0.96        36\n",
      "        23.0       0.88      0.92      0.90        71\n",
      "        24.0       0.90      0.98      0.94        46\n",
      "        25.0       0.97      0.91      0.94        79\n",
      "        26.0       0.92      0.96      0.94       115\n",
      "        27.0       0.95      0.93      0.94        81\n",
      "        28.0       0.93      0.95      0.94        60\n",
      "        29.0       0.98      0.98      0.98        45\n",
      "        30.0       0.94      0.96      0.95        48\n",
      "        31.0       0.98      0.97      0.98        65\n",
      "        32.0       1.00      1.00      1.00        11\n",
      "        33.0       1.00      0.93      0.97        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.71      0.83        14\n",
      "        61.0       1.00      0.95      0.97        19\n",
      "\n",
      "    accuracy                           0.94       750\n",
      "   macro avg       0.96      0.94      0.95       750\n",
      "weighted avg       0.95      0.94      0.94       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_9_age_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.96)),\n",
    "    ('clf', SVC(C=55, kernel='rbf', class_weight='balanced', probability=True))\n",
    "    ])\n",
    "\n",
    "layer_9_age_pipe.fit(l9_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_9_age_pipe.predict(l9_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_9_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       0.99      0.99      0.99       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_9_gender_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.90)),\n",
    "    ('clf', SVC(C=9, kernel='rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_9_gender_pipe.fit(l9_train_X, train_speaker_genders)\n",
    "report = classification_report(valid_speaker_genders, layer_9_gender_pipe.predict(l9_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_gender_preds_confidences['layer_9_speaker_gender_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0, 'layer_8_speaker_gender_pipe': 1.0, 'layer_9_speaker_gender_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.93      1.00      0.96        27\n",
      "           3       1.00      0.88      0.93         8\n",
      "           4       1.00      0.87      0.93        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.97      1.00      0.99       532\n",
      "           7       1.00      0.91      0.95        32\n",
      "           8       0.95      0.95      0.95        19\n",
      "           9       1.00      0.82      0.90        17\n",
      "          10       1.00      1.00      1.00        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       1.00      0.88      0.94        26\n",
      "          13       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.98       750\n",
      "   macro avg       0.99      0.94      0.96       750\n",
      "weighted avg       0.98      0.98      0.98       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_9_accent_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('clf', SVC(C=18, kernel='rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_9_accent_pipe.fit(l9_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_9_accent_pipe.predict(l9_valid_X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_accent_preds_confidences['layer_9_speaker_accent_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99, 'layer_8_speaker_accent_pipe': 1.0, 'layer_9_speaker_accent_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.027083</td>\n",
       "      <td>0.072947</td>\n",
       "      <td>-0.093659</td>\n",
       "      <td>0.053418</td>\n",
       "      <td>-0.085516</td>\n",
       "      <td>-0.102610</td>\n",
       "      <td>-0.021217</td>\n",
       "      <td>0.016162</td>\n",
       "      <td>-0.184269</td>\n",
       "      <td>0.110335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183643</td>\n",
       "      <td>0.091299</td>\n",
       "      <td>-0.037097</td>\n",
       "      <td>0.042607</td>\n",
       "      <td>-0.034361</td>\n",
       "      <td>-0.013748</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.070195</td>\n",
       "      <td>0.228641</td>\n",
       "      <td>-0.132860</td>\n",
       "      <td>-0.077761</td>\n",
       "      <td>-0.054993</td>\n",
       "      <td>-0.210365</td>\n",
       "      <td>0.127747</td>\n",
       "      <td>-0.132385</td>\n",
       "      <td>-0.161366</td>\n",
       "      <td>0.172764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123668</td>\n",
       "      <td>0.029626</td>\n",
       "      <td>-0.027345</td>\n",
       "      <td>0.055223</td>\n",
       "      <td>-0.179725</td>\n",
       "      <td>0.136841</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.164312</td>\n",
       "      <td>0.052808</td>\n",
       "      <td>-0.058510</td>\n",
       "      <td>0.104724</td>\n",
       "      <td>-0.025886</td>\n",
       "      <td>-0.101427</td>\n",
       "      <td>-0.047177</td>\n",
       "      <td>0.091298</td>\n",
       "      <td>-0.094569</td>\n",
       "      <td>0.088062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075410</td>\n",
       "      <td>0.070125</td>\n",
       "      <td>0.043022</td>\n",
       "      <td>0.012972</td>\n",
       "      <td>-0.028920</td>\n",
       "      <td>0.096725</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.113737</td>\n",
       "      <td>0.061113</td>\n",
       "      <td>-0.099329</td>\n",
       "      <td>-0.111600</td>\n",
       "      <td>-0.245942</td>\n",
       "      <td>0.086520</td>\n",
       "      <td>0.071996</td>\n",
       "      <td>0.028319</td>\n",
       "      <td>0.207910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062511</td>\n",
       "      <td>-0.226912</td>\n",
       "      <td>-0.046011</td>\n",
       "      <td>0.011282</td>\n",
       "      <td>-0.095167</td>\n",
       "      <td>0.039979</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.031364</td>\n",
       "      <td>0.142409</td>\n",
       "      <td>-0.160743</td>\n",
       "      <td>-0.076594</td>\n",
       "      <td>-0.062412</td>\n",
       "      <td>-0.264732</td>\n",
       "      <td>0.079197</td>\n",
       "      <td>0.026060</td>\n",
       "      <td>-0.217023</td>\n",
       "      <td>0.084656</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193882</td>\n",
       "      <td>0.107297</td>\n",
       "      <td>-0.042355</td>\n",
       "      <td>0.046763</td>\n",
       "      <td>-0.192469</td>\n",
       "      <td>0.006463</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.027083   0.072947  -0.093659   0.053418  -0.085516  -0.102610   \n",
       "1   0.070195   0.228641  -0.132860  -0.077761  -0.054993  -0.210365   \n",
       "2   0.164312   0.052808  -0.058510   0.104724  -0.025886  -0.101427   \n",
       "3   0.029730   0.113737   0.061113  -0.099329  -0.111600  -0.245942   \n",
       "4   0.031364   0.142409  -0.160743  -0.076594  -0.062412  -0.264732   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0  -0.021217   0.016162  -0.184269    0.110335  ...    -0.183643     0.091299   \n",
       "1   0.127747  -0.132385  -0.161366    0.172764  ...    -0.123668     0.029626   \n",
       "2  -0.047177   0.091298  -0.094569    0.088062  ...     0.075410     0.070125   \n",
       "3   0.086520   0.071996   0.028319    0.207910  ...    -0.062511    -0.226912   \n",
       "4   0.079197   0.026060  -0.217023    0.084656  ...    -0.193882     0.107297   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0    -0.037097     0.042607    -0.034361    -0.013748       45      NaN   \n",
       "1    -0.027345     0.055223    -0.179725     0.136841       45      NaN   \n",
       "2     0.043022     0.012972    -0.028920     0.096725       45      NaN   \n",
       "3    -0.046011     0.011282    -0.095167     0.039979       45      NaN   \n",
       "4    -0.042355     0.046763    -0.192469     0.006463       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_train_df = pd.read_csv(layer_10_train_csv_file_path)\n",
    "l10_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l10_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean: 28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l10_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean:\", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l10_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_train_X = l10_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_valid_df = pd.read_csv(layer_10_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l10_valid_df.shape)\n",
    "print(\"null values row count: \", l10_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l10_valid_df.isnull().sum().sum() / l10_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l10_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l10_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l10_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_valid_X = l10_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.92      0.89        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       1.00      1.00      1.00        16\n",
      "           5       0.94      0.89      0.91        18\n",
      "           6       1.00      0.89      0.94         9\n",
      "           7       0.94      0.94      0.94        17\n",
      "           8       1.00      0.79      0.88        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      0.95      0.97        19\n",
      "          12       0.88      1.00      0.93         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      1.00      1.00        15\n",
      "          15       1.00      0.88      0.94        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      0.93      0.93        14\n",
      "          18       1.00      0.95      0.97        19\n",
      "          19       0.78      0.93      0.85        15\n",
      "          20       1.00      0.82      0.90        11\n",
      "          21       1.00      1.00      1.00        14\n",
      "          22       0.92      1.00      0.96        11\n",
      "          23       1.00      1.00      1.00        10\n",
      "          24       0.92      1.00      0.96        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.94      1.00      0.97        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      0.88      0.93         8\n",
      "          31       0.85      0.92      0.88        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.85      0.92        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       0.94      1.00      0.97        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       0.93      1.00      0.96        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       0.92      1.00      0.96        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      1.00      1.00        19\n",
      "          45       0.93      1.00      0.97        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       0.89      1.00      0.94         8\n",
      "          48       0.89      0.94      0.91        17\n",
      "          49       0.87      1.00      0.93        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      1.00      1.00        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      0.95      0.97        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.97      0.97      0.97       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_10_speaker_ID_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('clf', SVC(C=10, kernel='rbf'))\n",
    "    ])\n",
    "\n",
    "layer_10_speaker_ID_pipe.fit(l10_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_10_speaker_ID_pipe.predict(l10_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_10_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96, 'layer_10_speaker_ID_pipe': 0.97}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.95      0.97      0.96        36\n",
      "        23.0       0.83      0.94      0.88        71\n",
      "        24.0       0.92      0.98      0.95        46\n",
      "        25.0       0.95      0.87      0.91        79\n",
      "        26.0       0.97      0.97      0.97       115\n",
      "        27.0       0.93      0.94      0.93        81\n",
      "        28.0       0.98      0.93      0.96        60\n",
      "        29.0       0.98      0.96      0.97        45\n",
      "        30.0       0.98      0.96      0.97        48\n",
      "        31.0       0.93      0.95      0.94        65\n",
      "        32.0       1.00      0.91      0.95        11\n",
      "        33.0       0.97      0.97      0.97        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.71      0.83        14\n",
      "        61.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.94       750\n",
      "   macro avg       0.96      0.93      0.95       750\n",
      "weighted avg       0.95      0.94      0.94       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_10_age_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.92)),\n",
    "    ('clf', SVC(C=27, kernel='rbf', class_weight='balanced', probability=True))\n",
    "    ])\n",
    "\n",
    "layer_10_age_pipe.fit(l10_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_10_age_pipe.predict(l10_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_10_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94, 'layer_10_speaker_age_pipe': 0.94}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      1.00      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_10_gender_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.83)),\n",
    "    ('clf', SVC(C=23, kernel='rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_10_gender_pipe.fit(l10_train_X, train_speaker_genders)\n",
    "report = classification_report(valid_speaker_genders, layer_10_gender_pipe.predict(l10_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_gender_preds_confidences['layer_10_speaker_gender_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0, 'layer_8_speaker_gender_pipe': 1.0, 'layer_9_speaker_gender_pipe': 1.0, 'layer_10_speaker_gender_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.96      0.96      0.96        27\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       1.00      0.93      0.97        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.98      1.00      0.99       532\n",
      "           7       1.00      0.94      0.97        32\n",
      "           8       1.00      0.95      0.97        19\n",
      "           9       1.00      0.82      0.90        17\n",
      "          10       1.00      0.90      0.95        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       1.00      0.92      0.96        26\n",
      "          13       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.98       750\n",
      "   macro avg       1.00      0.95      0.97       750\n",
      "weighted avg       0.98      0.98      0.98       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_10_accent_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('clf', SVC(C=10, kernel='rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_10_accent_pipe.fit(l10_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_10_accent_pipe.predict(l10_valid_X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_accent_preds_confidences['layer_10_speaker_accent_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99, 'layer_8_speaker_accent_pipe': 1.0, 'layer_9_speaker_accent_pipe': 1.0, 'layer_10_speaker_accent_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031138</td>\n",
       "      <td>0.079892</td>\n",
       "      <td>0.157382</td>\n",
       "      <td>-0.014636</td>\n",
       "      <td>-0.051778</td>\n",
       "      <td>-0.021332</td>\n",
       "      <td>-0.073593</td>\n",
       "      <td>-0.005386</td>\n",
       "      <td>-0.212557</td>\n",
       "      <td>0.099683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085248</td>\n",
       "      <td>-0.096007</td>\n",
       "      <td>-0.000766</td>\n",
       "      <td>0.021399</td>\n",
       "      <td>-0.041432</td>\n",
       "      <td>0.094806</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113040</td>\n",
       "      <td>0.175731</td>\n",
       "      <td>0.217741</td>\n",
       "      <td>-0.196254</td>\n",
       "      <td>-0.010129</td>\n",
       "      <td>-0.030586</td>\n",
       "      <td>0.067114</td>\n",
       "      <td>-0.072412</td>\n",
       "      <td>-0.239192</td>\n",
       "      <td>0.104741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090283</td>\n",
       "      <td>-0.053885</td>\n",
       "      <td>-0.010967</td>\n",
       "      <td>0.062209</td>\n",
       "      <td>-0.122958</td>\n",
       "      <td>0.192949</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.048570</td>\n",
       "      <td>0.091281</td>\n",
       "      <td>0.160776</td>\n",
       "      <td>-0.150937</td>\n",
       "      <td>0.020115</td>\n",
       "      <td>0.044117</td>\n",
       "      <td>-0.050092</td>\n",
       "      <td>-0.045661</td>\n",
       "      <td>-0.155332</td>\n",
       "      <td>0.117206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021524</td>\n",
       "      <td>-0.008411</td>\n",
       "      <td>-0.006248</td>\n",
       "      <td>0.031468</td>\n",
       "      <td>-0.056915</td>\n",
       "      <td>0.154731</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.039212</td>\n",
       "      <td>0.118388</td>\n",
       "      <td>0.173831</td>\n",
       "      <td>-0.096659</td>\n",
       "      <td>-0.008702</td>\n",
       "      <td>0.061298</td>\n",
       "      <td>0.008974</td>\n",
       "      <td>-0.003277</td>\n",
       "      <td>-0.065046</td>\n",
       "      <td>0.095480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071936</td>\n",
       "      <td>-0.023120</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>-0.121892</td>\n",
       "      <td>0.072796</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.056019</td>\n",
       "      <td>0.170639</td>\n",
       "      <td>0.157917</td>\n",
       "      <td>-0.228605</td>\n",
       "      <td>-0.065965</td>\n",
       "      <td>-0.088732</td>\n",
       "      <td>-0.082243</td>\n",
       "      <td>-0.080568</td>\n",
       "      <td>-0.341500</td>\n",
       "      <td>0.142430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155621</td>\n",
       "      <td>-0.079447</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>-0.151966</td>\n",
       "      <td>0.169634</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.031138   0.079892   0.157382  -0.014636  -0.051778  -0.021332   \n",
       "1   0.113040   0.175731   0.217741  -0.196254  -0.010129  -0.030586   \n",
       "2   0.048570   0.091281   0.160776  -0.150937   0.020115   0.044117   \n",
       "3   0.039212   0.118388   0.173831  -0.096659  -0.008702   0.061298   \n",
       "4   0.056019   0.170639   0.157917  -0.228605  -0.065965  -0.088732   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0  -0.073593  -0.005386  -0.212557    0.099683  ...    -0.085248    -0.096007   \n",
       "1   0.067114  -0.072412  -0.239192    0.104741  ...    -0.090283    -0.053885   \n",
       "2  -0.050092  -0.045661  -0.155332    0.117206  ...    -0.021524    -0.008411   \n",
       "3   0.008974  -0.003277  -0.065046    0.095480  ...    -0.071936    -0.023120   \n",
       "4  -0.082243  -0.080568  -0.341500    0.142430  ...    -0.155621    -0.079447   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0    -0.000766     0.021399    -0.041432     0.094806       45      NaN   \n",
       "1    -0.010967     0.062209    -0.122958     0.192949       45      NaN   \n",
       "2    -0.006248     0.031468    -0.056915     0.154731       45      NaN   \n",
       "3    -0.007812     0.057600    -0.121892     0.072796       45      NaN   \n",
       "4     0.015316     0.127726    -0.151966     0.169634       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_train_df = pd.read_csv(layer_11_train_csv_file_path)\n",
    "l11_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l11_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l11_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l11_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "l11_train_X = l11_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "l11_valid_df = pd.read_csv(layer_11_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l11_valid_df.shape)\n",
    "print(\"null values row count: \", l11_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l11_valid_df.isnull().sum().sum() / l11_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l11_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l11_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l11_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "l11_valid_X = l11_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      1.00      0.90        13\n",
      "           2       1.00      0.89      0.94         9\n",
      "           3       0.86      1.00      0.92        12\n",
      "           4       1.00      0.88      0.93        16\n",
      "           5       1.00      0.94      0.97        18\n",
      "           6       1.00      0.89      0.94         9\n",
      "           7       0.89      0.94      0.91        17\n",
      "           8       0.86      0.86      0.86        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      0.95      0.97        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       0.94      0.88      0.91        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.87      0.93      0.90        14\n",
      "          18       1.00      0.95      0.97        19\n",
      "          19       0.78      0.93      0.85        15\n",
      "          20       0.91      0.91      0.91        11\n",
      "          21       0.88      1.00      0.93        14\n",
      "          22       1.00      0.91      0.95        11\n",
      "          23       0.83      1.00      0.91        10\n",
      "          24       0.92      1.00      0.96        12\n",
      "          25       1.00      0.91      0.95        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      0.94      0.97        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.93      0.97        15\n",
      "          30       0.88      0.88      0.88         8\n",
      "          31       0.91      0.83      0.87        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       0.92      0.85      0.88        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      0.81      0.90        16\n",
      "          38       1.00      0.91      0.95        11\n",
      "          39       0.93      1.00      0.96        13\n",
      "          40       0.92      0.92      0.92        12\n",
      "          41       0.91      1.00      0.95        10\n",
      "          42       0.86      1.00      0.92        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      0.93      0.96        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       0.93      1.00      0.96        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       0.80      1.00      0.89         8\n",
      "          52       0.92      1.00      0.96        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      1.00      1.00         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      0.90      0.95        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      0.95      0.97        20\n",
      "          59       0.83      1.00      0.91        10\n",
      "          60       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.95       750\n",
      "   macro avg       0.95      0.95      0.95       750\n",
      "weighted avg       0.95      0.95      0.95       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_11_speaker_ID_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('clf', SVC(C=100, gamma=0.001, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_11_speaker_ID_pipe.fit(l11_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_11_speaker_ID_pipe.predict(l11_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_11_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96, 'layer_10_speaker_ID_pipe': 0.97, 'layer_11_speaker_ID_pipe': 0.95}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.89      0.89      0.89        36\n",
      "        23.0       0.77      0.87      0.82        71\n",
      "        24.0       0.91      0.93      0.92        46\n",
      "        25.0       0.92      0.84      0.87        79\n",
      "        26.0       0.85      0.91      0.88       115\n",
      "        27.0       0.95      0.85      0.90        81\n",
      "        28.0       0.95      0.87      0.90        60\n",
      "        29.0       0.98      0.96      0.97        45\n",
      "        30.0       0.90      0.96      0.93        48\n",
      "        31.0       0.86      0.94      0.90        65\n",
      "        32.0       1.00      0.82      0.90        11\n",
      "        33.0       0.97      0.93      0.95        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       0.92      0.86      0.89        14\n",
      "        61.0       1.00      0.95      0.97        19\n",
      "\n",
      "    accuracy                           0.90       750\n",
      "   macro avg       0.93      0.91      0.92       750\n",
      "weighted avg       0.90      0.90      0.90       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_11_age_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('clf', SVC(C=100, gamma=0.001, kernel = 'rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_11_age_pipe.fit(l11_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_11_age_pipe.predict(l11_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_11_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94, 'layer_10_speaker_age_pipe': 0.94, 'layer_11_speaker_age_pipe': 0.9}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      1.00      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_11_gender_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('clf', SVC(C=100, gamma=0.001, kernel = 'rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_11_gender_pipe.fit(l11_train_X, train_speaker_genders)\n",
    "report = classification_report(valid_speaker_genders, layer_11_gender_pipe.predict(l11_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_gender_preds_confidences['layer_11_speaker_gender_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0, 'layer_8_speaker_gender_pipe': 1.0, 'layer_9_speaker_gender_pipe': 1.0, 'layer_10_speaker_gender_pipe': 1.0, 'layer_11_speaker_gender_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        21\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       0.89      0.93      0.91        27\n",
      "           3       0.89      1.00      0.94         8\n",
      "           4       0.87      0.87      0.87        15\n",
      "           5       0.83      0.91      0.87        11\n",
      "           6       0.96      0.98      0.97       532\n",
      "           7       0.96      0.84      0.90        32\n",
      "           8       1.00      0.84      0.91        19\n",
      "           9       0.93      0.76      0.84        17\n",
      "          10       1.00      0.90      0.95        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       1.00      0.81      0.89        26\n",
      "          13       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.95       750\n",
      "   macro avg       0.95      0.90      0.92       750\n",
      "weighted avg       0.96      0.95      0.95       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_11_accent_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('clf', SVC(C=100, gamma=0.001, kernel = 'rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_11_accent_pipe.fit(l11_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_11_accent_pipe.predict(l11_valid_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_accent_preds_confidences['layer_11_speaker_accent_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99, 'layer_8_speaker_accent_pipe': 1.0, 'layer_9_speaker_accent_pipe': 1.0, 'layer_10_speaker_accent_pipe': 1.0, 'layer_11_speaker_accent_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.170378</td>\n",
       "      <td>0.237555</td>\n",
       "      <td>0.043467</td>\n",
       "      <td>0.098314</td>\n",
       "      <td>0.319561</td>\n",
       "      <td>-0.034903</td>\n",
       "      <td>0.273422</td>\n",
       "      <td>-0.181296</td>\n",
       "      <td>-0.130003</td>\n",
       "      <td>-0.050533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.599428</td>\n",
       "      <td>-0.040149</td>\n",
       "      <td>-0.039554</td>\n",
       "      <td>0.220258</td>\n",
       "      <td>0.253446</td>\n",
       "      <td>-0.029713</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.116804</td>\n",
       "      <td>0.272363</td>\n",
       "      <td>-0.053761</td>\n",
       "      <td>0.089773</td>\n",
       "      <td>0.272276</td>\n",
       "      <td>0.011914</td>\n",
       "      <td>0.208539</td>\n",
       "      <td>-0.223243</td>\n",
       "      <td>-0.353159</td>\n",
       "      <td>0.125565</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376731</td>\n",
       "      <td>-0.116969</td>\n",
       "      <td>-0.023879</td>\n",
       "      <td>0.376468</td>\n",
       "      <td>0.093542</td>\n",
       "      <td>0.012601</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.259977</td>\n",
       "      <td>0.373373</td>\n",
       "      <td>-0.266022</td>\n",
       "      <td>0.279235</td>\n",
       "      <td>0.392939</td>\n",
       "      <td>-0.095564</td>\n",
       "      <td>0.147236</td>\n",
       "      <td>-0.284521</td>\n",
       "      <td>-0.241698</td>\n",
       "      <td>0.023308</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503207</td>\n",
       "      <td>0.012953</td>\n",
       "      <td>-0.043270</td>\n",
       "      <td>0.252754</td>\n",
       "      <td>0.350915</td>\n",
       "      <td>-0.056625</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.124358</td>\n",
       "      <td>0.272876</td>\n",
       "      <td>-0.239626</td>\n",
       "      <td>0.192322</td>\n",
       "      <td>0.607384</td>\n",
       "      <td>0.179180</td>\n",
       "      <td>0.202379</td>\n",
       "      <td>-0.206914</td>\n",
       "      <td>0.095205</td>\n",
       "      <td>-0.027455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225356</td>\n",
       "      <td>-0.162062</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.291056</td>\n",
       "      <td>0.123154</td>\n",
       "      <td>-0.013536</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050589</td>\n",
       "      <td>0.362734</td>\n",
       "      <td>-0.046245</td>\n",
       "      <td>0.067255</td>\n",
       "      <td>0.329067</td>\n",
       "      <td>-0.012923</td>\n",
       "      <td>0.098053</td>\n",
       "      <td>-0.187970</td>\n",
       "      <td>-0.309667</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.588841</td>\n",
       "      <td>0.013616</td>\n",
       "      <td>-0.029236</td>\n",
       "      <td>0.365545</td>\n",
       "      <td>0.111253</td>\n",
       "      <td>-0.065507</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.170378   0.237555   0.043467   0.098314   0.319561  -0.034903   \n",
       "1   0.116804   0.272363  -0.053761   0.089773   0.272276   0.011914   \n",
       "2   0.259977   0.373373  -0.266022   0.279235   0.392939  -0.095564   \n",
       "3   0.124358   0.272876  -0.239626   0.192322   0.607384   0.179180   \n",
       "4   0.050589   0.362734  -0.046245   0.067255   0.329067  -0.012923   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0   0.273422  -0.181296  -0.130003   -0.050533  ...    -0.599428    -0.040149   \n",
       "1   0.208539  -0.223243  -0.353159    0.125565  ...    -0.376731    -0.116969   \n",
       "2   0.147236  -0.284521  -0.241698    0.023308  ...    -0.503207     0.012953   \n",
       "3   0.202379  -0.206914   0.095205   -0.027455  ...    -0.225356    -0.162062   \n",
       "4   0.098053  -0.187970  -0.309667    0.006264  ...    -0.588841     0.013616   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0    -0.039554     0.220258     0.253446    -0.029713       45      NaN   \n",
       "1    -0.023879     0.376468     0.093542     0.012601       45      NaN   \n",
       "2    -0.043270     0.252754     0.350915    -0.056625       45      NaN   \n",
       "3     0.007676     0.291056     0.123154    -0.013536       45      NaN   \n",
       "4    -0.029236     0.365545     0.111253    -0.065507       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_train_df = pd.read_csv(layer_12_train_csv_file_path)\n",
    "l12_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l12_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean: 28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l12_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean:\", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l12_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "l12_train_X = l12_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "l12_valid_df = pd.read_csv(layer_12_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l12_valid_df.shape)\n",
    "print(\"null values row count: \", l12_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l12_valid_df.isnull().sum().sum() / l12_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l12_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l12_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l12_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "l12_valid_X = l12_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      1.00      0.87        13\n",
      "           2       0.86      0.67      0.75         9\n",
      "           3       0.79      0.92      0.85        12\n",
      "           4       0.94      0.94      0.94        16\n",
      "           5       0.93      0.78      0.85        18\n",
      "           6       0.89      0.89      0.89         9\n",
      "           7       0.84      0.94      0.89        17\n",
      "           8       0.92      0.86      0.89        14\n",
      "           9       0.91      0.91      0.91        11\n",
      "          10       0.88      0.88      0.88         8\n",
      "          11       0.82      0.95      0.88        19\n",
      "          12       0.88      1.00      0.93         7\n",
      "          13       0.89      0.73      0.80        11\n",
      "          14       0.93      0.93      0.93        15\n",
      "          15       0.87      0.76      0.81        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       1.00      0.93      0.96        14\n",
      "          18       1.00      0.84      0.91        19\n",
      "          19       0.92      0.73      0.81        15\n",
      "          20       1.00      0.73      0.84        11\n",
      "          21       1.00      0.93      0.96        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.83      1.00      0.91        10\n",
      "          24       0.92      0.92      0.92        12\n",
      "          25       0.92      1.00      0.96        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.89      1.00      0.94        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       0.88      0.93      0.90        15\n",
      "          30       1.00      0.88      0.93         8\n",
      "          31       0.92      0.92      0.92        12\n",
      "          32       0.80      0.89      0.84         9\n",
      "          33       0.67      1.00      0.80         6\n",
      "          34       0.91      0.77      0.83        13\n",
      "          35       0.64      0.88      0.74         8\n",
      "          36       0.89      0.94      0.92        18\n",
      "          37       1.00      0.81      0.90        16\n",
      "          38       0.83      0.91      0.87        11\n",
      "          39       0.77      0.77      0.77        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       0.80      0.80      0.80        10\n",
      "          42       0.79      0.92      0.85        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       0.90      0.95      0.92        19\n",
      "          45       1.00      0.93      0.96        14\n",
      "          46       0.92      1.00      0.96        11\n",
      "          47       1.00      0.88      0.93         8\n",
      "          48       1.00      0.94      0.97        17\n",
      "          49       0.86      0.92      0.89        13\n",
      "          50       0.92      0.92      0.92        13\n",
      "          51       0.86      0.75      0.80         8\n",
      "          52       0.85      1.00      0.92        11\n",
      "          53       0.93      0.93      0.93        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       0.80      1.00      0.89         8\n",
      "          56       1.00      0.70      0.82        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       0.95      1.00      0.98        20\n",
      "          59       0.91      1.00      0.95        10\n",
      "          60       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.90       750\n",
      "   macro avg       0.91      0.90      0.90       750\n",
      "weighted avg       0.91      0.90      0.90       750\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "layer_12_speaker_ID_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('SFM_LR', SelectFromModel(LogisticRegression(C=0.01, penalty='l1', solver='liblinear', class_weight='balanced'))),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "layer_12_speaker_ID_pipe.fit(l12_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_12_speaker_ID_pipe.predict(l12_valid_X))\n",
    "print(report)\n",
    "\n",
    "# The above pipeline as a text\n",
    "# KNeighborsClassifier(weights='distance', p=2, n_neighbors=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_12_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96, 'layer_10_speaker_ID_pipe': 0.97, 'layer_11_speaker_ID_pipe': 0.95, 'layer_12_speaker_ID_pipe': 0.9}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.78      0.89      0.83        36\n",
      "        23.0       0.72      0.79      0.75        71\n",
      "        24.0       0.73      0.78      0.76        46\n",
      "        25.0       0.73      0.68      0.71        79\n",
      "        26.0       0.79      0.78      0.79       115\n",
      "        27.0       0.75      0.70      0.73        81\n",
      "        28.0       0.72      0.70      0.71        60\n",
      "        29.0       0.86      0.82      0.84        45\n",
      "        30.0       0.79      0.85      0.82        48\n",
      "        31.0       0.72      0.71      0.71        65\n",
      "        32.0       0.90      0.82      0.86        11\n",
      "        33.0       0.90      0.90      0.90        30\n",
      "        34.0       0.85      1.00      0.92        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       0.92      0.79      0.85        14\n",
      "        61.0       0.94      0.89      0.92        19\n",
      "\n",
      "    accuracy                           0.78       750\n",
      "   macro avg       0.83      0.82      0.83       750\n",
      "weighted avg       0.78      0.78      0.78       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_12_speaker_ages_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', SVC(class_weight=\"balanced\", kernel='linear'))\n",
    "])\n",
    "layer_12_speaker_ages_pipe.fit(l12_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_12_speaker_ages_pipe.predict(l12_valid_X))\n",
    "print(report)\n",
    "\n",
    "# The above pipeline as a text\n",
    "# KNeighborsClassifier(weights='uniform', p=2, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_12_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94, 'layer_10_speaker_age_pipe': 0.94, 'layer_11_speaker_age_pipe': 0.9, 'layer_12_speaker_age_pipe': 0.78}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97       142\n",
      "           1       1.00      0.99      0.99       608\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       0.97      0.98      0.98       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "layer_12_speaker_gender_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "layer_12_speaker_gender_pipe.fit(l12_train_X, train_speaker_genders)\n",
    "report = classification_report(valid_speaker_genders, layer_12_speaker_gender_pipe.predict(l12_valid_X))\n",
    "print(report)\n",
    "\n",
    "# The above pipeline as a text\n",
    "# PCA(n_components=0.95) -> LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l2', C=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_gender_preds_confidences['layer_12_speaker_gender_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0, 'layer_8_speaker_gender_pipe': 1.0, 'layer_9_speaker_gender_pipe': 1.0, 'layer_10_speaker_gender_pipe': 1.0, 'layer_11_speaker_gender_pipe': 1.0, 'layer_12_speaker_gender_pipe': 0.99}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.95      0.77        21\n",
      "           1       0.92      1.00      0.96        11\n",
      "           2       0.57      0.93      0.70        27\n",
      "           3       0.78      0.88      0.82         8\n",
      "           4       0.69      0.73      0.71        15\n",
      "           5       0.67      0.73      0.70        11\n",
      "           6       0.95      0.91      0.93       532\n",
      "           7       0.86      0.78      0.82        32\n",
      "           8       1.00      0.63      0.77        19\n",
      "           9       0.93      0.76      0.84        17\n",
      "          10       0.70      0.70      0.70        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       0.70      0.81      0.75        26\n",
      "          13       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.89       750\n",
      "   macro avg       0.81      0.82      0.81       750\n",
      "weighted avg       0.90      0.89      0.89       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_12_speaker_accent_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', SVC(class_weight=\"balanced\", kernel='linear'))\n",
    "])\n",
    "layer_12_speaker_accent_pipe.fit(l12_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_12_speaker_accent_pipe.predict(l12_valid_X)))\n",
    "\n",
    "# The above pipeline as a text\n",
    "# PCA(n_components=0.99) -> KNeighborsClassifier(weights='uniform', p=3, n_neighbors=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_accent_preds_confidences['layer_12_speaker_accent_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99, 'layer_8_speaker_accent_pipe': 1.0, 'layer_9_speaker_accent_pipe': 1.0, 'layer_10_speaker_accent_pipe': 1.0, 'layer_11_speaker_accent_pipe': 1.0, 'layer_12_speaker_accent_pipe': 0.99}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for speaker_IDs in validation dataset from each layer model\n",
    "layer_7_speaker_ID_predictions = layer_7_speaker_ID_pipe.predict(l7_valid_X)\n",
    "layer_8_speaker_ID_predictions = layer_8_speaker_ID_pipe.predict(l8_valid_X)\n",
    "layer_9_speaker_ID_predictions = layer_9_speaker_ID_pipe.predict(l9_valid_X)\n",
    "layer_10_speaker_ID_predictions = layer_10_speaker_ID_pipe.predict(l10_valid_X)\n",
    "layer_11_speaker_ID_predictions = layer_11_speaker_ID_pipe.predict(l11_valid_X)\n",
    "layer_12_speaker_ID_predictions = layer_12_speaker_ID_pipe.predict(l12_valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions from all layers into a 2D array\n",
    "all_predictions = np.vstack([\n",
    "    layer_7_speaker_ID_predictions,\n",
    "    layer_8_speaker_ID_predictions,\n",
    "    layer_9_speaker_ID_predictions,\n",
    "    layer_10_speaker_ID_predictions,\n",
    "    layer_11_speaker_ID_predictions,\n",
    "    layer_12_speaker_ID_predictions\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96, 'layer_10_speaker_ID_pipe': 0.97, 'layer_11_speaker_ID_pipe': 0.95, 'layer_12_speaker_ID_pipe': 0.9}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750,)\n"
     ]
    }
   ],
   "source": [
    "print(layer_7_speaker_ID_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 750)\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using just the majority prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.94      1.00      0.97        16\n",
      "           5       1.00      0.94      0.97        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       0.94      0.94      0.94        17\n",
      "           8       1.00      0.86      0.92        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       0.88      1.00      0.93         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      0.88      0.94        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      1.00      0.97        14\n",
      "          18       1.00      0.95      0.97        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       0.93      1.00      0.97        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.86      1.00      0.92        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.94      1.00      0.97        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      0.88      0.93         8\n",
      "          31       0.92      0.92      0.92        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.85      0.92        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       0.92      1.00      0.96        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       0.93      1.00      0.97        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       0.89      1.00      0.94         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      1.00      1.00        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       0.91      1.00      0.95        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.98      0.98      0.97       750\n",
      "weighted avg       0.98      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 0.93, 0.96, 0.97, 0.95, 0.9])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ID_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 4:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       1.00      1.00      1.00        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      1.00      1.00        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      1.00      0.97        14\n",
      "          18       1.00      1.00      1.00        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       1.00      0.93      0.96        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       1.00      1.00      1.00        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      1.00      1.00        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      1.00      1.00         8\n",
      "          31       1.00      1.00      1.00        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       1.00      1.00      1.00         6\n",
      "          34       1.00      1.00      1.00        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       1.00      1.00      1.00        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       1.00      1.00      1.00        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      1.00      1.00        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       0.99      0.99      0.99       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99, 0.93, 0.96, 0.97, 0.95, 0.9]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(speaker_ID_preds_confidences.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 3:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       1.00      0.94      0.97        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       0.94      0.94      0.94        17\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      0.94      0.97        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      1.00      0.97        14\n",
      "          18       1.00      1.00      1.00        19\n",
      "          19       0.93      0.93      0.93        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       1.00      0.93      0.96        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.92      1.00      0.96        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.94      1.00      0.97        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      1.00      1.00         8\n",
      "          31       0.92      0.92      0.92        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.92      0.96        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       0.92      1.00      0.96        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      1.00      1.00        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.98       750\n",
      "   macro avg       0.98      0.98      0.98       750\n",
      "weighted avg       0.98      0.98      0.98       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking based ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_7_train_speaker_ID_predictions = layer_7_speaker_ID_pipe.predict(l7_train_X)\n",
    "layer_8_train_speaker_ID_predictions = layer_8_speaker_ID_pipe.predict(l8_train_X)\n",
    "layer_9_train_speaker_ID_predictions = layer_9_speaker_ID_pipe.predict(l9_train_X)\n",
    "layer_10_train_speaker_ID_predictions = layer_10_speaker_ID_pipe.predict(l10_train_X)\n",
    "layer_11_train_speaker_ID_predictions = layer_11_speaker_ID_pipe.predict(l11_train_X)\n",
    "layer_12_train_speaker_ID_predictions = layer_12_speaker_ID_pipe.predict(l12_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_meta = [\n",
    "    layer_7_train_speaker_ID_predictions,\n",
    "    layer_8_train_speaker_ID_predictions,\n",
    "    layer_9_train_speaker_ID_predictions,\n",
    "    layer_10_train_speaker_ID_predictions,\n",
    "    layer_11_train_speaker_ID_predictions,\n",
    "    layer_12_train_speaker_ID_predictions\n",
    "]\n",
    "\n",
    "X_meta = np.vstack(X_meta).T  # Transpose the matrix to have predictions as columns\n",
    "\n",
    "X_train_meta, X_valid_meta, y_train_meta, y_valid_meta = train_test_split(X_meta, train_speaker_IDs, test_size=0.2, random_state=42)\n",
    "\n",
    "meta_model = SVC(C=40, class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28520, 6)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=40, class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=40, class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=40, class_weight='balanced')"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacked Ensemble (using hold-out method): 0.9975455820476858\n"
     ]
    }
   ],
   "source": [
    "meta_predictions = meta_model.predict(X_valid_meta)\n",
    "stacked_ensemble_accuracy = accuracy_score(y_valid_meta, meta_predictions)\n",
    "\n",
    "print(\"Accuracy of Stacked Ensemble (using hold-out method):\", stacked_ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_data_from_validation_dataset = [\n",
    "    layer_7_speaker_ID_predictions,\n",
    "    layer_8_speaker_ID_predictions,\n",
    "    layer_9_speaker_ID_predictions,\n",
    "    layer_10_speaker_ID_predictions,\n",
    "    layer_11_speaker_ID_predictions,\n",
    "    layer_12_speaker_ID_predictions\n",
    "]\n",
    "\n",
    "X_meta_data_from_validation_dataset = np.vstack(X_meta_data_from_validation_dataset).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       1.00      0.89      0.94         9\n",
      "           3       1.00      0.92      0.96        12\n",
      "           4       0.79      0.94      0.86        16\n",
      "           5       0.93      0.78      0.85        18\n",
      "           6       0.80      0.89      0.84         9\n",
      "           7       1.00      0.94      0.97        17\n",
      "           8       1.00      0.79      0.88        14\n",
      "           9       0.91      0.91      0.91        11\n",
      "          10       0.89      1.00      0.94         8\n",
      "          11       1.00      0.89      0.94        19\n",
      "          12       0.88      1.00      0.93         7\n",
      "          13       0.90      0.82      0.86        11\n",
      "          14       0.76      0.87      0.81        15\n",
      "          15       0.93      0.82      0.87        17\n",
      "          16       0.82      1.00      0.90        14\n",
      "          17       0.80      0.86      0.83        14\n",
      "          18       0.94      0.84      0.89        19\n",
      "          19       0.92      0.73      0.81        15\n",
      "          20       1.00      0.73      0.84        11\n",
      "          21       0.81      0.93      0.87        14\n",
      "          22       0.85      1.00      0.92        11\n",
      "          23       0.83      1.00      0.91        10\n",
      "          24       0.91      0.83      0.87        12\n",
      "          25       0.83      0.91      0.87        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.94      0.94      0.94        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       0.81      0.87      0.84        15\n",
      "          30       1.00      0.88      0.93         8\n",
      "          31       0.83      0.83      0.83        12\n",
      "          32       0.67      0.89      0.76         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       0.85      0.85      0.85        13\n",
      "          35       0.73      1.00      0.84         8\n",
      "          36       1.00      0.94      0.97        18\n",
      "          37       0.85      0.69      0.76        16\n",
      "          38       0.82      0.82      0.82        11\n",
      "          39       0.91      0.77      0.83        13\n",
      "          40       0.61      0.92      0.73        12\n",
      "          41       0.80      0.80      0.80        10\n",
      "          42       1.00      0.92      0.96        12\n",
      "          43       0.75      1.00      0.86        12\n",
      "          44       0.95      0.95      0.95        19\n",
      "          45       1.00      0.86      0.92        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      0.88      0.93         8\n",
      "          48       0.84      0.94      0.89        17\n",
      "          49       1.00      0.92      0.96        13\n",
      "          50       1.00      0.92      0.96        13\n",
      "          51       1.00      0.88      0.93         8\n",
      "          52       1.00      0.91      0.95        11\n",
      "          53       0.94      1.00      0.97        15\n",
      "          54       0.78      0.78      0.78         9\n",
      "          55       0.62      1.00      0.76         8\n",
      "          56       1.00      0.70      0.82        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      0.95      0.97        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.89       750\n",
      "   macro avg       0.90      0.90      0.89       750\n",
      "weighted avg       0.91      0.89      0.89       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, meta_model.predict(X_meta_data_from_validation_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 0.93, 0.96, 0.97, 0.95, 0.9])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ID_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(SVC(probability=True, class_weight=\"balanced\"), param_grid, cv=1, scoring='accuracy', verbose=3, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to your data\n",
    "grid_search.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best__C = PUT THE BEST ONE HERE\n",
    "# best__gamma = PUT THE BEST ONE HERE\n",
    "\n",
    "best_meta_model = SVC(C=best__C, kernel=best__kernel, gamma=best__gamma, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for speaker_IDs in validation dataset from each layer model\n",
    "layer_7_speaker_age_valid_predictions = layer_7_speaker_age_pipe.predict(l7_valid_X)\n",
    "layer_8_speaker_age_valid_predictions = layer_8_age_pipe.predict(l8_valid_X)\n",
    "layer_9_speaker_age_valid_predictions = layer_9_age_pipe.predict(l9_valid_X)\n",
    "layer_10_speaker_age_valid_predictions = layer_10_age_pipe.predict(l10_valid_X)\n",
    "layer_11_speaker_age_valid_predictions = layer_11_age_pipe.predict(l11_valid_X)\n",
    "layer_12_speaker_age_valid_predictions = layer_12_speaker_ages_pipe.predict(l12_valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions from all layers into a 2D array\n",
    "all_predictions = np.vstack([\n",
    "    layer_7_speaker_age_valid_predictions,\n",
    "    layer_8_speaker_age_valid_predictions,\n",
    "    layer_9_speaker_age_valid_predictions,\n",
    "    layer_10_speaker_age_valid_predictions,\n",
    "    layer_11_speaker_age_valid_predictions,\n",
    "    layer_12_speaker_age_valid_predictions\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94, 'layer_10_speaker_age_pipe': 0.94, 'layer_11_speaker_age_pipe': 0.9, 'layer_12_speaker_age_pipe': 0.78}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750,)\n"
     ]
    }
   ],
   "source": [
    "print(layer_7_speaker_age_valid_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 750)\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using just the majority prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_age_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    # print(predictions, majority_prediction)\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.95      1.00      0.97        36\n",
      "        23.0       0.92      0.96      0.94        71\n",
      "        24.0       0.96      0.98      0.97        46\n",
      "        25.0       1.00      0.94      0.97        79\n",
      "        26.0       0.95      0.99      0.97       115\n",
      "        27.0       0.97      0.96      0.97        81\n",
      "        28.0       0.98      0.95      0.97        60\n",
      "        29.0       1.00      1.00      1.00        45\n",
      "        30.0       1.00      0.98      0.99        48\n",
      "        31.0       0.96      0.98      0.97        65\n",
      "        32.0       1.00      1.00      1.00        11\n",
      "        33.0       1.00      1.00      1.00        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.86      0.92        14\n",
      "        61.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.98      0.97      0.97       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_ages, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.97, 0.91, 0.94, 0.94, 0.9, 0.78])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_age_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_age_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 4:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.97      1.00      0.99        36\n",
      "        23.0       0.96      0.97      0.97        71\n",
      "        24.0       0.96      0.98      0.97        46\n",
      "        25.0       0.99      0.94      0.96        79\n",
      "        26.0       0.96      0.99      0.97       115\n",
      "        27.0       0.99      0.94      0.96        81\n",
      "        28.0       0.97      0.98      0.98        60\n",
      "        29.0       1.00      1.00      1.00        45\n",
      "        30.0       1.00      0.98      0.99        48\n",
      "        31.0       0.93      0.98      0.96        65\n",
      "        32.0       1.00      1.00      1.00        11\n",
      "        33.0       1.00      1.00      1.00        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.93      0.96        14\n",
      "        61.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.98      0.97      0.98       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_ages, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.97, 0.91, 0.94, 0.94, 0.9, 0.78]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(speaker_age_preds_confidences.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_age_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 3:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.95      1.00      0.97        36\n",
      "        23.0       0.92      0.97      0.95        71\n",
      "        24.0       0.96      0.98      0.97        46\n",
      "        25.0       1.00      0.94      0.97        79\n",
      "        26.0       0.96      0.99      0.97       115\n",
      "        27.0       0.99      0.96      0.97        81\n",
      "        28.0       0.98      0.97      0.97        60\n",
      "        29.0       1.00      1.00      1.00        45\n",
      "        30.0       1.00      0.98      0.99        48\n",
      "        31.0       0.96      0.98      0.97        65\n",
      "        32.0       1.00      1.00      1.00        11\n",
      "        33.0       1.00      1.00      1.00        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.86      0.92        14\n",
      "        61.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.98      0.97      0.97       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_ages, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking based ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_7_train_speaker_age_predictions = layer_7_speaker_age_pipe.predict(l7_train_X)\n",
    "layer_8_train_speaker_age_predictions = layer_8_age_pipe.predict(l8_train_X)\n",
    "layer_9_train_speaker_age_predictions = layer_9_age_pipe.predict(l9_train_X)\n",
    "layer_10_train_speaker_age_predictions = layer_10_age_pipe.predict(l10_train_X)\n",
    "layer_11_train_speaker_age_predictions = layer_11_age_pipe.predict(l11_train_X)\n",
    "layer_12_train_speaker_age_predictions = layer_12_speaker_ages_pipe.predict(l12_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_meta shape: (28520, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_meta = [\n",
    "    layer_7_train_speaker_age_predictions,\n",
    "    layer_8_train_speaker_age_predictions,\n",
    "    layer_9_train_speaker_age_predictions,\n",
    "    layer_10_train_speaker_age_predictions,\n",
    "    layer_11_train_speaker_age_predictions,\n",
    "    layer_12_train_speaker_age_predictions\n",
    "]\n",
    "\n",
    "X_meta = np.vstack(X_meta).T  # Transpose the matrix to have predictions as columns\n",
    "print(\"X_meta shape:\", X_meta.shape)\n",
    "\n",
    "X_train_meta, X_valid_meta, y_train_meta, y_valid_meta = train_test_split(X_meta, train_speaker_ages, test_size=0.2, random_state=42)\n",
    "\n",
    "meta_model = SVC(class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(class_weight='balanced')"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacked Ensemble (using hold-out method): 0.9992987377279102\n"
     ]
    }
   ],
   "source": [
    "meta_predictions = meta_model.predict(X_valid_meta)\n",
    "stacked_ensemble_accuracy = accuracy_score(y_valid_meta, meta_predictions)\n",
    "\n",
    "print(\"Accuracy of Stacked Ensemble (using hold-out method):\", stacked_ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_data_from_validation_dataset = [\n",
    "    layer_7_speaker_age_valid_predictions,\n",
    "    layer_8_speaker_age_valid_predictions,\n",
    "    layer_9_speaker_age_valid_predictions,\n",
    "    layer_10_speaker_age_valid_predictions,\n",
    "    layer_11_speaker_age_valid_predictions,\n",
    "    layer_12_speaker_age_valid_predictions\n",
    "]\n",
    "\n",
    "X_meta_data_from_validation_dataset = np.vstack(X_meta_data_from_validation_dataset).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       1.00      0.78      0.88        36\n",
      "        23.0       0.84      0.92      0.88        71\n",
      "        24.0       0.84      0.89      0.86        46\n",
      "        25.0       0.87      0.86      0.87        79\n",
      "        26.0       0.92      0.95      0.93       115\n",
      "        27.0       0.85      0.84      0.84        81\n",
      "        28.0       0.76      0.87      0.81        60\n",
      "        29.0       0.83      0.89      0.86        45\n",
      "        30.0       0.91      0.88      0.89        48\n",
      "        31.0       0.92      0.92      0.92        65\n",
      "        32.0       0.55      0.55      0.55        11\n",
      "        33.0       1.00      0.77      0.87        30\n",
      "        34.0       0.83      0.91      0.87        11\n",
      "        35.0       1.00      0.73      0.84        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.71      0.83        14\n",
      "        61.0       0.95      1.00      0.97        19\n",
      "\n",
      "    accuracy                           0.88       750\n",
      "   macro avg       0.89      0.85      0.86       750\n",
      "weighted avg       0.88      0.88      0.88       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_ages, meta_model.predict(X_meta_data_from_validation_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 0.93, 0.96, 0.97, 0.95, 0.9])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ID_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(probability=True, class_weight=\"balanced\"), param_grid, cv=3, scoring='accuracy', verbose=3, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best__C = \n",
    "best__gamma =\n",
    "\n",
    "best_meta_model = SVC(C=best__C, kernel=best__kernel, gamma=best__gamma, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for speaker_IDs in validation dataset from each layer model\n",
    "layer_7_speaker_gender_valid_predictions = layer_7_speaker_gender_pipe.predict(l7_valid_X)\n",
    "layer_8_speaker_gender_valid_predictions = layer_8_gender_pipe.predict(l8_valid_X)\n",
    "layer_9_speaker_gender_valid_predictions = layer_9_gender_pipe.predict(l9_valid_X)\n",
    "layer_10_speaker_gender_valid_predictions = layer_10_gender_pipe.predict(l10_valid_X)\n",
    "layer_11_speaker_gender_valid_predictions = layer_11_gender_pipe.predict(l11_valid_X)\n",
    "layer_12_speaker_gender_valid_predictions = layer_12_speaker_gender_pipe.predict(l12_valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions from all layers into a 2D array\n",
    "all_predictions = np.vstack([\n",
    "    layer_7_speaker_gender_valid_predictions,\n",
    "    layer_8_speaker_gender_valid_predictions,\n",
    "    layer_9_speaker_gender_valid_predictions,\n",
    "    layer_10_speaker_gender_valid_predictions,\n",
    "    layer_11_speaker_gender_valid_predictions,\n",
    "    layer_12_speaker_gender_valid_predictions\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0, 'layer_8_speaker_gender_pipe': 1.0, 'layer_9_speaker_gender_pipe': 1.0, 'layer_10_speaker_gender_pipe': 1.0, 'layer_11_speaker_gender_pipe': 1.0, 'layer_12_speaker_gender_pipe': 0.99}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750,)\n"
     ]
    }
   ],
   "source": [
    "print(layer_7_speaker_gender_valid_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 750)\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using just the majority prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_gender_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    # print(predictions, majority_prediction)\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      1.00      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_genders, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1.0, 1.0, 1.0, 1.0, 1.0, 0.99])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_gender_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_gender_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 4:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      1.00      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_genders, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 0.99]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(speaker_gender_preds_confidences.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_gender_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 3:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      1.00      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_genders, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking based ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_7_train_speaker_gender_predictions = layer_7_speaker_gender_pipe.predict(l7_train_X)\n",
    "layer_8_train_speaker_gender_predictions = layer_8_gender_pipe.predict(l8_train_X)\n",
    "layer_9_train_speaker_gender_predictions = layer_9_gender_pipe.predict(l9_train_X)\n",
    "layer_10_train_speaker_gender_predictions = layer_10_gender_pipe.predict(l10_train_X)\n",
    "layer_11_train_speaker_gender_predictions = layer_11_gender_pipe.predict(l11_train_X)\n",
    "layer_12_train_speaker_gender_predictions = layer_12_speaker_gender_pipe.predict(l12_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_meta shape: (28520, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_meta = [\n",
    "    layer_7_train_speaker_gender_predictions,\n",
    "    layer_8_train_speaker_gender_predictions,\n",
    "    layer_9_train_speaker_gender_predictions,\n",
    "    layer_10_train_speaker_gender_predictions,\n",
    "    layer_11_train_speaker_gender_predictions,\n",
    "    layer_12_train_speaker_gender_predictions\n",
    "]\n",
    "\n",
    "X_meta = np.vstack(X_meta).T  # Transpose the matrix to have predictions as columns\n",
    "print(\"X_meta shape:\", X_meta.shape)\n",
    "\n",
    "X_train_meta, X_valid_meta, y_train_meta, y_valid_meta = train_test_split(X_meta, train_speaker_genders, test_size=0.2, random_state=42)\n",
    "\n",
    "meta_model = SVC(class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(class_weight='balanced')"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacked Ensemble (using hold-out method): 1.0\n"
     ]
    }
   ],
   "source": [
    "meta_predictions = meta_model.predict(X_valid_meta)\n",
    "stacked_ensemble_accuracy = accuracy_score(y_valid_meta, meta_predictions)\n",
    "\n",
    "print(\"Accuracy of Stacked Ensemble (using hold-out method):\", stacked_ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_data_from_validation_dataset = [\n",
    "    layer_7_speaker_gender_valid_predictions,\n",
    "    layer_8_speaker_gender_valid_predictions,\n",
    "    layer_9_speaker_gender_valid_predictions,\n",
    "    layer_10_speaker_gender_valid_predictions,\n",
    "    layer_11_speaker_gender_valid_predictions,\n",
    "    layer_12_speaker_gender_valid_predictions\n",
    "]\n",
    "\n",
    "X_meta_data_from_validation_dataset = np.vstack(X_meta_data_from_validation_dataset).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      0.99      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_genders, meta_model.predict(X_meta_data_from_validation_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1.0, 1.0, 1.0, 1.0, 1.0, 0.99])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_gender_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(probability=True, class_weight=\"balanced\"), param_grid, cv=3, scoring='accuracy', verbose=3, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best__C = \n",
    "best__gamma =\n",
    "\n",
    "best_meta_model = SVC(C=best__C, kernel=best__kernel, gamma=best__gamma, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for speaker_IDs in validation dataset from each layer model\n",
    "layer_7_speaker_accent_valid_predictions = layer_7_speaker_accent_pipe.predict(l7_valid_X)\n",
    "layer_8_speaker_accent_valid_predictions = layer_8_accent_pipe.predict(l8_valid_X)\n",
    "layer_9_speaker_accent_valid_predictions = layer_9_accent_pipe.predict(l9_valid_X)\n",
    "layer_10_speaker_accent_valid_predictions = layer_10_accent_pipe.predict(l10_valid_X)\n",
    "layer_11_speaker_accent_valid_predictions = layer_11_accent_pipe.predict(l11_valid_X)\n",
    "layer_12_speaker_accent_valid_predictions = layer_12_speaker_accent_pipe.predict(l12_valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions from all layers into a 2D array\n",
    "all_predictions = np.vstack([\n",
    "    layer_7_speaker_accent_valid_predictions,\n",
    "    layer_8_speaker_accent_valid_predictions,\n",
    "    layer_9_speaker_accent_valid_predictions,\n",
    "    layer_10_speaker_accent_valid_predictions,\n",
    "    layer_11_speaker_accent_valid_predictions,\n",
    "    layer_12_speaker_accent_valid_predictions\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99, 'layer_8_speaker_accent_pipe': 1.0, 'layer_9_speaker_accent_pipe': 1.0, 'layer_10_speaker_accent_pipe': 1.0, 'layer_11_speaker_accent_pipe': 1.0, 'layer_12_speaker_accent_pipe': 0.99}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750,)\n"
     ]
    }
   ],
   "source": [
    "print(layer_7_speaker_accent_valid_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 750)\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using just the majority prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_accent_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    # print(predictions, majority_prediction)\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       1.00      1.00      1.00        27\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       1.00      0.93      0.97        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.98      1.00      0.99       532\n",
      "           7       1.00      0.97      0.98        32\n",
      "           8       1.00      0.95      0.97        19\n",
      "           9       1.00      0.82      0.90        17\n",
      "          10       1.00      1.00      1.00        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       1.00      0.96      0.98        26\n",
      "          13       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       1.00      0.96      0.98       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_accents, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 1.0, 1.0, 1.0, 1.0, 0.99])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_accent_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_accent_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 4:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.96      1.00      0.98        27\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       1.00      0.87      0.93        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.98      1.00      0.99       532\n",
      "           7       1.00      0.88      0.93        32\n",
      "           8       1.00      0.89      0.94        19\n",
      "           9       1.00      0.82      0.90        17\n",
      "          10       1.00      1.00      1.00        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       0.96      0.96      0.96        26\n",
      "          13       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.98       750\n",
      "   macro avg       0.99      0.95      0.97       750\n",
      "weighted avg       0.98      0.98      0.98       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_accents, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99, 1.0, 1.0, 1.0, 1.0, 0.99]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(speaker_accent_preds_confidences.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_accent_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 3:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       1.00      1.00      1.00        27\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       1.00      0.93      0.97        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.99      1.00      0.99       532\n",
      "           7       1.00      0.97      0.98        32\n",
      "           8       1.00      0.95      0.97        19\n",
      "           9       1.00      0.82      0.90        17\n",
      "          10       1.00      1.00      1.00        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       0.96      0.96      0.96        26\n",
      "          13       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       1.00      0.96      0.98       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_accents, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking based ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_7_train_speaker_accent_predictions = layer_7_speaker_accent_pipe.predict(l7_train_X)\n",
    "layer_8_train_speaker_accent_predictions = layer_8_accent_pipe.predict(l8_train_X)\n",
    "layer_9_train_speaker_accent_predictions = layer_9_accent_pipe.predict(l9_train_X)\n",
    "layer_10_train_speaker_accent_predictions = layer_10_accent_pipe.predict(l10_train_X)\n",
    "layer_11_train_speaker_accent_predictions = layer_11_accent_pipe.predict(l11_train_X)\n",
    "layer_12_train_speaker_accent_predictions = layer_12_speaker_accent_pipe.predict(l12_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_meta shape: (28520, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_meta = [\n",
    "    layer_7_train_speaker_accent_predictions,\n",
    "    layer_8_train_speaker_accent_predictions,\n",
    "    layer_9_train_speaker_accent_predictions,\n",
    "    layer_10_train_speaker_accent_predictions,\n",
    "    layer_11_train_speaker_accent_predictions,\n",
    "    layer_12_train_speaker_accent_predictions\n",
    "]\n",
    "\n",
    "X_meta = np.vstack(X_meta).T  # Transpose the matrix to have predictions as columns\n",
    "print(\"X_meta shape:\", X_meta.shape)\n",
    "\n",
    "X_train_meta, X_valid_meta, y_train_meta, y_valid_meta = train_test_split(X_meta, train_speaker_accents, test_size=0.2, random_state=42)\n",
    "\n",
    "meta_model = SVC(class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(class_weight='balanced')"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacked Ensemble (using hold-out method): 0.998422159887798\n"
     ]
    }
   ],
   "source": [
    "meta_predictions = meta_model.predict(X_valid_meta)\n",
    "stacked_ensemble_accuracy = accuracy_score(y_valid_meta, meta_predictions)\n",
    "\n",
    "print(\"Accuracy of Stacked Ensemble (using hold-out method):\", stacked_ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_data_from_validation_dataset = [\n",
    "    layer_7_speaker_accent_valid_predictions,\n",
    "    layer_8_speaker_accent_valid_predictions,\n",
    "    layer_9_speaker_accent_valid_predictions,\n",
    "    layer_10_speaker_accent_valid_predictions,\n",
    "    layer_11_speaker_accent_valid_predictions,\n",
    "    layer_12_speaker_accent_valid_predictions\n",
    "]\n",
    "\n",
    "X_meta_data_from_validation_dataset = np.vstack(X_meta_data_from_validation_dataset).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        21\n",
      "           1       0.90      0.82      0.86        11\n",
      "           2       0.96      0.93      0.94        27\n",
      "           3       1.00      0.88      0.93         8\n",
      "           4       0.92      0.80      0.86        15\n",
      "           5       0.83      0.91      0.87        11\n",
      "           6       0.96      1.00      0.98       532\n",
      "           7       0.90      0.84      0.87        32\n",
      "           8       0.89      0.84      0.86        19\n",
      "           9       0.87      0.76      0.81        17\n",
      "          10       1.00      0.60      0.75        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       0.87      0.77      0.82        26\n",
      "          13       1.00      0.70      0.82        10\n",
      "\n",
      "    accuracy                           0.95       750\n",
      "   macro avg       0.94      0.83      0.88       750\n",
      "weighted avg       0.95      0.95      0.95       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_accents, meta_model.predict(X_meta_data_from_validation_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 0.93, 0.96, 0.97, 0.95, 0.9])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ID_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [1, 10, 100],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "    'gamma': [1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(class_weight=\"balanced\"), param_grid, cv=2, scoring='accuracy', verbose=4, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best__C = \n",
    "best__gamma =\n",
    "\n",
    "best_meta_model = SVC(C=best__C, kernel=best__kernel, gamma=best__gamma, class_weight='balanced')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
