{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np          # For mathematical calculations\n",
    "import matplotlib.pyplot as plt  # For plotting graphs\n",
    "# import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "layer_7_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\train.csv\"\n",
    "layer_7_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\valid.csv\"\n",
    "layer_7_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\test.csv\"\n",
    "\n",
    "layer_8_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer8\\\\train.csv\"\n",
    "layer_8_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer8\\\\valid.csv\"\n",
    "layer_8_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer8\\\\test.csv\"\n",
    "\n",
    "layer_9_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer9\\\\train.csv\"\n",
    "layer_9_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer9\\\\valid.csv\"\n",
    "layer_9_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer9\\\\test.csv\"\n",
    "\n",
    "layer_10_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer10\\\\train.csv\"\n",
    "layer_10_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer10\\\\valid.csv\"\n",
    "layer_10_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer10\\\\test.csv\"\n",
    "\n",
    "layer_11_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer11\\\\train.csv\"\n",
    "layer_11_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer11\\\\valid.csv\"\n",
    "layer_11_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer11\\\\test.csv\"\n",
    "\n",
    "layer_12_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer12\\\\train.csv\"\n",
    "layer_12_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer12\\\\valid.csv\"\n",
    "layer_12_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer12\\\\test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ID_preds_confidences = {}\n",
    "speaker_age_preds_confidences = {}\n",
    "speaker_gender_preds_confidences = {}\n",
    "speaker_accent_preds_confidences = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187868</td>\n",
       "      <td>-0.009268</td>\n",
       "      <td>0.039846</td>\n",
       "      <td>-0.102515</td>\n",
       "      <td>-0.109121</td>\n",
       "      <td>-0.079769</td>\n",
       "      <td>0.061295</td>\n",
       "      <td>-0.042890</td>\n",
       "      <td>0.099860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043647</td>\n",
       "      <td>0.061174</td>\n",
       "      <td>0.026771</td>\n",
       "      <td>-0.092734</td>\n",
       "      <td>-0.052383</td>\n",
       "      <td>-0.055526</td>\n",
       "      <td>0.029101</td>\n",
       "      <td>0.041857</td>\n",
       "      <td>-0.027872</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.122119</td>\n",
       "      <td>0.019015</td>\n",
       "      <td>0.134483</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>-0.157884</td>\n",
       "      <td>-0.033332</td>\n",
       "      <td>0.044334</td>\n",
       "      <td>-0.016869</td>\n",
       "      <td>-0.014088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179798</td>\n",
       "      <td>-0.027611</td>\n",
       "      <td>0.240023</td>\n",
       "      <td>-0.005648</td>\n",
       "      <td>-0.057537</td>\n",
       "      <td>0.023922</td>\n",
       "      <td>-0.038126</td>\n",
       "      <td>-0.015171</td>\n",
       "      <td>-0.006270</td>\n",
       "      <td>-0.003790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.079187</td>\n",
       "      <td>0.146195</td>\n",
       "      <td>-0.001316</td>\n",
       "      <td>-0.085596</td>\n",
       "      <td>-0.261093</td>\n",
       "      <td>-0.021620</td>\n",
       "      <td>0.268898</td>\n",
       "      <td>-0.103950</td>\n",
       "      <td>0.068976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075460</td>\n",
       "      <td>0.037346</td>\n",
       "      <td>0.124254</td>\n",
       "      <td>-0.369695</td>\n",
       "      <td>-0.056773</td>\n",
       "      <td>-0.028471</td>\n",
       "      <td>0.019971</td>\n",
       "      <td>-0.065683</td>\n",
       "      <td>-0.047366</td>\n",
       "      <td>-0.121744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.231196</td>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.215534</td>\n",
       "      <td>-0.210258</td>\n",
       "      <td>-0.158189</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.240254</td>\n",
       "      <td>-0.068112</td>\n",
       "      <td>-0.017550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0.141642</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>0.031547</td>\n",
       "      <td>-0.010762</td>\n",
       "      <td>-0.169036</td>\n",
       "      <td>-0.030963</td>\n",
       "      <td>0.086698</td>\n",
       "      <td>0.114194</td>\n",
       "      <td>-0.036775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.020412</td>\n",
       "      <td>0.063732</td>\n",
       "      <td>-0.065506</td>\n",
       "      <td>-0.089598</td>\n",
       "      <td>-0.130788</td>\n",
       "      <td>-0.018809</td>\n",
       "      <td>0.119304</td>\n",
       "      <td>-0.057494</td>\n",
       "      <td>0.094714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001969</td>\n",
       "      <td>-0.003025</td>\n",
       "      <td>-0.021156</td>\n",
       "      <td>-0.136993</td>\n",
       "      <td>-0.003615</td>\n",
       "      <td>0.036371</td>\n",
       "      <td>0.048715</td>\n",
       "      <td>-0.002688</td>\n",
       "      <td>-0.016957</td>\n",
       "      <td>-0.004331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   1   0.187868  -0.009268   0.039846  -0.102515  -0.109121  -0.079769   \n",
       "1   2   0.122119   0.019015   0.134483   0.007949  -0.157884  -0.033332   \n",
       "2   3   0.079187   0.146195  -0.001316  -0.085596  -0.261093  -0.021620   \n",
       "3   4   0.231196   0.021558   0.215534  -0.210258  -0.158189   0.041621   \n",
       "4   5   0.020412   0.063732  -0.065506  -0.089598  -0.130788  -0.018809   \n",
       "\n",
       "   feature_7  feature_8  feature_9  ...  feature_759  feature_760  \\\n",
       "0   0.061295  -0.042890   0.099860  ...     0.043647     0.061174   \n",
       "1   0.044334  -0.016869  -0.014088  ...    -0.179798    -0.027611   \n",
       "2   0.268898  -0.103950   0.068976  ...    -0.075460     0.037346   \n",
       "3   0.240254  -0.068112  -0.017550  ...     0.083334     0.141642   \n",
       "4   0.119304  -0.057494   0.094714  ...    -0.001969    -0.003025   \n",
       "\n",
       "   feature_761  feature_762  feature_763  feature_764  feature_765  \\\n",
       "0     0.026771    -0.092734    -0.052383    -0.055526     0.029101   \n",
       "1     0.240023    -0.005648    -0.057537     0.023922    -0.038126   \n",
       "2     0.124254    -0.369695    -0.056773    -0.028471     0.019971   \n",
       "3    -0.007219     0.031547    -0.010762    -0.169036    -0.030963   \n",
       "4    -0.021156    -0.136993    -0.003615     0.036371     0.048715   \n",
       "\n",
       "   feature_766  feature_767  feature_768  \n",
       "0     0.041857    -0.027872     0.099500  \n",
       "1    -0.015171    -0.006270    -0.003790  \n",
       "2    -0.065683    -0.047366    -0.121744  \n",
       "3     0.086698     0.114194    -0.036775  \n",
       "4    -0.002688    -0.016957    -0.004331  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(layer_7_test_csv_file_path)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID               int64\n",
       "feature_1      float64\n",
       "feature_2      float64\n",
       "feature_3      float64\n",
       "feature_4      float64\n",
       "                ...   \n",
       "feature_764    float64\n",
       "feature_765    float64\n",
       "feature_766    float64\n",
       "feature_767    float64\n",
       "feature_768    float64\n",
       "Length: 769, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get type of each column\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             False\n",
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "               ...  \n",
       "feature_764    False\n",
       "feature_765    False\n",
       "feature_766    False\n",
       "feature_767    False\n",
       "feature_768    False\n",
       "Length: 769, dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum().sum()\n",
    "\n",
    "# based on above output we can see that there are no missing values in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset shape: (744, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.187868</td>\n",
       "      <td>-0.009268</td>\n",
       "      <td>0.039846</td>\n",
       "      <td>-0.102515</td>\n",
       "      <td>-0.109121</td>\n",
       "      <td>-0.079769</td>\n",
       "      <td>0.061295</td>\n",
       "      <td>-0.042890</td>\n",
       "      <td>0.099860</td>\n",
       "      <td>-0.066319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043647</td>\n",
       "      <td>0.061174</td>\n",
       "      <td>0.026771</td>\n",
       "      <td>-0.092734</td>\n",
       "      <td>-0.052383</td>\n",
       "      <td>-0.055526</td>\n",
       "      <td>0.029101</td>\n",
       "      <td>0.041857</td>\n",
       "      <td>-0.027872</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122119</td>\n",
       "      <td>0.019015</td>\n",
       "      <td>0.134483</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>-0.157884</td>\n",
       "      <td>-0.033332</td>\n",
       "      <td>0.044334</td>\n",
       "      <td>-0.016869</td>\n",
       "      <td>-0.014088</td>\n",
       "      <td>-0.060542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179798</td>\n",
       "      <td>-0.027611</td>\n",
       "      <td>0.240023</td>\n",
       "      <td>-0.005648</td>\n",
       "      <td>-0.057537</td>\n",
       "      <td>0.023922</td>\n",
       "      <td>-0.038126</td>\n",
       "      <td>-0.015171</td>\n",
       "      <td>-0.006270</td>\n",
       "      <td>-0.003790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.079187</td>\n",
       "      <td>0.146195</td>\n",
       "      <td>-0.001316</td>\n",
       "      <td>-0.085596</td>\n",
       "      <td>-0.261093</td>\n",
       "      <td>-0.021620</td>\n",
       "      <td>0.268898</td>\n",
       "      <td>-0.103950</td>\n",
       "      <td>0.068976</td>\n",
       "      <td>-0.062388</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075460</td>\n",
       "      <td>0.037346</td>\n",
       "      <td>0.124254</td>\n",
       "      <td>-0.369695</td>\n",
       "      <td>-0.056773</td>\n",
       "      <td>-0.028471</td>\n",
       "      <td>0.019971</td>\n",
       "      <td>-0.065683</td>\n",
       "      <td>-0.047366</td>\n",
       "      <td>-0.121744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.231196</td>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.215534</td>\n",
       "      <td>-0.210258</td>\n",
       "      <td>-0.158189</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.240254</td>\n",
       "      <td>-0.068112</td>\n",
       "      <td>-0.017550</td>\n",
       "      <td>-0.233767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0.141642</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>0.031547</td>\n",
       "      <td>-0.010762</td>\n",
       "      <td>-0.169036</td>\n",
       "      <td>-0.030963</td>\n",
       "      <td>0.086698</td>\n",
       "      <td>0.114194</td>\n",
       "      <td>-0.036775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020412</td>\n",
       "      <td>0.063732</td>\n",
       "      <td>-0.065506</td>\n",
       "      <td>-0.089598</td>\n",
       "      <td>-0.130788</td>\n",
       "      <td>-0.018809</td>\n",
       "      <td>0.119304</td>\n",
       "      <td>-0.057494</td>\n",
       "      <td>0.094714</td>\n",
       "      <td>-0.047923</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001969</td>\n",
       "      <td>-0.003025</td>\n",
       "      <td>-0.021156</td>\n",
       "      <td>-0.136993</td>\n",
       "      <td>-0.003615</td>\n",
       "      <td>0.036371</td>\n",
       "      <td>0.048715</td>\n",
       "      <td>-0.002688</td>\n",
       "      <td>-0.016957</td>\n",
       "      <td>-0.004331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.187868  -0.009268   0.039846  -0.102515  -0.109121  -0.079769   \n",
       "1   0.122119   0.019015   0.134483   0.007949  -0.157884  -0.033332   \n",
       "2   0.079187   0.146195  -0.001316  -0.085596  -0.261093  -0.021620   \n",
       "3   0.231196   0.021558   0.215534  -0.210258  -0.158189   0.041621   \n",
       "4   0.020412   0.063732  -0.065506  -0.089598  -0.130788  -0.018809   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_759  feature_760  \\\n",
       "0   0.061295  -0.042890   0.099860   -0.066319  ...     0.043647     0.061174   \n",
       "1   0.044334  -0.016869  -0.014088   -0.060542  ...    -0.179798    -0.027611   \n",
       "2   0.268898  -0.103950   0.068976   -0.062388  ...    -0.075460     0.037346   \n",
       "3   0.240254  -0.068112  -0.017550   -0.233767  ...     0.083334     0.141642   \n",
       "4   0.119304  -0.057494   0.094714   -0.047923  ...    -0.001969    -0.003025   \n",
       "\n",
       "   feature_761  feature_762  feature_763  feature_764  feature_765  \\\n",
       "0     0.026771    -0.092734    -0.052383    -0.055526     0.029101   \n",
       "1     0.240023    -0.005648    -0.057537     0.023922    -0.038126   \n",
       "2     0.124254    -0.369695    -0.056773    -0.028471     0.019971   \n",
       "3    -0.007219     0.031547    -0.010762    -0.169036    -0.030963   \n",
       "4    -0.021156    -0.136993    -0.003615     0.036371     0.048715   \n",
       "\n",
       "   feature_766  feature_767  feature_768  \n",
       "0     0.041857    -0.027872     0.099500  \n",
       "1    -0.015171    -0.006270    -0.003790  \n",
       "2    -0.065683    -0.047366    -0.121744  \n",
       "3     0.086698     0.114194    -0.036775  \n",
       "4    -0.002688    -0.016957    -0.004331  \n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = test_df.drop([\"ID\"], axis=1)\n",
    "print(\"test dataset shape:\", test_X.shape)\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.186257</td>\n",
       "      <td>-0.058807</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>-0.163933</td>\n",
       "      <td>-0.146699</td>\n",
       "      <td>0.035889</td>\n",
       "      <td>0.111708</td>\n",
       "      <td>-0.162861</td>\n",
       "      <td>0.028249</td>\n",
       "      <td>-0.098063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055629</td>\n",
       "      <td>-0.010358</td>\n",
       "      <td>0.125754</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>0.079197</td>\n",
       "      <td>0.093215</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.063431</td>\n",
       "      <td>-0.023597</td>\n",
       "      <td>0.068057</td>\n",
       "      <td>-0.252915</td>\n",
       "      <td>-0.061094</td>\n",
       "      <td>-0.027316</td>\n",
       "      <td>0.135747</td>\n",
       "      <td>-0.168147</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>-0.078473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014893</td>\n",
       "      <td>0.071721</td>\n",
       "      <td>0.018918</td>\n",
       "      <td>0.100032</td>\n",
       "      <td>-0.083042</td>\n",
       "      <td>0.088615</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034962</td>\n",
       "      <td>0.035816</td>\n",
       "      <td>-0.029753</td>\n",
       "      <td>-0.094607</td>\n",
       "      <td>-0.017576</td>\n",
       "      <td>-0.053074</td>\n",
       "      <td>0.040121</td>\n",
       "      <td>-0.007932</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>-0.024042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>0.015215</td>\n",
       "      <td>0.083808</td>\n",
       "      <td>0.031312</td>\n",
       "      <td>-0.056277</td>\n",
       "      <td>0.064702</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033772</td>\n",
       "      <td>0.085612</td>\n",
       "      <td>0.067488</td>\n",
       "      <td>-0.073953</td>\n",
       "      <td>-0.180646</td>\n",
       "      <td>-0.024512</td>\n",
       "      <td>0.242879</td>\n",
       "      <td>-0.023374</td>\n",
       "      <td>-0.059999</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078246</td>\n",
       "      <td>-0.032903</td>\n",
       "      <td>0.082949</td>\n",
       "      <td>-0.020659</td>\n",
       "      <td>0.082274</td>\n",
       "      <td>-0.050164</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.134305</td>\n",
       "      <td>0.062096</td>\n",
       "      <td>0.106920</td>\n",
       "      <td>-0.089327</td>\n",
       "      <td>0.117093</td>\n",
       "      <td>-0.077107</td>\n",
       "      <td>0.152579</td>\n",
       "      <td>0.047529</td>\n",
       "      <td>-0.015998</td>\n",
       "      <td>-0.110657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094629</td>\n",
       "      <td>0.069718</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.048124</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>-0.016980</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.186257  -0.058807   0.024632  -0.163933  -0.146699   0.035889   \n",
       "1   0.063431  -0.023597   0.068057  -0.252915  -0.061094  -0.027316   \n",
       "2   0.034962   0.035816  -0.029753  -0.094607  -0.017576  -0.053074   \n",
       "3   0.033772   0.085612   0.067488  -0.073953  -0.180646  -0.024512   \n",
       "4   0.134305   0.062096   0.106920  -0.089327   0.117093  -0.077107   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0   0.111708  -0.162861   0.028249   -0.098063  ...     0.055629    -0.010358   \n",
       "1   0.135747  -0.168147   0.091236   -0.078473  ...    -0.014893     0.071721   \n",
       "2   0.040121  -0.007932   0.097872   -0.024042  ...     0.012415     0.015215   \n",
       "3   0.242879  -0.023374  -0.059999    0.002006  ...    -0.078246    -0.032903   \n",
       "4   0.152579   0.047529  -0.015998   -0.110657  ...    -0.094629     0.069718   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0     0.125754     0.011648     0.079197     0.093215       45      NaN   \n",
       "1     0.018918     0.100032    -0.083042     0.088615       45      NaN   \n",
       "2     0.083808     0.031312    -0.056277     0.064702       45      NaN   \n",
       "3     0.082949    -0.020659     0.082274    -0.050164       45      NaN   \n",
       "4     0.014379     0.048124     0.007586    -0.016980       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_train_df = pd.read_csv(layer_7_train_csv_file_path)\n",
    "l7_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l7_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l7_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l7_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "l7_train_X = l7_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)\n",
    "train_speaker_IDs = l7_train_df['label_1']\n",
    "train_speaker_ages = l7_train_df['label_2']\n",
    "train_speaker_genders = l7_train_df['label_3']\n",
    "train_speaker_accents = l7_train_df['label_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "l7_valid_df = pd.read_csv(layer_7_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l7_valid_df.shape)\n",
    "print(\"null values row count: \", l7_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l7_valid_df.isnull().sum().sum() / l7_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l7_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l7_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l7_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "l7_valid_X = l7_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)\n",
    "valid_speaker_IDs = l7_valid_df['label_1']\n",
    "valid_speaker_ages = l7_valid_df['label_2']\n",
    "valid_speaker_genders = l7_valid_df['label_3']\n",
    "valid_speaker_accents = l7_valid_df['label_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now develop pipelines to predict the speaker ID, speaker age, speaker gender, and speaker accent. \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##visualize Pipeline\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       1.00      1.00      1.00        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      1.00      1.00        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      1.00      0.97        14\n",
      "          18       1.00      1.00      1.00        19\n",
      "          19       0.93      0.93      0.93        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       1.00      0.93      0.96        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       1.00      1.00      1.00        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      1.00      1.00        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.93      0.97        15\n",
      "          30       1.00      1.00      1.00         8\n",
      "          31       1.00      1.00      1.00        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       1.00      1.00      1.00         6\n",
      "          34       1.00      1.00      1.00        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      1.00      1.00        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       1.00      1.00      1.00        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       1.00      1.00      1.00        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       0.91      1.00      0.95        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       0.99      0.99      0.99       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PCA__n_components = 0.97\n",
    "clf__C = 10.0\n",
    "clf__kernel = 'rbf'\n",
    "\n",
    "layer_7_speaker_ID_pipe = Pipeline([\n",
    "    ('pca', PCA(n_components=PCA__n_components)),\n",
    "    ('clf', SVC(C=clf__C, kernel=clf__kernel))\n",
    "    ])\n",
    "\n",
    "layer_7_speaker_ID_pipe.fit(l7_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_7_speaker_ID_pipe.predict(l7_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_7_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.92      1.00      0.96        36\n",
      "        23.0       0.96      0.97      0.97        71\n",
      "        24.0       0.96      0.96      0.96        46\n",
      "        25.0       0.99      0.90      0.94        79\n",
      "        26.0       0.93      0.99      0.96       115\n",
      "        27.0       1.00      0.95      0.97        81\n",
      "        28.0       0.97      0.97      0.97        60\n",
      "        29.0       1.00      1.00      1.00        45\n",
      "        30.0       0.96      1.00      0.98        48\n",
      "        31.0       0.95      0.97      0.96        65\n",
      "        32.0       1.00      1.00      1.00        11\n",
      "        33.0       1.00      1.00      1.00        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.93      0.96        14\n",
      "        61.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.98      0.97      0.97       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca__n_components = 0.97\n",
    "clf__C = 10.0\n",
    "clf__kernel = 'rbf'\n",
    "\n",
    "# Pipleline with best params\n",
    "layer_7_speaker_age_pipe = Pipeline([\n",
    "    # ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=pca__n_components)),\n",
    "    ('clf', SVC(C=clf__C, kernel=clf__kernel))\n",
    "    ])\n",
    "\n",
    "layer_7_speaker_age_pipe.fit(l7_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_7_speaker_age_pipe.predict(l7_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_7_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With SVC classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      1.00      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_7_speaker_gender_pipe = Pipeline([ \n",
    "    ('clf', SVC(class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_7_speaker_gender_pipe.fit(l7_train_X, train_speaker_genders)\n",
    "\n",
    "# now let's check the results\n",
    "print(\"With SVC classifier:\") \n",
    "print(classification_report(valid_speaker_genders, layer_7_speaker_gender_pipe.predict(l7_valid_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA for feature reduction (Hyperparameters tuned):  0.9906666666666667\n"
     ]
    }
   ],
   "source": [
    "pca__n_components = 0.97\n",
    "clf__C = 10.0\n",
    "clf__kernel = 'rbf'\n",
    "\n",
    "# Pipleline with best params\n",
    "layer_7_speaker_accent_pipe = Pipeline([\n",
    "    # ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=pca__n_components)),\n",
    "    ('clf', SVC(C=clf__C, kernel=clf__kernel))\n",
    "    ])\n",
    "\n",
    "layer_7_speaker_accent_pipe.fit(l7_train_X, train_speaker_accents)\n",
    "print(\"PCA for feature reduction (Hyperparameters tuned): \", layer_7_speaker_accent_pipe.score(l7_valid_X, valid_speaker_accents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.071810</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>-0.022749</td>\n",
       "      <td>0.086143</td>\n",
       "      <td>0.026361</td>\n",
       "      <td>-0.028817</td>\n",
       "      <td>0.199237</td>\n",
       "      <td>-0.287368</td>\n",
       "      <td>-0.059560</td>\n",
       "      <td>-0.043694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004646</td>\n",
       "      <td>0.123011</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>-0.042152</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>0.072623</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.030930</td>\n",
       "      <td>0.024088</td>\n",
       "      <td>0.057811</td>\n",
       "      <td>-0.230877</td>\n",
       "      <td>-0.146281</td>\n",
       "      <td>0.102807</td>\n",
       "      <td>0.128767</td>\n",
       "      <td>-0.146269</td>\n",
       "      <td>0.053893</td>\n",
       "      <td>0.055378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077742</td>\n",
       "      <td>0.081691</td>\n",
       "      <td>-0.004778</td>\n",
       "      <td>0.171727</td>\n",
       "      <td>-0.026027</td>\n",
       "      <td>0.171089</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.044019</td>\n",
       "      <td>-0.004626</td>\n",
       "      <td>-0.029383</td>\n",
       "      <td>-0.165376</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>-0.028142</td>\n",
       "      <td>-0.009649</td>\n",
       "      <td>-0.082088</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>-0.046272</td>\n",
       "      <td>0.027831</td>\n",
       "      <td>0.028096</td>\n",
       "      <td>0.030994</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.086241</td>\n",
       "      <td>0.129585</td>\n",
       "      <td>-0.013893</td>\n",
       "      <td>0.089885</td>\n",
       "      <td>-0.100300</td>\n",
       "      <td>-0.035184</td>\n",
       "      <td>0.240980</td>\n",
       "      <td>-0.128362</td>\n",
       "      <td>-0.072328</td>\n",
       "      <td>-0.019385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006934</td>\n",
       "      <td>-0.049213</td>\n",
       "      <td>0.078852</td>\n",
       "      <td>0.088323</td>\n",
       "      <td>0.168815</td>\n",
       "      <td>-0.049188</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126416</td>\n",
       "      <td>0.088338</td>\n",
       "      <td>0.088307</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.174417</td>\n",
       "      <td>-0.030560</td>\n",
       "      <td>0.181163</td>\n",
       "      <td>-0.009382</td>\n",
       "      <td>0.085396</td>\n",
       "      <td>0.015823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028883</td>\n",
       "      <td>0.110844</td>\n",
       "      <td>-0.041875</td>\n",
       "      <td>0.025686</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>-0.104945</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.071810   0.068413  -0.022749   0.086143   0.026361  -0.028817   \n",
       "1   0.030930   0.024088   0.057811  -0.230877  -0.146281   0.102807   \n",
       "2  -0.044019  -0.004626  -0.029383  -0.165376  -0.026611  -0.028142   \n",
       "3  -0.086241   0.129585  -0.013893   0.089885  -0.100300  -0.035184   \n",
       "4   0.126416   0.088338   0.088307   0.020371   0.174417  -0.030560   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0   0.199237  -0.287368  -0.059560   -0.043694  ...     0.004646     0.123011   \n",
       "1   0.128767  -0.146269   0.053893    0.055378  ...     0.077742     0.081691   \n",
       "2  -0.009649  -0.082088   0.018933    0.006830  ...     0.076249    -0.046272   \n",
       "3   0.240980  -0.128362  -0.072328   -0.019385  ...     0.006934    -0.049213   \n",
       "4   0.181163  -0.009382   0.085396    0.015823  ...    -0.028883     0.110844   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0     0.043040    -0.042152     0.026225     0.072623       45      NaN   \n",
       "1    -0.004778     0.171727    -0.026027     0.171089       45      NaN   \n",
       "2     0.027831     0.028096     0.030994     0.009709       45      NaN   \n",
       "3     0.078852     0.088323     0.168815    -0.049188       45      NaN   \n",
       "4    -0.041875     0.025686     0.003534    -0.104945       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_train_df = pd.read_csv(layer_8_train_csv_file_path)\n",
    "l8_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l8_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l8_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l8_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "l8_train_X = l8_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_valid_df = pd.read_csv(layer_8_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l8_valid_df.shape)\n",
    "print(\"null values row count: \", l8_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l8_valid_df.isnull().sum().sum() / l8_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l8_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l8_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l8_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_valid_X = l8_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.92      0.92        13\n",
      "           2       1.00      0.78      0.88         9\n",
      "           3       0.86      1.00      0.92        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       0.84      0.89      0.86        18\n",
      "           6       0.90      1.00      0.95         9\n",
      "           7       0.89      0.94      0.91        17\n",
      "           8       0.91      0.71      0.80        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      0.89      0.94        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.82      0.90        11\n",
      "          14       0.93      0.87      0.90        15\n",
      "          15       1.00      0.88      0.94        17\n",
      "          16       1.00      0.93      0.96        14\n",
      "          17       1.00      0.93      0.96        14\n",
      "          18       0.95      0.95      0.95        19\n",
      "          19       0.74      0.93      0.82        15\n",
      "          20       1.00      0.82      0.90        11\n",
      "          21       0.87      0.93      0.90        14\n",
      "          22       0.91      0.91      0.91        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.65      0.92      0.76        12\n",
      "          25       0.91      0.91      0.91        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.94      0.94      0.94        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.87      0.93        15\n",
      "          30       0.88      0.88      0.88         8\n",
      "          31       0.92      0.92      0.92        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.77      0.87        13\n",
      "          35       0.89      1.00      0.94         8\n",
      "          36       0.85      0.94      0.89        18\n",
      "          37       1.00      0.81      0.90        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       0.92      0.92      0.92        12\n",
      "          41       0.83      1.00      0.91        10\n",
      "          42       0.92      1.00      0.96        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       0.90      0.95      0.92        19\n",
      "          45       0.93      0.93      0.93        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       0.70      0.88      0.78         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       1.00      0.85      0.92        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       0.80      1.00      0.89         8\n",
      "          52       0.85      1.00      0.92        11\n",
      "          53       0.93      0.93      0.93        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       0.88      0.88      0.88         8\n",
      "          56       1.00      0.90      0.95        10\n",
      "          57       1.00      0.94      0.97        18\n",
      "          58       1.00      0.95      0.97        20\n",
      "          59       0.90      0.90      0.90        10\n",
      "          60       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.93       750\n",
      "   macro avg       0.93      0.93      0.93       750\n",
      "weighted avg       0.93      0.93      0.93       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_8_speaker_ID_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.85)),\n",
    "    ('clf', SVC(C=10, gamma=0.001, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_8_speaker_ID_pipe.fit(l8_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_8_speaker_ID_pipe.predict(l8_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_8_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.97      0.83      0.90        36\n",
      "        23.0       0.96      0.90      0.93        71\n",
      "        24.0       0.85      0.89      0.87        46\n",
      "        25.0       0.95      0.90      0.92        79\n",
      "        26.0       0.83      0.95      0.88       115\n",
      "        27.0       0.94      0.89      0.91        81\n",
      "        28.0       0.86      0.92      0.89        60\n",
      "        29.0       0.93      0.96      0.95        45\n",
      "        30.0       0.98      0.92      0.95        48\n",
      "        31.0       0.89      0.98      0.93        65\n",
      "        32.0       1.00      0.82      0.90        11\n",
      "        33.0       1.00      0.93      0.97        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.73      0.84        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.93      0.96        14\n",
      "        61.0       1.00      0.89      0.94        19\n",
      "\n",
      "    accuracy                           0.91       750\n",
      "   macro avg       0.95      0.90      0.92       750\n",
      "weighted avg       0.92      0.91      0.92       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_8_age_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.85)),\n",
    "    ('clf', SVC(C=100, class_weight='balanced', gamma=0.01, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_8_age_pipe.fit(l8_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_8_age_pipe.predict(l8_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_8_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       0.99      1.00      0.99       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_8_gender_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.85)),\n",
    "    ('clf', SVC(C=10, class_weight='balanced', gamma=0.001, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_8_gender_pipe.fit(l8_train_X, train_speaker_genders)\n",
    "print(classification_report(valid_speaker_genders, layer_8_gender_pipe.predict(l8_valid_X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        21\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       0.60      0.93      0.72        27\n",
      "           3       0.89      1.00      0.94         8\n",
      "           4       0.76      0.87      0.81        15\n",
      "           5       0.90      0.82      0.86        11\n",
      "           6       0.96      0.94      0.95       532\n",
      "           7       0.83      0.78      0.81        32\n",
      "           8       1.00      0.84      0.91        19\n",
      "           9       1.00      0.88      0.94        17\n",
      "          10       1.00      0.90      0.95        10\n",
      "          11       0.91      0.91      0.91        11\n",
      "          12       0.76      0.85      0.80        26\n",
      "          13       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.92       750\n",
      "   macro avg       0.89      0.89      0.88       750\n",
      "weighted avg       0.93      0.92      0.92       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_8_accent_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.85)),\n",
    "    ('clf', SVC(C=100, class_weight='balanced', gamma=0.001, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_8_accent_pipe.fit(l8_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_8_accent_pipe.predict(l8_valid_X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019301</td>\n",
       "      <td>0.059756</td>\n",
       "      <td>0.081375</td>\n",
       "      <td>0.057481</td>\n",
       "      <td>-0.068440</td>\n",
       "      <td>-0.165913</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>-0.091138</td>\n",
       "      <td>0.021688</td>\n",
       "      <td>0.057158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035576</td>\n",
       "      <td>0.127319</td>\n",
       "      <td>0.098128</td>\n",
       "      <td>-0.058787</td>\n",
       "      <td>0.100971</td>\n",
       "      <td>-0.047754</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049741</td>\n",
       "      <td>0.090030</td>\n",
       "      <td>0.035118</td>\n",
       "      <td>-0.013676</td>\n",
       "      <td>-0.194317</td>\n",
       "      <td>-0.101763</td>\n",
       "      <td>0.085875</td>\n",
       "      <td>-0.081317</td>\n",
       "      <td>0.112418</td>\n",
       "      <td>0.120523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020538</td>\n",
       "      <td>0.058968</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.111324</td>\n",
       "      <td>0.036727</td>\n",
       "      <td>0.031927</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019212</td>\n",
       "      <td>0.087779</td>\n",
       "      <td>0.093907</td>\n",
       "      <td>-0.033738</td>\n",
       "      <td>-0.141409</td>\n",
       "      <td>-0.062881</td>\n",
       "      <td>-0.071402</td>\n",
       "      <td>-0.006599</td>\n",
       "      <td>0.020372</td>\n",
       "      <td>-0.027777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119645</td>\n",
       "      <td>-0.040861</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>-0.061003</td>\n",
       "      <td>-0.042450</td>\n",
       "      <td>0.063340</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070283</td>\n",
       "      <td>0.049040</td>\n",
       "      <td>0.042126</td>\n",
       "      <td>0.122637</td>\n",
       "      <td>-0.056964</td>\n",
       "      <td>-0.113700</td>\n",
       "      <td>0.108454</td>\n",
       "      <td>0.051336</td>\n",
       "      <td>0.086610</td>\n",
       "      <td>0.141578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124494</td>\n",
       "      <td>-0.169225</td>\n",
       "      <td>-0.046391</td>\n",
       "      <td>0.148787</td>\n",
       "      <td>0.014616</td>\n",
       "      <td>-0.140644</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028864</td>\n",
       "      <td>0.165634</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>0.036117</td>\n",
       "      <td>-0.028871</td>\n",
       "      <td>-0.147748</td>\n",
       "      <td>0.053180</td>\n",
       "      <td>0.025071</td>\n",
       "      <td>-0.004200</td>\n",
       "      <td>-0.022183</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124862</td>\n",
       "      <td>0.044907</td>\n",
       "      <td>0.084005</td>\n",
       "      <td>-0.038450</td>\n",
       "      <td>0.084371</td>\n",
       "      <td>-0.072146</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.019301   0.059756   0.081375   0.057481  -0.068440  -0.165913   \n",
       "1   0.049741   0.090030   0.035118  -0.013676  -0.194317  -0.101763   \n",
       "2   0.019212   0.087779   0.093907  -0.033738  -0.141409  -0.062881   \n",
       "3   0.070283   0.049040   0.042126   0.122637  -0.056964  -0.113700   \n",
       "4   0.028864   0.165634   0.016302   0.036117  -0.028871  -0.147748   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0   0.035643  -0.091138   0.021688    0.057158  ...    -0.035576     0.127319   \n",
       "1   0.085875  -0.081317   0.112418    0.120523  ...     0.020538     0.058968   \n",
       "2  -0.071402  -0.006599   0.020372   -0.027777  ...     0.119645    -0.040861   \n",
       "3   0.108454   0.051336   0.086610    0.141578  ...    -0.124494    -0.169225   \n",
       "4   0.053180   0.025071  -0.004200   -0.022183  ...    -0.124862     0.044907   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0     0.098128    -0.058787     0.100971    -0.047754       45      NaN   \n",
       "1     0.029803     0.111324     0.036727     0.031927       45      NaN   \n",
       "2     0.000548    -0.061003    -0.042450     0.063340       45      NaN   \n",
       "3    -0.046391     0.148787     0.014616    -0.140644       45      NaN   \n",
       "4     0.084005    -0.038450     0.084371    -0.072146       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_train_df = pd.read_csv(layer_9_train_csv_file_path)\n",
    "l9_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l9_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean: 28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l9_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean:\", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l9_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_train_X = l9_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_valid_df = pd.read_csv(layer_9_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l9_valid_df.shape)\n",
    "print(\"null values row count: \", l9_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l9_valid_df.isnull().sum().sum() / l9_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l9_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l9_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l9_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_valid_X = l9_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       0.92      1.00      0.96        12\n",
      "           4       0.88      0.94      0.91        16\n",
      "           5       1.00      0.89      0.94        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       0.94      0.94      0.94        17\n",
      "           8       1.00      0.79      0.88        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       0.89      1.00      0.94         8\n",
      "          11       0.95      0.95      0.95        19\n",
      "          12       0.88      1.00      0.93         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.87      0.93        15\n",
      "          15       1.00      0.88      0.94        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       1.00      1.00      1.00        14\n",
      "          18       0.90      0.95      0.92        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      0.91      0.95        11\n",
      "          21       0.93      0.93      0.93        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.92      0.92      0.92        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.84      1.00      0.91        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      0.88      0.93         8\n",
      "          31       0.85      0.92      0.88        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.85      0.92        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       0.95      1.00      0.97        18\n",
      "          37       1.00      0.94      0.97        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       0.91      1.00      0.95        10\n",
      "          42       0.92      1.00      0.96        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       0.93      1.00      0.97        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       0.88      0.88      0.88         8\n",
      "          48       0.84      0.94      0.89        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       0.89      1.00      0.94         8\n",
      "          52       0.91      0.91      0.91        11\n",
      "          53       1.00      0.93      0.97        15\n",
      "          54       0.89      0.89      0.89         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       0.90      0.90      0.90        10\n",
      "          57       0.95      1.00      0.97        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.96       750\n",
      "   macro avg       0.96      0.96      0.96       750\n",
      "weighted avg       0.96      0.96      0.96       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_9_speaker_ID_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.94)),\n",
    "    ('clf', SVC(C=28, kernel='rbf'))\n",
    "    ])\n",
    "\n",
    "layer_9_speaker_ID_pipe.fit(l9_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_9_speaker_ID_pipe.predict(l9_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_9_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.92      1.00      0.96        36\n",
      "        23.0       0.88      0.92      0.90        71\n",
      "        24.0       0.90      0.98      0.94        46\n",
      "        25.0       0.97      0.91      0.94        79\n",
      "        26.0       0.92      0.96      0.94       115\n",
      "        27.0       0.95      0.93      0.94        81\n",
      "        28.0       0.93      0.95      0.94        60\n",
      "        29.0       0.98      0.98      0.98        45\n",
      "        30.0       0.94      0.96      0.95        48\n",
      "        31.0       0.98      0.97      0.98        65\n",
      "        32.0       1.00      1.00      1.00        11\n",
      "        33.0       1.00      0.93      0.97        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.71      0.83        14\n",
      "        61.0       1.00      0.95      0.97        19\n",
      "\n",
      "    accuracy                           0.94       750\n",
      "   macro avg       0.96      0.94      0.95       750\n",
      "weighted avg       0.95      0.94      0.94       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_9_age_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.96)),\n",
    "    ('clf', SVC(C=55, kernel='rbf', class_weight='balanced', probability=True))\n",
    "    ])\n",
    "\n",
    "layer_9_age_pipe.fit(l9_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_9_age_pipe.predict(l9_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_9_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       0.99      0.99      0.99       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_9_gender_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.90)),\n",
    "    ('clf', SVC(C=9, kernel='rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_9_gender_pipe.fit(l9_train_X, train_speaker_genders)\n",
    "print(classification_report(valid_speaker_genders, layer_9_gender_pipe.predict(l9_valid_X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.93      1.00      0.96        27\n",
      "           3       1.00      0.88      0.93         8\n",
      "           4       1.00      0.87      0.93        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.97      1.00      0.99       532\n",
      "           7       1.00      0.91      0.95        32\n",
      "           8       0.95      0.95      0.95        19\n",
      "           9       1.00      0.82      0.90        17\n",
      "          10       1.00      1.00      1.00        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       1.00      0.88      0.94        26\n",
      "          13       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.98       750\n",
      "   macro avg       0.99      0.94      0.96       750\n",
      "weighted avg       0.98      0.98      0.98       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_9_accent_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('clf', SVC(C=18, kernel='rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_9_accent_pipe.fit(l9_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_9_accent_pipe.predict(l9_valid_X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.027083</td>\n",
       "      <td>0.072947</td>\n",
       "      <td>-0.093659</td>\n",
       "      <td>0.053418</td>\n",
       "      <td>-0.085516</td>\n",
       "      <td>-0.102610</td>\n",
       "      <td>-0.021217</td>\n",
       "      <td>0.016162</td>\n",
       "      <td>-0.184269</td>\n",
       "      <td>0.110335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183643</td>\n",
       "      <td>0.091299</td>\n",
       "      <td>-0.037097</td>\n",
       "      <td>0.042607</td>\n",
       "      <td>-0.034361</td>\n",
       "      <td>-0.013748</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.070195</td>\n",
       "      <td>0.228641</td>\n",
       "      <td>-0.132860</td>\n",
       "      <td>-0.077761</td>\n",
       "      <td>-0.054993</td>\n",
       "      <td>-0.210365</td>\n",
       "      <td>0.127747</td>\n",
       "      <td>-0.132385</td>\n",
       "      <td>-0.161366</td>\n",
       "      <td>0.172764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123668</td>\n",
       "      <td>0.029626</td>\n",
       "      <td>-0.027345</td>\n",
       "      <td>0.055223</td>\n",
       "      <td>-0.179725</td>\n",
       "      <td>0.136841</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.164312</td>\n",
       "      <td>0.052808</td>\n",
       "      <td>-0.058510</td>\n",
       "      <td>0.104724</td>\n",
       "      <td>-0.025886</td>\n",
       "      <td>-0.101427</td>\n",
       "      <td>-0.047177</td>\n",
       "      <td>0.091298</td>\n",
       "      <td>-0.094569</td>\n",
       "      <td>0.088062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075410</td>\n",
       "      <td>0.070125</td>\n",
       "      <td>0.043022</td>\n",
       "      <td>0.012972</td>\n",
       "      <td>-0.028920</td>\n",
       "      <td>0.096725</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.113737</td>\n",
       "      <td>0.061113</td>\n",
       "      <td>-0.099329</td>\n",
       "      <td>-0.111600</td>\n",
       "      <td>-0.245942</td>\n",
       "      <td>0.086520</td>\n",
       "      <td>0.071996</td>\n",
       "      <td>0.028319</td>\n",
       "      <td>0.207910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062511</td>\n",
       "      <td>-0.226912</td>\n",
       "      <td>-0.046011</td>\n",
       "      <td>0.011282</td>\n",
       "      <td>-0.095167</td>\n",
       "      <td>0.039979</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.031364</td>\n",
       "      <td>0.142409</td>\n",
       "      <td>-0.160743</td>\n",
       "      <td>-0.076594</td>\n",
       "      <td>-0.062412</td>\n",
       "      <td>-0.264732</td>\n",
       "      <td>0.079197</td>\n",
       "      <td>0.026060</td>\n",
       "      <td>-0.217023</td>\n",
       "      <td>0.084656</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193882</td>\n",
       "      <td>0.107297</td>\n",
       "      <td>-0.042355</td>\n",
       "      <td>0.046763</td>\n",
       "      <td>-0.192469</td>\n",
       "      <td>0.006463</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.027083   0.072947  -0.093659   0.053418  -0.085516  -0.102610   \n",
       "1   0.070195   0.228641  -0.132860  -0.077761  -0.054993  -0.210365   \n",
       "2   0.164312   0.052808  -0.058510   0.104724  -0.025886  -0.101427   \n",
       "3   0.029730   0.113737   0.061113  -0.099329  -0.111600  -0.245942   \n",
       "4   0.031364   0.142409  -0.160743  -0.076594  -0.062412  -0.264732   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0  -0.021217   0.016162  -0.184269    0.110335  ...    -0.183643     0.091299   \n",
       "1   0.127747  -0.132385  -0.161366    0.172764  ...    -0.123668     0.029626   \n",
       "2  -0.047177   0.091298  -0.094569    0.088062  ...     0.075410     0.070125   \n",
       "3   0.086520   0.071996   0.028319    0.207910  ...    -0.062511    -0.226912   \n",
       "4   0.079197   0.026060  -0.217023    0.084656  ...    -0.193882     0.107297   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0    -0.037097     0.042607    -0.034361    -0.013748       45      NaN   \n",
       "1    -0.027345     0.055223    -0.179725     0.136841       45      NaN   \n",
       "2     0.043022     0.012972    -0.028920     0.096725       45      NaN   \n",
       "3    -0.046011     0.011282    -0.095167     0.039979       45      NaN   \n",
       "4    -0.042355     0.046763    -0.192469     0.006463       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_train_df = pd.read_csv(layer_10_train_csv_file_path)\n",
    "l10_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l10_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean: 28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l10_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean:\", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l10_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_train_X = l10_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_valid_df = pd.read_csv(layer_10_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l10_valid_df.shape)\n",
    "print(\"null values row count: \", l10_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l10_valid_df.isnull().sum().sum() / l10_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l10_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l10_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l10_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_valid_X = l10_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.92      0.89        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       1.00      1.00      1.00        16\n",
      "           5       0.94      0.89      0.91        18\n",
      "           6       1.00      0.89      0.94         9\n",
      "           7       0.94      0.94      0.94        17\n",
      "           8       1.00      0.79      0.88        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      0.95      0.97        19\n",
      "          12       0.88      1.00      0.93         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      1.00      1.00        15\n",
      "          15       1.00      0.88      0.94        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      0.93      0.93        14\n",
      "          18       1.00      0.95      0.97        19\n",
      "          19       0.78      0.93      0.85        15\n",
      "          20       1.00      0.82      0.90        11\n",
      "          21       1.00      1.00      1.00        14\n",
      "          22       0.92      1.00      0.96        11\n",
      "          23       1.00      1.00      1.00        10\n",
      "          24       0.92      1.00      0.96        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.94      1.00      0.97        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      0.88      0.93         8\n",
      "          31       0.85      0.92      0.88        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.85      0.92        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       0.94      1.00      0.97        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       0.93      1.00      0.96        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       0.92      1.00      0.96        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      1.00      1.00        19\n",
      "          45       0.93      1.00      0.97        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       0.89      1.00      0.94         8\n",
      "          48       0.89      0.94      0.91        17\n",
      "          49       0.87      1.00      0.93        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      1.00      1.00        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      0.95      0.97        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.97      0.97      0.97       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_10_speaker_ID_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('clf', SVC(C=10, kernel='rbf'))\n",
    "    ])\n",
    "\n",
    "layer_10_speaker_ID_pipe.fit(l10_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_10_speaker_ID_pipe.predict(l10_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_10_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96, 'layer_10_speaker_ID_pipe': 0.97}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.95      0.97      0.96        36\n",
      "        23.0       0.83      0.94      0.88        71\n",
      "        24.0       0.92      0.98      0.95        46\n",
      "        25.0       0.95      0.87      0.91        79\n",
      "        26.0       0.97      0.97      0.97       115\n",
      "        27.0       0.93      0.94      0.93        81\n",
      "        28.0       0.98      0.93      0.96        60\n",
      "        29.0       0.98      0.96      0.97        45\n",
      "        30.0       0.98      0.96      0.97        48\n",
      "        31.0       0.93      0.95      0.94        65\n",
      "        32.0       1.00      0.91      0.95        11\n",
      "        33.0       0.97      0.97      0.97        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.71      0.83        14\n",
      "        61.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.94       750\n",
      "   macro avg       0.96      0.93      0.95       750\n",
      "weighted avg       0.95      0.94      0.94       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_10_age_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.92)),\n",
    "    ('clf', SVC(C=27, kernel='rbf', class_weight='balanced', probability=True))\n",
    "    ])\n",
    "\n",
    "layer_10_age_pipe.fit(l10_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_10_age_pipe.predict(l10_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_10_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94, 'layer_10_speaker_age_pipe': 0.94}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.42      0.51       142\n",
      "           1       0.88      0.95      0.91       608\n",
      "\n",
      "    accuracy                           0.85       750\n",
      "   macro avg       0.76      0.68      0.71       750\n",
      "weighted avg       0.83      0.85      0.83       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_10_gender_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.83)),\n",
    "    ('clf', SVC(C=23, kernel='rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_10_gender_pipe.fit(l10_train_X, train_speaker_genders)\n",
    "print(classification_report(valid_speaker_genders, layer_10_gender_pipe.predict(l10_valid_X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        21\n",
      "           1       0.00      0.00      0.00        11\n",
      "           2       0.00      0.00      0.00        27\n",
      "           3       0.00      0.00      0.00         8\n",
      "           4       0.00      0.00      0.00        15\n",
      "           5       0.00      0.00      0.00        11\n",
      "           6       0.71      1.00      0.83       532\n",
      "           7       0.00      0.00      0.00        32\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00        17\n",
      "          10       0.00      0.00      0.00        10\n",
      "          11       0.00      0.00      0.00        11\n",
      "          12       0.50      0.04      0.07        26\n",
      "          13       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.71       750\n",
      "   macro avg       0.09      0.07      0.06       750\n",
      "weighted avg       0.52      0.71      0.59       750\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "layer_10_accent_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('clf', SVC(C=10, kernel='rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_10_accent_pipe.fit(l10_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_10_accent_pipe.predict(l10_valid_X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031138</td>\n",
       "      <td>0.079892</td>\n",
       "      <td>0.157382</td>\n",
       "      <td>-0.014636</td>\n",
       "      <td>-0.051778</td>\n",
       "      <td>-0.021332</td>\n",
       "      <td>-0.073593</td>\n",
       "      <td>-0.005386</td>\n",
       "      <td>-0.212557</td>\n",
       "      <td>0.099683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085248</td>\n",
       "      <td>-0.096007</td>\n",
       "      <td>-0.000766</td>\n",
       "      <td>0.021399</td>\n",
       "      <td>-0.041432</td>\n",
       "      <td>0.094806</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113040</td>\n",
       "      <td>0.175731</td>\n",
       "      <td>0.217741</td>\n",
       "      <td>-0.196254</td>\n",
       "      <td>-0.010129</td>\n",
       "      <td>-0.030586</td>\n",
       "      <td>0.067114</td>\n",
       "      <td>-0.072412</td>\n",
       "      <td>-0.239192</td>\n",
       "      <td>0.104741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090283</td>\n",
       "      <td>-0.053885</td>\n",
       "      <td>-0.010967</td>\n",
       "      <td>0.062209</td>\n",
       "      <td>-0.122958</td>\n",
       "      <td>0.192949</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.048570</td>\n",
       "      <td>0.091281</td>\n",
       "      <td>0.160776</td>\n",
       "      <td>-0.150937</td>\n",
       "      <td>0.020115</td>\n",
       "      <td>0.044117</td>\n",
       "      <td>-0.050092</td>\n",
       "      <td>-0.045661</td>\n",
       "      <td>-0.155332</td>\n",
       "      <td>0.117206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021524</td>\n",
       "      <td>-0.008411</td>\n",
       "      <td>-0.006248</td>\n",
       "      <td>0.031468</td>\n",
       "      <td>-0.056915</td>\n",
       "      <td>0.154731</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.039212</td>\n",
       "      <td>0.118388</td>\n",
       "      <td>0.173831</td>\n",
       "      <td>-0.096659</td>\n",
       "      <td>-0.008702</td>\n",
       "      <td>0.061298</td>\n",
       "      <td>0.008974</td>\n",
       "      <td>-0.003277</td>\n",
       "      <td>-0.065046</td>\n",
       "      <td>0.095480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071936</td>\n",
       "      <td>-0.023120</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>-0.121892</td>\n",
       "      <td>0.072796</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.056019</td>\n",
       "      <td>0.170639</td>\n",
       "      <td>0.157917</td>\n",
       "      <td>-0.228605</td>\n",
       "      <td>-0.065965</td>\n",
       "      <td>-0.088732</td>\n",
       "      <td>-0.082243</td>\n",
       "      <td>-0.080568</td>\n",
       "      <td>-0.341500</td>\n",
       "      <td>0.142430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155621</td>\n",
       "      <td>-0.079447</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>-0.151966</td>\n",
       "      <td>0.169634</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.031138   0.079892   0.157382  -0.014636  -0.051778  -0.021332   \n",
       "1   0.113040   0.175731   0.217741  -0.196254  -0.010129  -0.030586   \n",
       "2   0.048570   0.091281   0.160776  -0.150937   0.020115   0.044117   \n",
       "3   0.039212   0.118388   0.173831  -0.096659  -0.008702   0.061298   \n",
       "4   0.056019   0.170639   0.157917  -0.228605  -0.065965  -0.088732   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0  -0.073593  -0.005386  -0.212557    0.099683  ...    -0.085248    -0.096007   \n",
       "1   0.067114  -0.072412  -0.239192    0.104741  ...    -0.090283    -0.053885   \n",
       "2  -0.050092  -0.045661  -0.155332    0.117206  ...    -0.021524    -0.008411   \n",
       "3   0.008974  -0.003277  -0.065046    0.095480  ...    -0.071936    -0.023120   \n",
       "4  -0.082243  -0.080568  -0.341500    0.142430  ...    -0.155621    -0.079447   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0    -0.000766     0.021399    -0.041432     0.094806       45      NaN   \n",
       "1    -0.010967     0.062209    -0.122958     0.192949       45      NaN   \n",
       "2    -0.006248     0.031468    -0.056915     0.154731       45      NaN   \n",
       "3    -0.007812     0.057600    -0.121892     0.072796       45      NaN   \n",
       "4     0.015316     0.127726    -0.151966     0.169634       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_train_df = pd.read_csv(layer_11_train_csv_file_path)\n",
    "l11_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l11_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l11_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l11_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "l11_train_X = l11_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "l11_valid_df = pd.read_csv(layer_11_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l11_valid_df.shape)\n",
    "print(\"null values row count: \", l11_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l11_valid_df.isnull().sum().sum() / l11_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l11_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l11_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l11_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "l11_valid_X = l11_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      1.00      0.90        13\n",
      "           2       1.00      0.89      0.94         9\n",
      "           3       0.86      1.00      0.92        12\n",
      "           4       1.00      0.88      0.93        16\n",
      "           5       1.00      0.94      0.97        18\n",
      "           6       1.00      0.89      0.94         9\n",
      "           7       0.89      0.94      0.91        17\n",
      "           8       0.86      0.86      0.86        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      0.95      0.97        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       0.94      0.88      0.91        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.87      0.93      0.90        14\n",
      "          18       1.00      0.95      0.97        19\n",
      "          19       0.78      0.93      0.85        15\n",
      "          20       0.91      0.91      0.91        11\n",
      "          21       0.88      1.00      0.93        14\n",
      "          22       1.00      0.91      0.95        11\n",
      "          23       0.83      1.00      0.91        10\n",
      "          24       0.92      1.00      0.96        12\n",
      "          25       1.00      0.91      0.95        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      0.94      0.97        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.93      0.97        15\n",
      "          30       0.88      0.88      0.88         8\n",
      "          31       0.91      0.83      0.87        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       0.92      0.85      0.88        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      0.81      0.90        16\n",
      "          38       1.00      0.91      0.95        11\n",
      "          39       0.93      1.00      0.96        13\n",
      "          40       0.92      0.92      0.92        12\n",
      "          41       0.91      1.00      0.95        10\n",
      "          42       0.86      1.00      0.92        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      0.93      0.96        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       0.93      1.00      0.96        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       0.80      1.00      0.89         8\n",
      "          52       0.92      1.00      0.96        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      1.00      1.00         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      0.90      0.95        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      0.95      0.97        20\n",
      "          59       0.83      1.00      0.91        10\n",
      "          60       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.95       750\n",
      "   macro avg       0.95      0.95      0.95       750\n",
      "weighted avg       0.95      0.95      0.95       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_11_speaker_ID_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('clf', SVC(C=100, gamma=0.001, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_11_speaker_ID_pipe.fit(l11_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_11_speaker_ID_pipe.predict(l11_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_11_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96, 'layer_10_speaker_ID_pipe': 0.97, 'layer_11_speaker_ID_pipe': 0.95}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.89      0.89      0.89        36\n",
      "        23.0       0.77      0.87      0.82        71\n",
      "        24.0       0.91      0.93      0.92        46\n",
      "        25.0       0.92      0.84      0.87        79\n",
      "        26.0       0.85      0.91      0.88       115\n",
      "        27.0       0.95      0.85      0.90        81\n",
      "        28.0       0.95      0.87      0.90        60\n",
      "        29.0       0.98      0.96      0.97        45\n",
      "        30.0       0.90      0.96      0.93        48\n",
      "        31.0       0.86      0.94      0.90        65\n",
      "        32.0       1.00      0.82      0.90        11\n",
      "        33.0       0.97      0.93      0.95        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       0.92      0.86      0.89        14\n",
      "        61.0       1.00      0.95      0.97        19\n",
      "\n",
      "    accuracy                           0.90       750\n",
      "   macro avg       0.93      0.91      0.92       750\n",
      "weighted avg       0.90      0.90      0.90       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_11_age_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('clf', SVC(C=100, gamma=0.001, kernel = 'rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_11_age_pipe.fit(l11_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_11_age_pipe.predict(l11_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_11_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94, 'layer_10_speaker_age_pipe': 0.94, 'layer_11_speaker_age_pipe': 0.9}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       142\n",
      "           1       0.81      1.00      0.89       608\n",
      "\n",
      "    accuracy                           0.81       750\n",
      "   macro avg       0.41      0.50      0.45       750\n",
      "weighted avg       0.66      0.81      0.73       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_11_gender_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('clf', SVC(C=100, gamma=0.001, kernel = 'rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_11_gender_pipe.fit(l11_train_X, train_speaker_genders)\n",
    "print(classification_report(valid_speaker_genders, layer_11_gender_pipe.predict(l11_valid_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        21\n",
      "           1       0.00      0.00      0.00        11\n",
      "           2       0.00      0.00      0.00        27\n",
      "           3       0.00      0.00      0.00         8\n",
      "           4       0.00      0.00      0.00        15\n",
      "           5       0.00      0.00      0.00        11\n",
      "           6       0.71      1.00      0.83       532\n",
      "           7       0.00      0.00      0.00        32\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00        17\n",
      "          10       0.00      0.00      0.00        10\n",
      "          11       0.00      0.00      0.00        11\n",
      "          12       0.00      0.00      0.00        26\n",
      "          13       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.71       750\n",
      "   macro avg       0.05      0.07      0.06       750\n",
      "weighted avg       0.50      0.71      0.59       750\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "layer_11_accent_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('clf', SVC(C=100, gamma=0.001, kernel = 'rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_11_accent_pipe.fit(l11_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_11_accent_pipe.predict(l11_valid_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031138</td>\n",
       "      <td>0.079892</td>\n",
       "      <td>0.157382</td>\n",
       "      <td>-0.014636</td>\n",
       "      <td>-0.051778</td>\n",
       "      <td>-0.021332</td>\n",
       "      <td>-0.073593</td>\n",
       "      <td>-0.005386</td>\n",
       "      <td>-0.212557</td>\n",
       "      <td>0.099683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085248</td>\n",
       "      <td>-0.096007</td>\n",
       "      <td>-0.000766</td>\n",
       "      <td>0.021399</td>\n",
       "      <td>-0.041432</td>\n",
       "      <td>0.094806</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113040</td>\n",
       "      <td>0.175731</td>\n",
       "      <td>0.217741</td>\n",
       "      <td>-0.196254</td>\n",
       "      <td>-0.010129</td>\n",
       "      <td>-0.030586</td>\n",
       "      <td>0.067114</td>\n",
       "      <td>-0.072412</td>\n",
       "      <td>-0.239192</td>\n",
       "      <td>0.104741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090283</td>\n",
       "      <td>-0.053885</td>\n",
       "      <td>-0.010967</td>\n",
       "      <td>0.062209</td>\n",
       "      <td>-0.122958</td>\n",
       "      <td>0.192949</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.048570</td>\n",
       "      <td>0.091281</td>\n",
       "      <td>0.160776</td>\n",
       "      <td>-0.150937</td>\n",
       "      <td>0.020115</td>\n",
       "      <td>0.044117</td>\n",
       "      <td>-0.050092</td>\n",
       "      <td>-0.045661</td>\n",
       "      <td>-0.155332</td>\n",
       "      <td>0.117206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021524</td>\n",
       "      <td>-0.008411</td>\n",
       "      <td>-0.006248</td>\n",
       "      <td>0.031468</td>\n",
       "      <td>-0.056915</td>\n",
       "      <td>0.154731</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.039212</td>\n",
       "      <td>0.118388</td>\n",
       "      <td>0.173831</td>\n",
       "      <td>-0.096659</td>\n",
       "      <td>-0.008702</td>\n",
       "      <td>0.061298</td>\n",
       "      <td>0.008974</td>\n",
       "      <td>-0.003277</td>\n",
       "      <td>-0.065046</td>\n",
       "      <td>0.095480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071936</td>\n",
       "      <td>-0.023120</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>-0.121892</td>\n",
       "      <td>0.072796</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.056019</td>\n",
       "      <td>0.170639</td>\n",
       "      <td>0.157917</td>\n",
       "      <td>-0.228605</td>\n",
       "      <td>-0.065965</td>\n",
       "      <td>-0.088732</td>\n",
       "      <td>-0.082243</td>\n",
       "      <td>-0.080568</td>\n",
       "      <td>-0.341500</td>\n",
       "      <td>0.142430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155621</td>\n",
       "      <td>-0.079447</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>-0.151966</td>\n",
       "      <td>0.169634</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.031138   0.079892   0.157382  -0.014636  -0.051778  -0.021332   \n",
       "1   0.113040   0.175731   0.217741  -0.196254  -0.010129  -0.030586   \n",
       "2   0.048570   0.091281   0.160776  -0.150937   0.020115   0.044117   \n",
       "3   0.039212   0.118388   0.173831  -0.096659  -0.008702   0.061298   \n",
       "4   0.056019   0.170639   0.157917  -0.228605  -0.065965  -0.088732   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0  -0.073593  -0.005386  -0.212557    0.099683  ...    -0.085248    -0.096007   \n",
       "1   0.067114  -0.072412  -0.239192    0.104741  ...    -0.090283    -0.053885   \n",
       "2  -0.050092  -0.045661  -0.155332    0.117206  ...    -0.021524    -0.008411   \n",
       "3   0.008974  -0.003277  -0.065046    0.095480  ...    -0.071936    -0.023120   \n",
       "4  -0.082243  -0.080568  -0.341500    0.142430  ...    -0.155621    -0.079447   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0    -0.000766     0.021399    -0.041432     0.094806       45      NaN   \n",
       "1    -0.010967     0.062209    -0.122958     0.192949       45      NaN   \n",
       "2    -0.006248     0.031468    -0.056915     0.154731       45      NaN   \n",
       "3    -0.007812     0.057600    -0.121892     0.072796       45      NaN   \n",
       "4     0.015316     0.127726    -0.151966     0.169634       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_train_df = pd.read_csv(layer_12_train_csv_file_path)\n",
    "l12_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l12_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean: 28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l12_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean:\", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l12_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "l12_train_X = l12_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "l12_valid_df = pd.read_csv(layer_12_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l12_valid_df.shape)\n",
    "print(\"null values row count: \", l12_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l12_valid_df.isnull().sum().sum() / l12_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l12_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l12_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l12_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "l12_valid_X = l12_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.15      0.27        13\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.08      0.08      0.08        12\n",
      "           4       0.08      0.06      0.07        16\n",
      "           5       0.00      0.00      0.00        18\n",
      "           6       0.45      0.56      0.50         9\n",
      "           7       0.40      0.47      0.43        17\n",
      "           8       0.50      0.07      0.12        14\n",
      "           9       0.67      0.55      0.60        11\n",
      "          10       0.60      0.38      0.46         8\n",
      "          11       0.36      0.47      0.41        19\n",
      "          12       0.05      0.43      0.09         7\n",
      "          13       0.50      0.09      0.15        11\n",
      "          14       0.50      0.20      0.29        15\n",
      "          15       0.13      0.29      0.18        17\n",
      "          16       0.67      0.14      0.24        14\n",
      "          17       0.80      0.29      0.42        14\n",
      "          18       0.25      0.05      0.09        19\n",
      "          19       0.25      0.20      0.22        15\n",
      "          20       0.11      0.09      0.10        11\n",
      "          21       0.00      0.00      0.00        14\n",
      "          22       0.40      0.36      0.38        11\n",
      "          23       0.26      0.60      0.36        10\n",
      "          24       0.00      0.00      0.00        12\n",
      "          25       0.67      0.18      0.29        11\n",
      "          26       1.00      0.43      0.60         7\n",
      "          27       0.80      0.25      0.38        16\n",
      "          28       0.47      0.64      0.54        11\n",
      "          29       0.00      0.00      0.00        15\n",
      "          30       0.14      0.25      0.18         8\n",
      "          31       0.50      0.08      0.14        12\n",
      "          32       0.29      0.22      0.25         9\n",
      "          33       0.14      0.50      0.22         6\n",
      "          34       0.50      0.15      0.24        13\n",
      "          35       0.38      0.38      0.38         8\n",
      "          36       0.40      0.11      0.17        18\n",
      "          37       0.17      0.25      0.21        16\n",
      "          38       0.13      0.64      0.22        11\n",
      "          39       0.60      0.23      0.33        13\n",
      "          40       1.00      0.17      0.29        12\n",
      "          41       0.11      0.20      0.14        10\n",
      "          42       0.14      0.50      0.22        12\n",
      "          43       0.33      0.08      0.13        12\n",
      "          44       0.67      0.21      0.32        19\n",
      "          45       0.38      0.21      0.27        14\n",
      "          46       0.10      0.09      0.10        11\n",
      "          47       1.00      0.50      0.67         8\n",
      "          48       1.00      0.06      0.11        17\n",
      "          49       0.29      0.31      0.30        13\n",
      "          50       0.14      0.46      0.22        13\n",
      "          51       0.11      0.12      0.12         8\n",
      "          52       0.18      0.64      0.29        11\n",
      "          53       0.80      0.27      0.40        15\n",
      "          54       0.29      0.22      0.25         9\n",
      "          55       0.50      0.12      0.20         8\n",
      "          56       0.05      0.20      0.08        10\n",
      "          57       1.00      0.56      0.71        18\n",
      "          58       0.54      0.35      0.42        20\n",
      "          59       0.75      0.30      0.43        10\n",
      "          60       0.75      0.30      0.43        10\n",
      "\n",
      "    accuracy                           0.25       750\n",
      "   macro avg       0.41      0.26      0.26       750\n",
      "weighted avg       0.41      0.25      0.26       750\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "best_classifier__weights = 'distance' \n",
    "best_classifier__p = 2 \n",
    "best_classifier__n_neighbors = 7\n",
    "\n",
    "layer_12_speaker_ID_pipe = Pipeline([\n",
    "    ('classifier', KNeighborsClassifier(weights=best_classifier__weights, p=best_classifier__p, n_neighbors=best_classifier__n_neighbors))\n",
    "])\n",
    "\n",
    "layer_12_speaker_ID_pipe.fit(l12_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_12_speaker_ID_pipe.predict(l12_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_12_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96, 'layer_10_speaker_ID_pipe': 0.97, 'layer_11_speaker_ID_pipe': 0.95, 'layer_12_speaker_ID_pipe': 0.25}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.33      0.31      0.32        36\n",
      "        23.0       0.19      0.25      0.21        71\n",
      "        24.0       0.20      0.46      0.28        46\n",
      "        25.0       0.31      0.14      0.19        79\n",
      "        26.0       0.29      0.30      0.30       115\n",
      "        27.0       0.51      0.37      0.43        81\n",
      "        28.0       0.26      0.43      0.33        60\n",
      "        29.0       0.39      0.40      0.40        45\n",
      "        30.0       0.25      0.15      0.18        48\n",
      "        31.0       0.37      0.17      0.23        65\n",
      "        32.0       0.18      0.55      0.27        11\n",
      "        33.0       0.45      0.43      0.44        30\n",
      "        34.0       0.23      0.45      0.30        11\n",
      "        35.0       0.86      0.55      0.67        11\n",
      "        36.0       1.00      0.25      0.40         8\n",
      "        41.0       0.50      0.07      0.12        14\n",
      "        61.0       0.80      0.21      0.33        19\n",
      "\n",
      "    accuracy                           0.30       750\n",
      "   macro avg       0.42      0.32      0.32       750\n",
      "weighted avg       0.35      0.30      0.30       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_12_speaker_ages_pipe = Pipeline([\n",
    "    # ('scaler', StandardScaler()),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "layer_12_speaker_ages_pipe.fit(l12_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_12_speaker_ages_pipe.predict(l12_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_12_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94, 'layer_10_speaker_age_pipe': 0.94, 'layer_11_speaker_age_pipe': 0.9, 'layer_12_speaker_age_pipe': 0.3}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_12_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.94      0.76       142\n",
      "           1       0.98      0.88      0.93       608\n",
      "\n",
      "    accuracy                           0.89       750\n",
      "   macro avg       0.81      0.91      0.84       750\n",
      "weighted avg       0.92      0.89      0.89       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_classifier__solver = 'liblinear'\n",
    "best_classifier__penalty = 'l2'\n",
    "best_classifier__C = 10\n",
    "\n",
    "layer_12_speaker_gender_pipe = Pipeline([\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', LogisticRegression(class_weight=\"balanced\", solver=best_classifier__solver, penalty=best_classifier__penalty, C=best_classifier__C))\n",
    "    ])\n",
    "\n",
    "layer_12_speaker_gender_pipe.fit(l12_train_X, train_speaker_genders)\n",
    "print(classification_report(valid_speaker_genders, layer_12_speaker_gender_pipe.predict(l12_valid_X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        21\n",
      "           1       0.00      0.00      0.00        11\n",
      "           2       0.00      0.00      0.00        27\n",
      "           3       0.00      0.00      0.00         8\n",
      "           4       0.08      0.07      0.07        15\n",
      "           5       0.00      0.00      0.00        11\n",
      "           6       0.71      0.98      0.82       532\n",
      "           7       0.00      0.00      0.00        32\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.00      0.00      0.00        17\n",
      "          10       0.00      0.00      0.00        10\n",
      "          11       0.00      0.00      0.00        11\n",
      "          12       0.00      0.00      0.00        26\n",
      "          13       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.69       750\n",
      "   macro avg       0.06      0.07      0.06       750\n",
      "weighted avg       0.50      0.69      0.58       750\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "best_classifier__weights = 'uniform'\n",
    "best_classifier__p = 3\n",
    "best_classifier__n_neighbors = 8\n",
    "\n",
    "layer_12_speaker_accent_pipe = Pipeline([\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('classifier', KNeighborsClassifier(p=best_classifier__p, n_neighbors=best_classifier__n_neighbors, weights=best_classifier__weights))\n",
    "    ])\n",
    "\n",
    "layer_12_speaker_accent_pipe.fit(l12_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_12_speaker_accent_pipe.predict(l12_valid_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all 6 layers' datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all 6 layers' datasets\n",
    "combined_train_X = pd.concat([l7_train_X, l8_train_X], axis=1)\n",
    "combined_valid_X = pd.concat([l7_valid_X, l8_valid_X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23532\\644083621.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m pipe = Pipeline([\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombined_train_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m768\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_7_speaker_ID_pipe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m ])\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[0mPipeline\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \"\"\"\n\u001b[0;32m    400\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;31m# shallow copy of steps - this should really be steps_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[1;31m# Setup the memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mtransformers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             if not (hasattr(t, \"fit\") or hasattr(t, \"fit_transform\")) or not hasattr(\n\u001b[0;32m    228\u001b[0m                 \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transform\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1525\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1527\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[1;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"data\", combined_train_X[0:768]),\n",
    "    ('model', layer_7_speaker_ID_pipe)\n",
    "])\n",
    "\n",
    "pipe.fit(combined_train_X, train_speaker_IDs)\n",
    "print(classification_report(valid_speaker_IDs, pipe.predict(combined_valid_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           1       0.93      1.00      0.96        13\\n           2       1.00      1.00      1.00         9\\n           3       0.92      1.00      0.96        12\\n           4       0.89      1.00      0.94        16\\n           5       1.00      1.00      1.00        18\\n           6       1.00      1.00      1.00         9\\n           7       1.00      1.00      1.00        17\\n           8       1.00      1.00      1.00        14\\n           9       1.00      0.91      0.95        11\\n          10       1.00      1.00      1.00         8\\n          11       1.00      1.00      1.00        19\\n          12       1.00      1.00      1.00         7\\n          13       1.00      0.91      0.95        11\\n          14       0.94      1.00      0.97        15\\n          15       1.00      0.94      0.97        17\\n          16       1.00      1.00      1.00        14\\n          17       1.00      1.00      1.00        14\\n          18       1.00      0.95      0.97        19\\n          19       0.93      0.93      0.93        15\\n          20       1.00      1.00      1.00        11\\n          21       0.93      0.93      0.93        14\\n          22       1.00      1.00      1.00        11\\n          23       0.83      1.00      0.91        10\\n          24       1.00      1.00      1.00        12\\n          25       1.00      0.91      0.95        11\\n          26       1.00      1.00      1.00         7\\n          27       1.00      1.00      1.00        16\\n          28       1.00      1.00      1.00        11\\n          29       1.00      0.93      0.97        15\\n          30       1.00      1.00      1.00         8\\n          31       1.00      0.92      0.96        12\\n          32       1.00      0.89      0.94         9\\n          33       1.00      1.00      1.00         6\\n          34       1.00      1.00      1.00        13\\n          35       0.80      1.00      0.89         8\\n          36       0.95      1.00      0.97        18\\n          37       1.00      1.00      1.00        16\\n          38       1.00      1.00      1.00        11\\n          39       1.00      1.00      1.00        13\\n          40       0.92      0.92      0.92        12\\n          41       1.00      1.00      1.00        10\\n          42       1.00      1.00      1.00        12\\n          43       1.00      1.00      1.00        12\\n          44       1.00      0.95      0.97        19\\n          45       1.00      1.00      1.00        14\\n          46       1.00      1.00      1.00        11\\n          47       1.00      0.88      0.93         8\\n          48       0.94      1.00      0.97        17\\n          49       1.00      1.00      1.00        13\\n          50       1.00      1.00      1.00        13\\n          51       0.89      1.00      0.94         8\\n          52       1.00      0.91      0.95        11\\n          53       1.00      1.00      1.00        15\\n          54       1.00      0.89      0.94         9\\n          55       1.00      1.00      1.00         8\\n          56       1.00      1.00      1.00        10\\n          57       1.00      1.00      1.00        18\\n          58       1.00      1.00      1.00        20\\n          59       1.00      1.00      1.00        10\\n          60       1.00      1.00      1.00        10\\n\\n    accuracy                           0.98       750\\n   macro avg       0.98      0.98      0.98       750\\nweighted avg       0.98      0.98      0.98       750\\n'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('model1', layer_7_speaker_ID_pipe),\n",
    "    ('model2', layer_9_speaker_ID_pipe),\n",
    "    ('model3', layer_10_speaker_ID_pipe),\n",
    "    ('model4', layer_12_speaker_ID_pipe),\n",
    "    # ('model3', model3),\n",
    "    # ('model4', model4)\n",
    "], voting='hard')  # 'hard' for majority voting\n",
    "\n",
    "# Fit the ensemble on your data\n",
    "ensemble.fit(l7_train_X, train_speaker_IDs)\n",
    "\n",
    "# Make predictions\n",
    "predictions = ensemble.predict(valid_X)\n",
    "classification_report(valid_speaker_IDs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for speaker_IDs in validation dataset from each layer model\n",
    "layer_7_speaker_ID_predictions = layer_7_speaker_ID_pipe.predict(l7_valid_X)\n",
    "layer_8_speaker_ID_predictions = layer_8_speaker_ID_pipe.predict(l8_valid_X)\n",
    "layer_9_speaker_ID_predictions = layer_9_speaker_ID_pipe.predict(l9_valid_X)\n",
    "layer_10_speaker_ID_predictions = layer_10_speaker_ID_pipe.predict(l10_valid_X)\n",
    "layer_11_speaker_ID_predictions = layer_11_speaker_ID_pipe.predict(l11_valid_X)\n",
    "layer_12_speaker_ID_predictions = layer_12_speaker_ID_pipe.predict(l12_valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions from all layers into a 2D array\n",
    "all_predictions = np.vstack([\n",
    "    layer_7_speaker_ID_predictions,\n",
    "    layer_8_speaker_ID_predictions,\n",
    "    layer_9_speaker_ID_predictions,\n",
    "    layer_10_speaker_ID_predictions,\n",
    "    layer_11_speaker_ID_predictions,\n",
    "    layer_12_speaker_ID_predictions\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96, 'layer_10_speaker_ID_pipe': 0.97, 'layer_11_speaker_ID_pipe': 0.95, 'layer_12_speaker_ID_pipe': 0.25}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750,)\n",
      "(750, 768) (750, 768) (750, 768) (750, 768) (750, 768) (750, 768)\n"
     ]
    }
   ],
   "source": [
    "print(layer_7_speaker_ID_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 750)\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using just the majority prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    # print(predictions, majority_prediction)\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.94      1.00      0.97        16\n",
      "           5       1.00      0.94      0.97        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      0.94      0.97        17\n",
      "           8       1.00      0.93      0.96        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      0.95      0.97        19\n",
      "          12       0.88      1.00      0.93         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      0.88      0.94        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       1.00      1.00      1.00        14\n",
      "          18       1.00      0.95      0.97        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       0.93      1.00      0.97        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.86      1.00      0.92        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.94      1.00      0.97        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      0.88      0.93         8\n",
      "          31       0.85      0.92      0.88        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.85      0.92        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       0.86      1.00      0.92        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       0.93      1.00      0.97        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       0.89      1.00      0.94         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       0.91      1.00      0.95        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       0.91      1.00      0.95        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.97      0.98      0.97       750\n",
      "weighted avg       0.98      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 0.93, 0.96, 0.97, 0.95, 0.25])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ID_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 4:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       1.00      1.00      1.00        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      1.00      1.00        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      1.00      0.97        14\n",
      "          18       1.00      1.00      1.00        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       1.00      0.93      0.96        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       1.00      1.00      1.00        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      1.00      1.00        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.93      0.97        15\n",
      "          30       1.00      1.00      1.00         8\n",
      "          31       1.00      1.00      1.00        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       1.00      1.00      1.00         6\n",
      "          34       1.00      1.00      1.00        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       1.00      1.00      1.00        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       1.00      1.00      1.00        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       0.91      1.00      0.95        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       0.99      0.99      0.99       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99, 0.93, 0.96, 0.97, 0.95, 0.25]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(speaker_ID_preds_confidences.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 3:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       1.00      0.94      0.97        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      1.00      1.00        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      1.00      0.97        14\n",
      "          18       1.00      1.00      1.00        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       1.00      0.93      0.96        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.92      1.00      0.96        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      1.00      1.00        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      1.00      1.00         8\n",
      "          31       0.92      0.92      0.92        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.92      0.96        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       1.00      1.00      1.00        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      1.00      1.00        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.98       750\n",
      "   macro avg       0.98      0.98      0.98       750\n",
      "weighted avg       0.99      0.98      0.98       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking based ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_7_train_speaker_ID_predictions = layer_7_speaker_ID_pipe.predict(l7_train_X)\n",
    "layer_8_train_speaker_ID_predictions = layer_8_speaker_ID_pipe.predict(l8_train_X)\n",
    "layer_9_train_speaker_ID_predictions = layer_9_speaker_ID_pipe.predict(l9_train_X)\n",
    "layer_10_train_speaker_ID_predictions = layer_10_speaker_ID_pipe.predict(l10_train_X)\n",
    "layer_11_train_speaker_ID_predictions = layer_11_speaker_ID_pipe.predict(l11_train_X)\n",
    "layer_12_train_speaker_ID_predictions = layer_12_speaker_ID_pipe.predict(l12_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_meta = [\n",
    "    layer_7_train_speaker_ID_predictions,\n",
    "    layer_8_train_speaker_ID_predictions,\n",
    "    layer_9_train_speaker_ID_predictions,\n",
    "    layer_10_train_speaker_ID_predictions,\n",
    "    layer_11_train_speaker_ID_predictions,\n",
    "    layer_12_train_speaker_ID_predictions\n",
    "]\n",
    "\n",
    "X_meta = np.vstack(X_meta).T  # Transpose the matrix to have predictions as columns\n",
    "\n",
    "X_train_meta, X_valid_meta, y_train_meta, y_valid_meta = train_test_split(X_meta, train_speaker_IDs, test_size=0.2, random_state=42)\n",
    "\n",
    "meta_model = SVC(C=40, class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28520, 6)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=40)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=40)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=40)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacked Ensemble: 0.9991234221598878\n"
     ]
    }
   ],
   "source": [
    "meta_predictions = meta_model.predict(X_valid_meta)\n",
    "stacked_ensemble_accuracy = accuracy_score(y_valid_meta, meta_predictions)\n",
    "\n",
    "print(\"Accuracy of Stacked Ensemble (using hold-out method):\", stacked_ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_data_from_validation_dataset = [\n",
    "    layer_7_speaker_ID_predictions,\n",
    "    layer_8_speaker_ID_predictions,\n",
    "    layer_9_speaker_ID_predictions,\n",
    "    layer_10_speaker_ID_predictions,\n",
    "    layer_11_speaker_ID_predictions,\n",
    "    layer_12_speaker_ID_predictions\n",
    "]\n",
    "\n",
    "X_meta_data_from_validation_dataset = np.vstack(X_meta_data_from_validation_dataset).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.15      0.27        13\n",
      "           2       0.40      0.22      0.29         9\n",
      "           3       0.25      0.08      0.12        12\n",
      "           4       0.18      0.12      0.15        16\n",
      "           5       0.00      0.00      0.00        18\n",
      "           6       0.45      0.56      0.50         9\n",
      "           7       0.80      0.47      0.59        17\n",
      "           8       0.11      0.07      0.09        14\n",
      "           9       0.55      0.55      0.55        11\n",
      "          10       0.36      0.50      0.42         8\n",
      "          11       0.69      0.58      0.63        19\n",
      "          12       0.50      0.43      0.46         7\n",
      "          13       0.50      0.36      0.42        11\n",
      "          14       0.09      0.27      0.14        15\n",
      "          15       0.31      0.29      0.30        17\n",
      "          16       0.27      0.21      0.24        14\n",
      "          17       0.50      0.36      0.42        14\n",
      "          18       0.11      0.05      0.07        19\n",
      "          19       0.27      0.20      0.23        15\n",
      "          20       0.08      0.09      0.08        11\n",
      "          21       0.12      0.14      0.13        14\n",
      "          22       0.22      0.36      0.28        11\n",
      "          23       0.24      0.70      0.36        10\n",
      "          24       0.00      0.00      0.00        12\n",
      "          25       0.18      0.18      0.18        11\n",
      "          26       0.21      0.43      0.29         7\n",
      "          27       0.27      0.19      0.22        16\n",
      "          28       0.58      0.64      0.61        11\n",
      "          29       0.00      0.00      0.00        15\n",
      "          30       0.12      0.25      0.16         8\n",
      "          31       0.12      0.17      0.14        12\n",
      "          32       0.20      0.22      0.21         9\n",
      "          33       0.18      0.50      0.26         6\n",
      "          34       0.23      0.23      0.23        13\n",
      "          35       0.17      0.38      0.23         8\n",
      "          36       0.33      0.22      0.27        18\n",
      "          37       0.18      0.25      0.21        16\n",
      "          38       0.43      0.55      0.48        11\n",
      "          39       0.25      0.23      0.24        13\n",
      "          40       0.50      0.33      0.40        12\n",
      "          41       0.14      0.20      0.17        10\n",
      "          42       0.43      0.50      0.46        12\n",
      "          43       0.17      0.25      0.20        12\n",
      "          44       0.43      0.32      0.36        19\n",
      "          45       0.19      0.21      0.20        14\n",
      "          46       0.12      0.18      0.14        11\n",
      "          47       0.40      0.50      0.44         8\n",
      "          48       0.22      0.12      0.15        17\n",
      "          49       0.67      0.31      0.42        13\n",
      "          50       0.50      0.54      0.52        13\n",
      "          51       0.15      0.25      0.19         8\n",
      "          52       0.41      0.64      0.50        11\n",
      "          53       0.62      0.33      0.43        15\n",
      "          54       0.11      0.22      0.15         9\n",
      "          55       0.25      0.12      0.17         8\n",
      "          56       0.22      0.20      0.21        10\n",
      "          57       0.86      0.67      0.75        18\n",
      "          58       0.88      0.35      0.50        20\n",
      "          59       0.75      0.30      0.43        10\n",
      "          60       1.00      0.30      0.46        10\n",
      "\n",
      "    accuracy                           0.29       750\n",
      "   macro avg       0.34      0.30      0.30       750\n",
      "weighted avg       0.35      0.29      0.30       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, meta_model.predict(X_meta_data_from_validation_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 0.93, 0.96, 0.97, 0.95, 0.25])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ID_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(SVC(probability=True, class_weight=\"balanced\"), param_grid, cv=3, scoring='accuracy', verbose=3, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to your data\n",
    "grid_search.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best__C = PUT THE BEST ONE HERE\n",
    "# best__gamma = PUT THE BEST ONE HERE\n",
    "\n",
    "best_meta_model = SVC(C=best__C, kernel=best__kernel, gamma=best__gamma, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_8_age_pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\ACA semester 7\\CS4622 - Machine Learning\\ML-Project\\Combining_all_layers\\Combined_all_layers.ipynb Cell 265\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ACA%20semester%207/CS4622%20-%20Machine%20Learning/ML-Project/Combining_all_layers/Combined_all_layers.ipynb#Z1130sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Predictions for speaker_IDs in validation dataset from each layer model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ACA%20semester%207/CS4622%20-%20Machine%20Learning/ML-Project/Combining_all_layers/Combined_all_layers.ipynb#Z1130sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# layer_7_speaker_age_predictions = layer_7_speaker_age_pipe.predict(l7_valid_X)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/ACA%20semester%207/CS4622%20-%20Machine%20Learning/ML-Project/Combining_all_layers/Combined_all_layers.ipynb#Z1130sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m layer_8_speaker_age_predictions \u001b[39m=\u001b[39m layer_8_age_pipe\u001b[39m.\u001b[39mpredict(l8_valid_X)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ACA%20semester%207/CS4622%20-%20Machine%20Learning/ML-Project/Combining_all_layers/Combined_all_layers.ipynb#Z1130sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m layer_9_speaker_age_predictions \u001b[39m=\u001b[39m layer_9_age_pipe\u001b[39m.\u001b[39mpredict(l9_valid_X)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ACA%20semester%207/CS4622%20-%20Machine%20Learning/ML-Project/Combining_all_layers/Combined_all_layers.ipynb#Z1130sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m layer_10_speaker_age_predictions \u001b[39m=\u001b[39m layer_10_age_pipe\u001b[39m.\u001b[39mpredict(l10_valid_X)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'layer_8_age_pipe' is not defined"
     ]
    }
   ],
   "source": [
    "# Predictions for speaker_IDs in validation dataset from each layer model\n",
    "layer_7_speaker_age_predictions = layer_7_speaker_age_pipe.predict(l7_valid_X)\n",
    "layer_8_speaker_age_predictions = layer_8_age_pipe.predict(l8_valid_X)\n",
    "layer_9_speaker_age_predictions = layer_9_age_pipe.predict(l9_valid_X)\n",
    "layer_10_speaker_age_predictions = layer_10_age_pipe.predict(l10_valid_X)\n",
    "layer_11_speaker_age_predictions = layer_11_age_pipe.predict(l11_valid_X)\n",
    "layer_12_speaker_age_predictions = layer_12_speaker_ages_pipe.predict(l12_valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions from all layers into a 2D array\n",
    "all_predictions = np.vstack([\n",
    "    layer_7_speaker_ID_predictions,\n",
    "    layer_8_speaker_ID_predictions,\n",
    "    layer_9_speaker_ID_predictions,\n",
    "    layer_10_speaker_ID_predictions,\n",
    "    layer_11_speaker_ID_predictions,\n",
    "    layer_12_speaker_ID_predictions\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96, 'layer_10_speaker_ID_pipe': 0.97, 'layer_11_speaker_ID_pipe': 0.95, 'layer_12_speaker_ID_pipe': 0.25}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750,)\n",
      "(750, 768) (750, 768) (750, 768) (750, 768) (750, 768) (750, 768)\n"
     ]
    }
   ],
   "source": [
    "print(layer_7_speaker_ID_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 750)\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using just the majority prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    # print(predictions, majority_prediction)\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.94      1.00      0.97        16\n",
      "           5       1.00      0.94      0.97        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      0.94      0.97        17\n",
      "           8       1.00      0.93      0.96        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      0.95      0.97        19\n",
      "          12       0.88      1.00      0.93         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      0.88      0.94        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       1.00      1.00      1.00        14\n",
      "          18       1.00      0.95      0.97        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       0.93      1.00      0.97        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.86      1.00      0.92        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.94      1.00      0.97        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      0.88      0.93         8\n",
      "          31       0.85      0.92      0.88        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.85      0.92        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       0.86      1.00      0.92        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       0.93      1.00      0.97        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       0.89      1.00      0.94         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       0.91      1.00      0.95        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       0.91      1.00      0.95        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.97      0.98      0.97       750\n",
      "weighted avg       0.98      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 0.93, 0.96, 0.97, 0.95, 0.25])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speaker_ID_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 4:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       1.00      1.00      1.00        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      1.00      1.00        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      1.00      0.97        14\n",
      "          18       1.00      1.00      1.00        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       1.00      0.93      0.96        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       1.00      1.00      1.00        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      1.00      1.00        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.93      0.97        15\n",
      "          30       1.00      1.00      1.00         8\n",
      "          31       1.00      1.00      1.00        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       1.00      1.00      1.00         6\n",
      "          34       1.00      1.00      1.00        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       1.00      1.00      1.00        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       1.00      1.00      1.00        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       0.91      1.00      0.95        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       0.99      0.99      0.99       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99, 0.93, 0.96, 0.97, 0.95, 0.25]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(speaker_ID_preds_confidences.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 3:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       1.00      0.94      0.97        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      1.00      1.00        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      1.00      0.97        14\n",
      "          18       1.00      1.00      1.00        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       1.00      0.93      0.96        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.92      1.00      0.96        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      1.00      1.00        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      1.00      1.00         8\n",
      "          31       0.92      0.92      0.92        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.92      0.96        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       1.00      1.00      1.00        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      1.00      1.00        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.98       750\n",
      "   macro avg       0.98      0.98      0.98       750\n",
      "weighted avg       0.99      0.98      0.98       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking based ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_7_train_speaker_ID_predictions = layer_7_speaker_ID_pipe.predict(l7_train_X)\n",
    "layer_8_train_speaker_ID_predictions = layer_8_speaker_ID_pipe.predict(l8_train_X)\n",
    "layer_9_train_speaker_ID_predictions = layer_9_speaker_ID_pipe.predict(l9_train_X)\n",
    "layer_10_train_speaker_ID_predictions = layer_10_speaker_ID_pipe.predict(l10_train_X)\n",
    "layer_11_train_speaker_ID_predictions = layer_11_speaker_ID_pipe.predict(l11_train_X)\n",
    "layer_12_train_speaker_ID_predictions = layer_12_speaker_ID_pipe.predict(l12_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_meta = [\n",
    "    layer_7_train_speaker_ID_predictions,\n",
    "    layer_8_train_speaker_ID_predictions,\n",
    "    layer_9_train_speaker_ID_predictions,\n",
    "    layer_10_train_speaker_ID_predictions,\n",
    "    layer_11_train_speaker_ID_predictions,\n",
    "    layer_12_train_speaker_ID_predictions\n",
    "]\n",
    "\n",
    "X_meta = np.vstack(X_meta).T  # Transpose the matrix to have predictions as columns\n",
    "\n",
    "X_train_meta, X_valid_meta, y_train_meta, y_valid_meta = train_test_split(X_meta, train_speaker_IDs, test_size=0.2, random_state=42)\n",
    "\n",
    "meta_model = SVC(C=40, class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28520, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=40)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=40)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=40)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_model.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacked Ensemble: 0.9991234221598878\n"
     ]
    }
   ],
   "source": [
    "meta_predictions = meta_model.predict(X_valid_meta)\n",
    "stacked_ensemble_accuracy = accuracy_score(y_valid_meta, meta_predictions)\n",
    "\n",
    "print(\"Accuracy of Stacked Ensemble (using hold-out method):\", stacked_ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_data_from_validation_dataset = [\n",
    "    layer_7_speaker_ID_predictions,\n",
    "    layer_8_speaker_ID_predictions,\n",
    "    layer_9_speaker_ID_predictions,\n",
    "    layer_10_speaker_ID_predictions,\n",
    "    layer_11_speaker_ID_predictions,\n",
    "    layer_12_speaker_ID_predictions\n",
    "]\n",
    "\n",
    "X_meta_data_from_validation_dataset = np.vstack(X_meta_data_from_validation_dataset).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.15      0.27        13\n",
      "           2       0.40      0.22      0.29         9\n",
      "           3       0.25      0.08      0.12        12\n",
      "           4       0.18      0.12      0.15        16\n",
      "           5       0.00      0.00      0.00        18\n",
      "           6       0.45      0.56      0.50         9\n",
      "           7       0.80      0.47      0.59        17\n",
      "           8       0.11      0.07      0.09        14\n",
      "           9       0.55      0.55      0.55        11\n",
      "          10       0.36      0.50      0.42         8\n",
      "          11       0.69      0.58      0.63        19\n",
      "          12       0.50      0.43      0.46         7\n",
      "          13       0.50      0.36      0.42        11\n",
      "          14       0.09      0.27      0.14        15\n",
      "          15       0.31      0.29      0.30        17\n",
      "          16       0.27      0.21      0.24        14\n",
      "          17       0.50      0.36      0.42        14\n",
      "          18       0.11      0.05      0.07        19\n",
      "          19       0.27      0.20      0.23        15\n",
      "          20       0.08      0.09      0.08        11\n",
      "          21       0.12      0.14      0.13        14\n",
      "          22       0.22      0.36      0.28        11\n",
      "          23       0.24      0.70      0.36        10\n",
      "          24       0.00      0.00      0.00        12\n",
      "          25       0.18      0.18      0.18        11\n",
      "          26       0.21      0.43      0.29         7\n",
      "          27       0.27      0.19      0.22        16\n",
      "          28       0.58      0.64      0.61        11\n",
      "          29       0.00      0.00      0.00        15\n",
      "          30       0.12      0.25      0.16         8\n",
      "          31       0.12      0.17      0.14        12\n",
      "          32       0.20      0.22      0.21         9\n",
      "          33       0.18      0.50      0.26         6\n",
      "          34       0.23      0.23      0.23        13\n",
      "          35       0.17      0.38      0.23         8\n",
      "          36       0.33      0.22      0.27        18\n",
      "          37       0.18      0.25      0.21        16\n",
      "          38       0.43      0.55      0.48        11\n",
      "          39       0.25      0.23      0.24        13\n",
      "          40       0.50      0.33      0.40        12\n",
      "          41       0.14      0.20      0.17        10\n",
      "          42       0.43      0.50      0.46        12\n",
      "          43       0.17      0.25      0.20        12\n",
      "          44       0.43      0.32      0.36        19\n",
      "          45       0.19      0.21      0.20        14\n",
      "          46       0.12      0.18      0.14        11\n",
      "          47       0.40      0.50      0.44         8\n",
      "          48       0.22      0.12      0.15        17\n",
      "          49       0.67      0.31      0.42        13\n",
      "          50       0.50      0.54      0.52        13\n",
      "          51       0.15      0.25      0.19         8\n",
      "          52       0.41      0.64      0.50        11\n",
      "          53       0.62      0.33      0.43        15\n",
      "          54       0.11      0.22      0.15         9\n",
      "          55       0.25      0.12      0.17         8\n",
      "          56       0.22      0.20      0.21        10\n",
      "          57       0.86      0.67      0.75        18\n",
      "          58       0.88      0.35      0.50        20\n",
      "          59       0.75      0.30      0.43        10\n",
      "          60       1.00      0.30      0.46        10\n",
      "\n",
      "    accuracy                           0.29       750\n",
      "   macro avg       0.34      0.30      0.30       750\n",
      "weighted avg       0.35      0.29      0.30       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, meta_model.predict(X_meta_data_from_validation_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 0.93, 0.96, 0.97, 0.95, 0.25])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speaker_ID_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(SVC(probability=True, class_weight=\"balanced\"), param_grid, cv=3, scoring='accuracy', verbose=3, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to your data\n",
    "grid_search.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best__C = \n",
    "best__gamma =\n",
    "\n",
    "best_meta_model = SVC(C=best__C, kernel=best__kernel, gamma=best__gamma, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble_predictions = []\n",
    "# # List of confidence values (weighted f1 scores) for each model\n",
    "# confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "# # Iterate through each data point \n",
    "# for i in range(all_predictions.shape[1]):\n",
    "#     layer_7_pred = all_predictions[0][i]\n",
    "#     layer_8_pred = all_predictions[1][i]\n",
    "#     layer_9_pred = all_predictions[2][i]\n",
    "#     layer_10_pred = all_predictions[3][i]\n",
    "#     layer_11_pred = all_predictions[4][i]\n",
    "#     layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "#     predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "#     total = sum(confidences)\n",
    "#     normalized_confidences = [value / total for value in confidences]\n",
    "    \n",
    "#     weighted_mean = sum(round(prediction * n_confidence) for prediction, n_confidence in zip(predictions, normalized_confidences)) / sum(normalized_confidences)\n",
    "\n",
    "#     print(predictions, weighted_mean, valid_speaker_IDs[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
