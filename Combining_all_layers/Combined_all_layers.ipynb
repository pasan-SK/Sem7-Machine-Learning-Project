{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np          # For mathematical calculations\n",
    "import matplotlib.pyplot as plt  # For plotting graphs\n",
    "# import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "layer_7_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\train.csv\"\n",
    "layer_7_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\valid.csv\"\n",
    "layer_7_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\test.csv\"\n",
    "\n",
    "layer_8_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer8\\\\train.csv\"\n",
    "layer_8_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer8\\\\valid.csv\"\n",
    "layer_8_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer8\\\\test.csv\"\n",
    "\n",
    "layer_9_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer9\\\\train.csv\"\n",
    "layer_9_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer9\\\\valid.csv\"\n",
    "layer_9_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer9\\\\test.csv\"\n",
    "\n",
    "layer_10_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer10\\\\train.csv\"\n",
    "layer_10_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer10\\\\valid.csv\"\n",
    "layer_10_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer10\\\\test.csv\"\n",
    "\n",
    "layer_11_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer11\\\\train.csv\"\n",
    "layer_11_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer11\\\\valid.csv\"\n",
    "layer_11_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer11\\\\test.csv\"\n",
    "\n",
    "layer_12_train_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer12\\\\train.csv\"\n",
    "layer_12_valid_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer12\\\\valid.csv\"\n",
    "layer_12_test_csv_file_path = \"D:\\\\ACA semester 7\\\\CS4622 - Machine Learning\\\\ML-Project\\Data\\\\layer12\\\\test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ID_preds_confidences = {}\n",
    "speaker_age_preds_confidences = {}\n",
    "speaker_gender_preds_confidences = {}\n",
    "speaker_accent_preds_confidences = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187868</td>\n",
       "      <td>-0.009268</td>\n",
       "      <td>0.039846</td>\n",
       "      <td>-0.102515</td>\n",
       "      <td>-0.109121</td>\n",
       "      <td>-0.079769</td>\n",
       "      <td>0.061295</td>\n",
       "      <td>-0.042890</td>\n",
       "      <td>0.099860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043647</td>\n",
       "      <td>0.061174</td>\n",
       "      <td>0.026771</td>\n",
       "      <td>-0.092734</td>\n",
       "      <td>-0.052383</td>\n",
       "      <td>-0.055526</td>\n",
       "      <td>0.029101</td>\n",
       "      <td>0.041857</td>\n",
       "      <td>-0.027872</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.122119</td>\n",
       "      <td>0.019015</td>\n",
       "      <td>0.134483</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>-0.157884</td>\n",
       "      <td>-0.033332</td>\n",
       "      <td>0.044334</td>\n",
       "      <td>-0.016869</td>\n",
       "      <td>-0.014088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179798</td>\n",
       "      <td>-0.027611</td>\n",
       "      <td>0.240023</td>\n",
       "      <td>-0.005648</td>\n",
       "      <td>-0.057537</td>\n",
       "      <td>0.023922</td>\n",
       "      <td>-0.038126</td>\n",
       "      <td>-0.015171</td>\n",
       "      <td>-0.006270</td>\n",
       "      <td>-0.003790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.079187</td>\n",
       "      <td>0.146195</td>\n",
       "      <td>-0.001316</td>\n",
       "      <td>-0.085596</td>\n",
       "      <td>-0.261093</td>\n",
       "      <td>-0.021620</td>\n",
       "      <td>0.268898</td>\n",
       "      <td>-0.103950</td>\n",
       "      <td>0.068976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075460</td>\n",
       "      <td>0.037346</td>\n",
       "      <td>0.124254</td>\n",
       "      <td>-0.369695</td>\n",
       "      <td>-0.056773</td>\n",
       "      <td>-0.028471</td>\n",
       "      <td>0.019971</td>\n",
       "      <td>-0.065683</td>\n",
       "      <td>-0.047366</td>\n",
       "      <td>-0.121744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.231196</td>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.215534</td>\n",
       "      <td>-0.210258</td>\n",
       "      <td>-0.158189</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.240254</td>\n",
       "      <td>-0.068112</td>\n",
       "      <td>-0.017550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0.141642</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>0.031547</td>\n",
       "      <td>-0.010762</td>\n",
       "      <td>-0.169036</td>\n",
       "      <td>-0.030963</td>\n",
       "      <td>0.086698</td>\n",
       "      <td>0.114194</td>\n",
       "      <td>-0.036775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.020412</td>\n",
       "      <td>0.063732</td>\n",
       "      <td>-0.065506</td>\n",
       "      <td>-0.089598</td>\n",
       "      <td>-0.130788</td>\n",
       "      <td>-0.018809</td>\n",
       "      <td>0.119304</td>\n",
       "      <td>-0.057494</td>\n",
       "      <td>0.094714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001969</td>\n",
       "      <td>-0.003025</td>\n",
       "      <td>-0.021156</td>\n",
       "      <td>-0.136993</td>\n",
       "      <td>-0.003615</td>\n",
       "      <td>0.036371</td>\n",
       "      <td>0.048715</td>\n",
       "      <td>-0.002688</td>\n",
       "      <td>-0.016957</td>\n",
       "      <td>-0.004331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   1   0.187868  -0.009268   0.039846  -0.102515  -0.109121  -0.079769   \n",
       "1   2   0.122119   0.019015   0.134483   0.007949  -0.157884  -0.033332   \n",
       "2   3   0.079187   0.146195  -0.001316  -0.085596  -0.261093  -0.021620   \n",
       "3   4   0.231196   0.021558   0.215534  -0.210258  -0.158189   0.041621   \n",
       "4   5   0.020412   0.063732  -0.065506  -0.089598  -0.130788  -0.018809   \n",
       "\n",
       "   feature_7  feature_8  feature_9  ...  feature_759  feature_760  \\\n",
       "0   0.061295  -0.042890   0.099860  ...     0.043647     0.061174   \n",
       "1   0.044334  -0.016869  -0.014088  ...    -0.179798    -0.027611   \n",
       "2   0.268898  -0.103950   0.068976  ...    -0.075460     0.037346   \n",
       "3   0.240254  -0.068112  -0.017550  ...     0.083334     0.141642   \n",
       "4   0.119304  -0.057494   0.094714  ...    -0.001969    -0.003025   \n",
       "\n",
       "   feature_761  feature_762  feature_763  feature_764  feature_765  \\\n",
       "0     0.026771    -0.092734    -0.052383    -0.055526     0.029101   \n",
       "1     0.240023    -0.005648    -0.057537     0.023922    -0.038126   \n",
       "2     0.124254    -0.369695    -0.056773    -0.028471     0.019971   \n",
       "3    -0.007219     0.031547    -0.010762    -0.169036    -0.030963   \n",
       "4    -0.021156    -0.136993    -0.003615     0.036371     0.048715   \n",
       "\n",
       "   feature_766  feature_767  feature_768  \n",
       "0     0.041857    -0.027872     0.099500  \n",
       "1    -0.015171    -0.006270    -0.003790  \n",
       "2    -0.065683    -0.047366    -0.121744  \n",
       "3     0.086698     0.114194    -0.036775  \n",
       "4    -0.002688    -0.016957    -0.004331  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(layer_7_test_csv_file_path)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID               int64\n",
       "feature_1      float64\n",
       "feature_2      float64\n",
       "feature_3      float64\n",
       "feature_4      float64\n",
       "                ...   \n",
       "feature_764    float64\n",
       "feature_765    float64\n",
       "feature_766    float64\n",
       "feature_767    float64\n",
       "feature_768    float64\n",
       "Length: 769, dtype: object"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get type of each column\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             False\n",
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "               ...  \n",
       "feature_764    False\n",
       "feature_765    False\n",
       "feature_766    False\n",
       "feature_767    False\n",
       "feature_768    False\n",
       "Length: 769, dtype: bool"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum().sum()\n",
    "\n",
    "# based on above output we can see that there are no missing values in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset shape: (744, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.187868</td>\n",
       "      <td>-0.009268</td>\n",
       "      <td>0.039846</td>\n",
       "      <td>-0.102515</td>\n",
       "      <td>-0.109121</td>\n",
       "      <td>-0.079769</td>\n",
       "      <td>0.061295</td>\n",
       "      <td>-0.042890</td>\n",
       "      <td>0.099860</td>\n",
       "      <td>-0.066319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043647</td>\n",
       "      <td>0.061174</td>\n",
       "      <td>0.026771</td>\n",
       "      <td>-0.092734</td>\n",
       "      <td>-0.052383</td>\n",
       "      <td>-0.055526</td>\n",
       "      <td>0.029101</td>\n",
       "      <td>0.041857</td>\n",
       "      <td>-0.027872</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122119</td>\n",
       "      <td>0.019015</td>\n",
       "      <td>0.134483</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>-0.157884</td>\n",
       "      <td>-0.033332</td>\n",
       "      <td>0.044334</td>\n",
       "      <td>-0.016869</td>\n",
       "      <td>-0.014088</td>\n",
       "      <td>-0.060542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179798</td>\n",
       "      <td>-0.027611</td>\n",
       "      <td>0.240023</td>\n",
       "      <td>-0.005648</td>\n",
       "      <td>-0.057537</td>\n",
       "      <td>0.023922</td>\n",
       "      <td>-0.038126</td>\n",
       "      <td>-0.015171</td>\n",
       "      <td>-0.006270</td>\n",
       "      <td>-0.003790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.079187</td>\n",
       "      <td>0.146195</td>\n",
       "      <td>-0.001316</td>\n",
       "      <td>-0.085596</td>\n",
       "      <td>-0.261093</td>\n",
       "      <td>-0.021620</td>\n",
       "      <td>0.268898</td>\n",
       "      <td>-0.103950</td>\n",
       "      <td>0.068976</td>\n",
       "      <td>-0.062388</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075460</td>\n",
       "      <td>0.037346</td>\n",
       "      <td>0.124254</td>\n",
       "      <td>-0.369695</td>\n",
       "      <td>-0.056773</td>\n",
       "      <td>-0.028471</td>\n",
       "      <td>0.019971</td>\n",
       "      <td>-0.065683</td>\n",
       "      <td>-0.047366</td>\n",
       "      <td>-0.121744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.231196</td>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.215534</td>\n",
       "      <td>-0.210258</td>\n",
       "      <td>-0.158189</td>\n",
       "      <td>0.041621</td>\n",
       "      <td>0.240254</td>\n",
       "      <td>-0.068112</td>\n",
       "      <td>-0.017550</td>\n",
       "      <td>-0.233767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0.141642</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>0.031547</td>\n",
       "      <td>-0.010762</td>\n",
       "      <td>-0.169036</td>\n",
       "      <td>-0.030963</td>\n",
       "      <td>0.086698</td>\n",
       "      <td>0.114194</td>\n",
       "      <td>-0.036775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020412</td>\n",
       "      <td>0.063732</td>\n",
       "      <td>-0.065506</td>\n",
       "      <td>-0.089598</td>\n",
       "      <td>-0.130788</td>\n",
       "      <td>-0.018809</td>\n",
       "      <td>0.119304</td>\n",
       "      <td>-0.057494</td>\n",
       "      <td>0.094714</td>\n",
       "      <td>-0.047923</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001969</td>\n",
       "      <td>-0.003025</td>\n",
       "      <td>-0.021156</td>\n",
       "      <td>-0.136993</td>\n",
       "      <td>-0.003615</td>\n",
       "      <td>0.036371</td>\n",
       "      <td>0.048715</td>\n",
       "      <td>-0.002688</td>\n",
       "      <td>-0.016957</td>\n",
       "      <td>-0.004331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.187868  -0.009268   0.039846  -0.102515  -0.109121  -0.079769   \n",
       "1   0.122119   0.019015   0.134483   0.007949  -0.157884  -0.033332   \n",
       "2   0.079187   0.146195  -0.001316  -0.085596  -0.261093  -0.021620   \n",
       "3   0.231196   0.021558   0.215534  -0.210258  -0.158189   0.041621   \n",
       "4   0.020412   0.063732  -0.065506  -0.089598  -0.130788  -0.018809   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_759  feature_760  \\\n",
       "0   0.061295  -0.042890   0.099860   -0.066319  ...     0.043647     0.061174   \n",
       "1   0.044334  -0.016869  -0.014088   -0.060542  ...    -0.179798    -0.027611   \n",
       "2   0.268898  -0.103950   0.068976   -0.062388  ...    -0.075460     0.037346   \n",
       "3   0.240254  -0.068112  -0.017550   -0.233767  ...     0.083334     0.141642   \n",
       "4   0.119304  -0.057494   0.094714   -0.047923  ...    -0.001969    -0.003025   \n",
       "\n",
       "   feature_761  feature_762  feature_763  feature_764  feature_765  \\\n",
       "0     0.026771    -0.092734    -0.052383    -0.055526     0.029101   \n",
       "1     0.240023    -0.005648    -0.057537     0.023922    -0.038126   \n",
       "2     0.124254    -0.369695    -0.056773    -0.028471     0.019971   \n",
       "3    -0.007219     0.031547    -0.010762    -0.169036    -0.030963   \n",
       "4    -0.021156    -0.136993    -0.003615     0.036371     0.048715   \n",
       "\n",
       "   feature_766  feature_767  feature_768  \n",
       "0     0.041857    -0.027872     0.099500  \n",
       "1    -0.015171    -0.006270    -0.003790  \n",
       "2    -0.065683    -0.047366    -0.121744  \n",
       "3     0.086698     0.114194    -0.036775  \n",
       "4    -0.002688    -0.016957    -0.004331  \n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = test_df.drop([\"ID\"], axis=1)\n",
    "print(\"test dataset shape:\", test_X.shape)\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.186257</td>\n",
       "      <td>-0.058807</td>\n",
       "      <td>0.024632</td>\n",
       "      <td>-0.163933</td>\n",
       "      <td>-0.146699</td>\n",
       "      <td>0.035889</td>\n",
       "      <td>0.111708</td>\n",
       "      <td>-0.162861</td>\n",
       "      <td>0.028249</td>\n",
       "      <td>-0.098063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055629</td>\n",
       "      <td>-0.010358</td>\n",
       "      <td>0.125754</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>0.079197</td>\n",
       "      <td>0.093215</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.063431</td>\n",
       "      <td>-0.023597</td>\n",
       "      <td>0.068057</td>\n",
       "      <td>-0.252915</td>\n",
       "      <td>-0.061094</td>\n",
       "      <td>-0.027316</td>\n",
       "      <td>0.135747</td>\n",
       "      <td>-0.168147</td>\n",
       "      <td>0.091236</td>\n",
       "      <td>-0.078473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014893</td>\n",
       "      <td>0.071721</td>\n",
       "      <td>0.018918</td>\n",
       "      <td>0.100032</td>\n",
       "      <td>-0.083042</td>\n",
       "      <td>0.088615</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034962</td>\n",
       "      <td>0.035816</td>\n",
       "      <td>-0.029753</td>\n",
       "      <td>-0.094607</td>\n",
       "      <td>-0.017576</td>\n",
       "      <td>-0.053074</td>\n",
       "      <td>0.040121</td>\n",
       "      <td>-0.007932</td>\n",
       "      <td>0.097872</td>\n",
       "      <td>-0.024042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>0.015215</td>\n",
       "      <td>0.083808</td>\n",
       "      <td>0.031312</td>\n",
       "      <td>-0.056277</td>\n",
       "      <td>0.064702</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033772</td>\n",
       "      <td>0.085612</td>\n",
       "      <td>0.067488</td>\n",
       "      <td>-0.073953</td>\n",
       "      <td>-0.180646</td>\n",
       "      <td>-0.024512</td>\n",
       "      <td>0.242879</td>\n",
       "      <td>-0.023374</td>\n",
       "      <td>-0.059999</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078246</td>\n",
       "      <td>-0.032903</td>\n",
       "      <td>0.082949</td>\n",
       "      <td>-0.020659</td>\n",
       "      <td>0.082274</td>\n",
       "      <td>-0.050164</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.134305</td>\n",
       "      <td>0.062096</td>\n",
       "      <td>0.106920</td>\n",
       "      <td>-0.089327</td>\n",
       "      <td>0.117093</td>\n",
       "      <td>-0.077107</td>\n",
       "      <td>0.152579</td>\n",
       "      <td>0.047529</td>\n",
       "      <td>-0.015998</td>\n",
       "      <td>-0.110657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094629</td>\n",
       "      <td>0.069718</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.048124</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>-0.016980</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.186257  -0.058807   0.024632  -0.163933  -0.146699   0.035889   \n",
       "1   0.063431  -0.023597   0.068057  -0.252915  -0.061094  -0.027316   \n",
       "2   0.034962   0.035816  -0.029753  -0.094607  -0.017576  -0.053074   \n",
       "3   0.033772   0.085612   0.067488  -0.073953  -0.180646  -0.024512   \n",
       "4   0.134305   0.062096   0.106920  -0.089327   0.117093  -0.077107   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0   0.111708  -0.162861   0.028249   -0.098063  ...     0.055629    -0.010358   \n",
       "1   0.135747  -0.168147   0.091236   -0.078473  ...    -0.014893     0.071721   \n",
       "2   0.040121  -0.007932   0.097872   -0.024042  ...     0.012415     0.015215   \n",
       "3   0.242879  -0.023374  -0.059999    0.002006  ...    -0.078246    -0.032903   \n",
       "4   0.152579   0.047529  -0.015998   -0.110657  ...    -0.094629     0.069718   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0     0.125754     0.011648     0.079197     0.093215       45      NaN   \n",
       "1     0.018918     0.100032    -0.083042     0.088615       45      NaN   \n",
       "2     0.083808     0.031312    -0.056277     0.064702       45      NaN   \n",
       "3     0.082949    -0.020659     0.082274    -0.050164       45      NaN   \n",
       "4     0.014379     0.048124     0.007586    -0.016980       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_train_df = pd.read_csv(layer_7_train_csv_file_path)\n",
    "l7_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l7_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l7_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l7_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "l7_train_X = l7_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)\n",
    "train_speaker_IDs = l7_train_df['label_1']\n",
    "train_speaker_ages = l7_train_df['label_2']\n",
    "train_speaker_genders = l7_train_df['label_3']\n",
    "train_speaker_accents = l7_train_df['label_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "l7_valid_df = pd.read_csv(layer_7_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l7_valid_df.shape)\n",
    "print(\"null values row count: \", l7_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l7_valid_df.isnull().sum().sum() / l7_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l7_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l7_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l7_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l7_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "l7_valid_X = l7_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)\n",
    "valid_speaker_IDs = l7_valid_df['label_1']\n",
    "valid_speaker_ages = l7_valid_df['label_2']\n",
    "valid_speaker_genders = l7_valid_df['label_3']\n",
    "valid_speaker_accents = l7_valid_df['label_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now develop pipelines to predict the speaker ID, speaker age, speaker gender, and speaker accent. \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "##visualize Pipeline\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       1.00      1.00      1.00        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      1.00      1.00        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      1.00      0.97        14\n",
      "          18       1.00      1.00      1.00        19\n",
      "          19       0.93      0.93      0.93        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       1.00      0.93      0.96        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       1.00      1.00      1.00        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      1.00      1.00        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.93      0.97        15\n",
      "          30       1.00      1.00      1.00         8\n",
      "          31       1.00      1.00      1.00        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       1.00      1.00      1.00         6\n",
      "          34       1.00      1.00      1.00        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      1.00      1.00        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       1.00      1.00      1.00        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       1.00      1.00      1.00        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       0.91      1.00      0.95        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       0.99      0.99      0.99       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PCA__n_components = 0.97\n",
    "clf__C = 10.0\n",
    "clf__kernel = 'rbf'\n",
    "\n",
    "layer_7_speaker_ID_pipe = Pipeline([\n",
    "    ('pca', PCA(n_components=PCA__n_components)),\n",
    "    ('clf', SVC(C=clf__C, kernel=clf__kernel))\n",
    "    ])\n",
    "\n",
    "layer_7_speaker_ID_pipe.fit(l7_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_7_speaker_ID_pipe.predict(l7_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_7_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.92      1.00      0.96        36\n",
      "        23.0       0.96      0.97      0.97        71\n",
      "        24.0       0.96      0.96      0.96        46\n",
      "        25.0       0.99      0.90      0.94        79\n",
      "        26.0       0.93      0.99      0.96       115\n",
      "        27.0       1.00      0.95      0.97        81\n",
      "        28.0       0.97      0.97      0.97        60\n",
      "        29.0       1.00      1.00      1.00        45\n",
      "        30.0       0.96      1.00      0.98        48\n",
      "        31.0       0.95      0.97      0.96        65\n",
      "        32.0       1.00      1.00      1.00        11\n",
      "        33.0       1.00      1.00      1.00        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.93      0.96        14\n",
      "        61.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.98      0.97      0.97       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca__n_components = 0.97\n",
    "clf__C = 10.0\n",
    "clf__kernel = 'rbf'\n",
    "\n",
    "# Pipleline with best params\n",
    "layer_7_speaker_age_pipe = Pipeline([\n",
    "    # ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=pca__n_components)),\n",
    "    ('clf', SVC(C=clf__C, kernel=clf__kernel))\n",
    "    ])\n",
    "\n",
    "layer_7_speaker_age_pipe.fit(l7_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_7_speaker_age_pipe.predict(l7_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_7_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With SVC classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      1.00      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_7_speaker_gender_pipe = Pipeline([ \n",
    "    ('clf', SVC(class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_7_speaker_gender_pipe.fit(l7_train_X, train_speaker_genders)\n",
    "\n",
    "# now let's check the results\n",
    "print(\"With SVC classifier:\")\n",
    "report = classification_report(valid_speaker_genders, layer_7_speaker_gender_pipe.predict(l7_valid_X)) \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_gender_preds_confidences['layer_7_speaker_gender_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        21\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       1.00      1.00      1.00        27\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       1.00      0.93      0.97        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.99      1.00      0.99       532\n",
      "           7       1.00      0.97      0.98        32\n",
      "           8       1.00      0.95      0.97        19\n",
      "           9       1.00      1.00      1.00        17\n",
      "          10       1.00      1.00      1.00        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       1.00      1.00      1.00        26\n",
      "          13       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       1.00      0.97      0.98       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca__n_components = 0.97\n",
    "clf__C = 10.0\n",
    "clf__kernel = 'rbf'\n",
    "\n",
    "# Pipleline with best params\n",
    "layer_7_speaker_accent_pipe = Pipeline([\n",
    "    # ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=pca__n_components)),\n",
    "    ('clf', SVC(C=clf__C, kernel=clf__kernel))\n",
    "    ])\n",
    "\n",
    "layer_7_speaker_accent_pipe.fit(l7_train_X, train_speaker_accents)\n",
    "report = classification_report(valid_speaker_accents, layer_7_speaker_accent_pipe.predict(l7_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_accent_preds_confidences['layer_7_speaker_accent_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.071810</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>-0.022749</td>\n",
       "      <td>0.086143</td>\n",
       "      <td>0.026361</td>\n",
       "      <td>-0.028817</td>\n",
       "      <td>0.199237</td>\n",
       "      <td>-0.287368</td>\n",
       "      <td>-0.059560</td>\n",
       "      <td>-0.043694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004646</td>\n",
       "      <td>0.123011</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>-0.042152</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>0.072623</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.030930</td>\n",
       "      <td>0.024088</td>\n",
       "      <td>0.057811</td>\n",
       "      <td>-0.230877</td>\n",
       "      <td>-0.146281</td>\n",
       "      <td>0.102807</td>\n",
       "      <td>0.128767</td>\n",
       "      <td>-0.146269</td>\n",
       "      <td>0.053893</td>\n",
       "      <td>0.055378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077742</td>\n",
       "      <td>0.081691</td>\n",
       "      <td>-0.004778</td>\n",
       "      <td>0.171727</td>\n",
       "      <td>-0.026027</td>\n",
       "      <td>0.171089</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.044019</td>\n",
       "      <td>-0.004626</td>\n",
       "      <td>-0.029383</td>\n",
       "      <td>-0.165376</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>-0.028142</td>\n",
       "      <td>-0.009649</td>\n",
       "      <td>-0.082088</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>-0.046272</td>\n",
       "      <td>0.027831</td>\n",
       "      <td>0.028096</td>\n",
       "      <td>0.030994</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.086241</td>\n",
       "      <td>0.129585</td>\n",
       "      <td>-0.013893</td>\n",
       "      <td>0.089885</td>\n",
       "      <td>-0.100300</td>\n",
       "      <td>-0.035184</td>\n",
       "      <td>0.240980</td>\n",
       "      <td>-0.128362</td>\n",
       "      <td>-0.072328</td>\n",
       "      <td>-0.019385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006934</td>\n",
       "      <td>-0.049213</td>\n",
       "      <td>0.078852</td>\n",
       "      <td>0.088323</td>\n",
       "      <td>0.168815</td>\n",
       "      <td>-0.049188</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126416</td>\n",
       "      <td>0.088338</td>\n",
       "      <td>0.088307</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.174417</td>\n",
       "      <td>-0.030560</td>\n",
       "      <td>0.181163</td>\n",
       "      <td>-0.009382</td>\n",
       "      <td>0.085396</td>\n",
       "      <td>0.015823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028883</td>\n",
       "      <td>0.110844</td>\n",
       "      <td>-0.041875</td>\n",
       "      <td>0.025686</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>-0.104945</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.071810   0.068413  -0.022749   0.086143   0.026361  -0.028817   \n",
       "1   0.030930   0.024088   0.057811  -0.230877  -0.146281   0.102807   \n",
       "2  -0.044019  -0.004626  -0.029383  -0.165376  -0.026611  -0.028142   \n",
       "3  -0.086241   0.129585  -0.013893   0.089885  -0.100300  -0.035184   \n",
       "4   0.126416   0.088338   0.088307   0.020371   0.174417  -0.030560   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0   0.199237  -0.287368  -0.059560   -0.043694  ...     0.004646     0.123011   \n",
       "1   0.128767  -0.146269   0.053893    0.055378  ...     0.077742     0.081691   \n",
       "2  -0.009649  -0.082088   0.018933    0.006830  ...     0.076249    -0.046272   \n",
       "3   0.240980  -0.128362  -0.072328   -0.019385  ...     0.006934    -0.049213   \n",
       "4   0.181163  -0.009382   0.085396    0.015823  ...    -0.028883     0.110844   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0     0.043040    -0.042152     0.026225     0.072623       45      NaN   \n",
       "1    -0.004778     0.171727    -0.026027     0.171089       45      NaN   \n",
       "2     0.027831     0.028096     0.030994     0.009709       45      NaN   \n",
       "3     0.078852     0.088323     0.168815    -0.049188       45      NaN   \n",
       "4    -0.041875     0.025686     0.003534    -0.104945       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_train_df = pd.read_csv(layer_8_train_csv_file_path)\n",
    "l8_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l8_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l8_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l8_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "l8_train_X = l8_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_valid_df = pd.read_csv(layer_8_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l8_valid_df.shape)\n",
    "print(\"null values row count: \", l8_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l8_valid_df.isnull().sum().sum() / l8_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l8_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l8_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l8_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l8_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_valid_X = l8_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.92      0.92        13\n",
      "           2       1.00      0.78      0.88         9\n",
      "           3       0.86      1.00      0.92        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       0.84      0.89      0.86        18\n",
      "           6       0.90      1.00      0.95         9\n",
      "           7       0.89      0.94      0.91        17\n",
      "           8       0.91      0.71      0.80        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      0.89      0.94        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.82      0.90        11\n",
      "          14       0.93      0.87      0.90        15\n",
      "          15       1.00      0.88      0.94        17\n",
      "          16       1.00      0.93      0.96        14\n",
      "          17       1.00      0.93      0.96        14\n",
      "          18       0.95      0.95      0.95        19\n",
      "          19       0.74      0.93      0.82        15\n",
      "          20       1.00      0.82      0.90        11\n",
      "          21       0.87      0.93      0.90        14\n",
      "          22       0.91      0.91      0.91        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.65      0.92      0.76        12\n",
      "          25       0.91      0.91      0.91        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.94      0.94      0.94        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.87      0.93        15\n",
      "          30       0.88      0.88      0.88         8\n",
      "          31       0.92      0.92      0.92        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.77      0.87        13\n",
      "          35       0.89      1.00      0.94         8\n",
      "          36       0.85      0.94      0.89        18\n",
      "          37       1.00      0.81      0.90        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       0.92      0.92      0.92        12\n",
      "          41       0.83      1.00      0.91        10\n",
      "          42       0.92      1.00      0.96        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       0.90      0.95      0.92        19\n",
      "          45       0.93      0.93      0.93        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       0.70      0.88      0.78         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       1.00      0.85      0.92        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       0.80      1.00      0.89         8\n",
      "          52       0.85      1.00      0.92        11\n",
      "          53       0.93      0.93      0.93        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       0.88      0.88      0.88         8\n",
      "          56       1.00      0.90      0.95        10\n",
      "          57       1.00      0.94      0.97        18\n",
      "          58       1.00      0.95      0.97        20\n",
      "          59       0.90      0.90      0.90        10\n",
      "          60       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.93       750\n",
      "   macro avg       0.93      0.93      0.93       750\n",
      "weighted avg       0.93      0.93      0.93       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_8_speaker_ID_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.85)),\n",
    "    ('clf', SVC(C=10, gamma=0.001, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_8_speaker_ID_pipe.fit(l8_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_8_speaker_ID_pipe.predict(l8_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_8_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.97      0.83      0.90        36\n",
      "        23.0       0.96      0.90      0.93        71\n",
      "        24.0       0.85      0.89      0.87        46\n",
      "        25.0       0.95      0.90      0.92        79\n",
      "        26.0       0.83      0.95      0.88       115\n",
      "        27.0       0.94      0.89      0.91        81\n",
      "        28.0       0.86      0.92      0.89        60\n",
      "        29.0       0.93      0.96      0.95        45\n",
      "        30.0       0.98      0.92      0.95        48\n",
      "        31.0       0.89      0.98      0.93        65\n",
      "        32.0       1.00      0.82      0.90        11\n",
      "        33.0       1.00      0.93      0.97        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.73      0.84        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.93      0.96        14\n",
      "        61.0       1.00      0.89      0.94        19\n",
      "\n",
      "    accuracy                           0.91       750\n",
      "   macro avg       0.95      0.90      0.92       750\n",
      "weighted avg       0.92      0.91      0.92       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_8_age_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.85)),\n",
    "    ('clf', SVC(C=100, class_weight='balanced', gamma=0.01, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_8_age_pipe.fit(l8_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_8_age_pipe.predict(l8_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_8_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       0.99      1.00      0.99       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_8_gender_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.85)),\n",
    "    ('clf', SVC(C=10, class_weight='balanced', gamma=0.001, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_8_gender_pipe.fit(l8_train_X, train_speaker_genders)\n",
    "report = classification_report(valid_speaker_genders, layer_8_gender_pipe.predict(l8_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_gender_preds_confidences['layer_8_speaker_gender_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0, 'layer_8_speaker_gender_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        21\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       0.60      0.93      0.72        27\n",
      "           3       0.89      1.00      0.94         8\n",
      "           4       0.76      0.87      0.81        15\n",
      "           5       0.90      0.82      0.86        11\n",
      "           6       0.96      0.94      0.95       532\n",
      "           7       0.83      0.78      0.81        32\n",
      "           8       1.00      0.84      0.91        19\n",
      "           9       1.00      0.88      0.94        17\n",
      "          10       1.00      0.90      0.95        10\n",
      "          11       0.91      0.91      0.91        11\n",
      "          12       0.76      0.85      0.80        26\n",
      "          13       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.92       750\n",
      "   macro avg       0.89      0.89      0.88       750\n",
      "weighted avg       0.93      0.92      0.92       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_8_accent_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.85)),\n",
    "    ('clf', SVC(C=100, class_weight='balanced', gamma=0.001, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_8_accent_pipe.fit(l8_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_8_accent_pipe.predict(l8_valid_X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_accent_preds_confidences['layer_8_speaker_accent_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99, 'layer_8_speaker_accent_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019301</td>\n",
       "      <td>0.059756</td>\n",
       "      <td>0.081375</td>\n",
       "      <td>0.057481</td>\n",
       "      <td>-0.068440</td>\n",
       "      <td>-0.165913</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>-0.091138</td>\n",
       "      <td>0.021688</td>\n",
       "      <td>0.057158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035576</td>\n",
       "      <td>0.127319</td>\n",
       "      <td>0.098128</td>\n",
       "      <td>-0.058787</td>\n",
       "      <td>0.100971</td>\n",
       "      <td>-0.047754</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049741</td>\n",
       "      <td>0.090030</td>\n",
       "      <td>0.035118</td>\n",
       "      <td>-0.013676</td>\n",
       "      <td>-0.194317</td>\n",
       "      <td>-0.101763</td>\n",
       "      <td>0.085875</td>\n",
       "      <td>-0.081317</td>\n",
       "      <td>0.112418</td>\n",
       "      <td>0.120523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020538</td>\n",
       "      <td>0.058968</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.111324</td>\n",
       "      <td>0.036727</td>\n",
       "      <td>0.031927</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019212</td>\n",
       "      <td>0.087779</td>\n",
       "      <td>0.093907</td>\n",
       "      <td>-0.033738</td>\n",
       "      <td>-0.141409</td>\n",
       "      <td>-0.062881</td>\n",
       "      <td>-0.071402</td>\n",
       "      <td>-0.006599</td>\n",
       "      <td>0.020372</td>\n",
       "      <td>-0.027777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119645</td>\n",
       "      <td>-0.040861</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>-0.061003</td>\n",
       "      <td>-0.042450</td>\n",
       "      <td>0.063340</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070283</td>\n",
       "      <td>0.049040</td>\n",
       "      <td>0.042126</td>\n",
       "      <td>0.122637</td>\n",
       "      <td>-0.056964</td>\n",
       "      <td>-0.113700</td>\n",
       "      <td>0.108454</td>\n",
       "      <td>0.051336</td>\n",
       "      <td>0.086610</td>\n",
       "      <td>0.141578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124494</td>\n",
       "      <td>-0.169225</td>\n",
       "      <td>-0.046391</td>\n",
       "      <td>0.148787</td>\n",
       "      <td>0.014616</td>\n",
       "      <td>-0.140644</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028864</td>\n",
       "      <td>0.165634</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>0.036117</td>\n",
       "      <td>-0.028871</td>\n",
       "      <td>-0.147748</td>\n",
       "      <td>0.053180</td>\n",
       "      <td>0.025071</td>\n",
       "      <td>-0.004200</td>\n",
       "      <td>-0.022183</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124862</td>\n",
       "      <td>0.044907</td>\n",
       "      <td>0.084005</td>\n",
       "      <td>-0.038450</td>\n",
       "      <td>0.084371</td>\n",
       "      <td>-0.072146</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.019301   0.059756   0.081375   0.057481  -0.068440  -0.165913   \n",
       "1   0.049741   0.090030   0.035118  -0.013676  -0.194317  -0.101763   \n",
       "2   0.019212   0.087779   0.093907  -0.033738  -0.141409  -0.062881   \n",
       "3   0.070283   0.049040   0.042126   0.122637  -0.056964  -0.113700   \n",
       "4   0.028864   0.165634   0.016302   0.036117  -0.028871  -0.147748   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0   0.035643  -0.091138   0.021688    0.057158  ...    -0.035576     0.127319   \n",
       "1   0.085875  -0.081317   0.112418    0.120523  ...     0.020538     0.058968   \n",
       "2  -0.071402  -0.006599   0.020372   -0.027777  ...     0.119645    -0.040861   \n",
       "3   0.108454   0.051336   0.086610    0.141578  ...    -0.124494    -0.169225   \n",
       "4   0.053180   0.025071  -0.004200   -0.022183  ...    -0.124862     0.044907   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0     0.098128    -0.058787     0.100971    -0.047754       45      NaN   \n",
       "1     0.029803     0.111324     0.036727     0.031927       45      NaN   \n",
       "2     0.000548    -0.061003    -0.042450     0.063340       45      NaN   \n",
       "3    -0.046391     0.148787     0.014616    -0.140644       45      NaN   \n",
       "4     0.084005    -0.038450     0.084371    -0.072146       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_train_df = pd.read_csv(layer_9_train_csv_file_path)\n",
    "l9_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l9_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean: 28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l9_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean:\", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l9_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_train_X = l9_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_valid_df = pd.read_csv(layer_9_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l9_valid_df.shape)\n",
    "print(\"null values row count: \", l9_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l9_valid_df.isnull().sum().sum() / l9_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l9_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l9_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l9_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l9_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "l9_valid_X = l9_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       0.92      1.00      0.96        12\n",
      "           4       0.88      0.94      0.91        16\n",
      "           5       1.00      0.89      0.94        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       0.94      0.94      0.94        17\n",
      "           8       1.00      0.79      0.88        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       0.89      1.00      0.94         8\n",
      "          11       0.95      0.95      0.95        19\n",
      "          12       0.88      1.00      0.93         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.87      0.93        15\n",
      "          15       1.00      0.88      0.94        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       1.00      1.00      1.00        14\n",
      "          18       0.90      0.95      0.92        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      0.91      0.95        11\n",
      "          21       0.93      0.93      0.93        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.92      0.92      0.92        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.84      1.00      0.91        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      0.88      0.93         8\n",
      "          31       0.85      0.92      0.88        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.85      0.92        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       0.95      1.00      0.97        18\n",
      "          37       1.00      0.94      0.97        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       0.91      1.00      0.95        10\n",
      "          42       0.92      1.00      0.96        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       0.93      1.00      0.97        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       0.88      0.88      0.88         8\n",
      "          48       0.84      0.94      0.89        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       0.89      1.00      0.94         8\n",
      "          52       0.91      0.91      0.91        11\n",
      "          53       1.00      0.93      0.97        15\n",
      "          54       0.89      0.89      0.89         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       0.90      0.90      0.90        10\n",
      "          57       0.95      1.00      0.97        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.96       750\n",
      "   macro avg       0.96      0.96      0.96       750\n",
      "weighted avg       0.96      0.96      0.96       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_9_speaker_ID_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.94)),\n",
    "    ('clf', SVC(C=28, kernel='rbf'))\n",
    "    ])\n",
    "\n",
    "layer_9_speaker_ID_pipe.fit(l9_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_9_speaker_ID_pipe.predict(l9_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_9_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.92      1.00      0.96        36\n",
      "        23.0       0.88      0.92      0.90        71\n",
      "        24.0       0.90      0.98      0.94        46\n",
      "        25.0       0.97      0.91      0.94        79\n",
      "        26.0       0.92      0.96      0.94       115\n",
      "        27.0       0.95      0.93      0.94        81\n",
      "        28.0       0.93      0.95      0.94        60\n",
      "        29.0       0.98      0.98      0.98        45\n",
      "        30.0       0.94      0.96      0.95        48\n",
      "        31.0       0.98      0.97      0.98        65\n",
      "        32.0       1.00      1.00      1.00        11\n",
      "        33.0       1.00      0.93      0.97        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.71      0.83        14\n",
      "        61.0       1.00      0.95      0.97        19\n",
      "\n",
      "    accuracy                           0.94       750\n",
      "   macro avg       0.96      0.94      0.95       750\n",
      "weighted avg       0.95      0.94      0.94       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_9_age_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.96)),\n",
    "    ('clf', SVC(C=55, kernel='rbf', class_weight='balanced', probability=True))\n",
    "    ])\n",
    "\n",
    "layer_9_age_pipe.fit(l9_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_9_age_pipe.predict(l9_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_9_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       0.99      0.99      0.99       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_9_gender_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.90)),\n",
    "    ('clf', SVC(C=9, kernel='rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_9_gender_pipe.fit(l9_train_X, train_speaker_genders)\n",
    "report = classification_report(valid_speaker_genders, layer_9_gender_pipe.predict(l9_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_gender_preds_confidences['layer_9_speaker_gender_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0, 'layer_8_speaker_gender_pipe': 1.0, 'layer_9_speaker_gender_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.93      1.00      0.96        27\n",
      "           3       1.00      0.88      0.93         8\n",
      "           4       1.00      0.87      0.93        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.97      1.00      0.99       532\n",
      "           7       1.00      0.91      0.95        32\n",
      "           8       0.95      0.95      0.95        19\n",
      "           9       1.00      0.82      0.90        17\n",
      "          10       1.00      1.00      1.00        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       1.00      0.88      0.94        26\n",
      "          13       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.98       750\n",
      "   macro avg       0.99      0.94      0.96       750\n",
      "weighted avg       0.98      0.98      0.98       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_9_accent_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('clf', SVC(C=18, kernel='rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_9_accent_pipe.fit(l9_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_9_accent_pipe.predict(l9_valid_X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_accent_preds_confidences['layer_9_speaker_accent_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99, 'layer_8_speaker_accent_pipe': 1.0, 'layer_9_speaker_accent_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.027083</td>\n",
       "      <td>0.072947</td>\n",
       "      <td>-0.093659</td>\n",
       "      <td>0.053418</td>\n",
       "      <td>-0.085516</td>\n",
       "      <td>-0.102610</td>\n",
       "      <td>-0.021217</td>\n",
       "      <td>0.016162</td>\n",
       "      <td>-0.184269</td>\n",
       "      <td>0.110335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183643</td>\n",
       "      <td>0.091299</td>\n",
       "      <td>-0.037097</td>\n",
       "      <td>0.042607</td>\n",
       "      <td>-0.034361</td>\n",
       "      <td>-0.013748</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.070195</td>\n",
       "      <td>0.228641</td>\n",
       "      <td>-0.132860</td>\n",
       "      <td>-0.077761</td>\n",
       "      <td>-0.054993</td>\n",
       "      <td>-0.210365</td>\n",
       "      <td>0.127747</td>\n",
       "      <td>-0.132385</td>\n",
       "      <td>-0.161366</td>\n",
       "      <td>0.172764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123668</td>\n",
       "      <td>0.029626</td>\n",
       "      <td>-0.027345</td>\n",
       "      <td>0.055223</td>\n",
       "      <td>-0.179725</td>\n",
       "      <td>0.136841</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.164312</td>\n",
       "      <td>0.052808</td>\n",
       "      <td>-0.058510</td>\n",
       "      <td>0.104724</td>\n",
       "      <td>-0.025886</td>\n",
       "      <td>-0.101427</td>\n",
       "      <td>-0.047177</td>\n",
       "      <td>0.091298</td>\n",
       "      <td>-0.094569</td>\n",
       "      <td>0.088062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075410</td>\n",
       "      <td>0.070125</td>\n",
       "      <td>0.043022</td>\n",
       "      <td>0.012972</td>\n",
       "      <td>-0.028920</td>\n",
       "      <td>0.096725</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.113737</td>\n",
       "      <td>0.061113</td>\n",
       "      <td>-0.099329</td>\n",
       "      <td>-0.111600</td>\n",
       "      <td>-0.245942</td>\n",
       "      <td>0.086520</td>\n",
       "      <td>0.071996</td>\n",
       "      <td>0.028319</td>\n",
       "      <td>0.207910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062511</td>\n",
       "      <td>-0.226912</td>\n",
       "      <td>-0.046011</td>\n",
       "      <td>0.011282</td>\n",
       "      <td>-0.095167</td>\n",
       "      <td>0.039979</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.031364</td>\n",
       "      <td>0.142409</td>\n",
       "      <td>-0.160743</td>\n",
       "      <td>-0.076594</td>\n",
       "      <td>-0.062412</td>\n",
       "      <td>-0.264732</td>\n",
       "      <td>0.079197</td>\n",
       "      <td>0.026060</td>\n",
       "      <td>-0.217023</td>\n",
       "      <td>0.084656</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193882</td>\n",
       "      <td>0.107297</td>\n",
       "      <td>-0.042355</td>\n",
       "      <td>0.046763</td>\n",
       "      <td>-0.192469</td>\n",
       "      <td>0.006463</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0  -0.027083   0.072947  -0.093659   0.053418  -0.085516  -0.102610   \n",
       "1   0.070195   0.228641  -0.132860  -0.077761  -0.054993  -0.210365   \n",
       "2   0.164312   0.052808  -0.058510   0.104724  -0.025886  -0.101427   \n",
       "3   0.029730   0.113737   0.061113  -0.099329  -0.111600  -0.245942   \n",
       "4   0.031364   0.142409  -0.160743  -0.076594  -0.062412  -0.264732   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0  -0.021217   0.016162  -0.184269    0.110335  ...    -0.183643     0.091299   \n",
       "1   0.127747  -0.132385  -0.161366    0.172764  ...    -0.123668     0.029626   \n",
       "2  -0.047177   0.091298  -0.094569    0.088062  ...     0.075410     0.070125   \n",
       "3   0.086520   0.071996   0.028319    0.207910  ...    -0.062511    -0.226912   \n",
       "4   0.079197   0.026060  -0.217023    0.084656  ...    -0.193882     0.107297   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0    -0.037097     0.042607    -0.034361    -0.013748       45      NaN   \n",
       "1    -0.027345     0.055223    -0.179725     0.136841       45      NaN   \n",
       "2     0.043022     0.012972    -0.028920     0.096725       45      NaN   \n",
       "3    -0.046011     0.011282    -0.095167     0.039979       45      NaN   \n",
       "4    -0.042355     0.046763    -0.192469     0.006463       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_train_df = pd.read_csv(layer_10_train_csv_file_path)\n",
    "l10_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l10_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean: 28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l10_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean:\", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l10_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_train_X = l10_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_valid_df = pd.read_csv(layer_10_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l10_valid_df.shape)\n",
    "print(\"null values row count: \", l10_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l10_valid_df.isnull().sum().sum() / l10_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l10_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l10_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l10_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l10_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_valid_X = l10_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.92      0.89        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       1.00      1.00      1.00        16\n",
      "           5       0.94      0.89      0.91        18\n",
      "           6       1.00      0.89      0.94         9\n",
      "           7       0.94      0.94      0.94        17\n",
      "           8       1.00      0.79      0.88        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      0.95      0.97        19\n",
      "          12       0.88      1.00      0.93         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      1.00      1.00        15\n",
      "          15       1.00      0.88      0.94        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      0.93      0.93        14\n",
      "          18       1.00      0.95      0.97        19\n",
      "          19       0.78      0.93      0.85        15\n",
      "          20       1.00      0.82      0.90        11\n",
      "          21       1.00      1.00      1.00        14\n",
      "          22       0.92      1.00      0.96        11\n",
      "          23       1.00      1.00      1.00        10\n",
      "          24       0.92      1.00      0.96        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.94      1.00      0.97        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      0.88      0.93         8\n",
      "          31       0.85      0.92      0.88        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.85      0.92        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       0.94      1.00      0.97        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       0.93      1.00      0.96        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       0.92      1.00      0.96        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      1.00      1.00        19\n",
      "          45       0.93      1.00      0.97        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       0.89      1.00      0.94         8\n",
      "          48       0.89      0.94      0.91        17\n",
      "          49       0.87      1.00      0.93        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      1.00      1.00        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      0.95      0.97        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.97      0.97      0.97       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_10_speaker_ID_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('clf', SVC(C=10, kernel='rbf'))\n",
    "    ])\n",
    "\n",
    "layer_10_speaker_ID_pipe.fit(l10_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_10_speaker_ID_pipe.predict(l10_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_10_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96, 'layer_10_speaker_ID_pipe': 0.97}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.95      0.97      0.96        36\n",
      "        23.0       0.83      0.94      0.88        71\n",
      "        24.0       0.92      0.98      0.95        46\n",
      "        25.0       0.95      0.87      0.91        79\n",
      "        26.0       0.97      0.97      0.97       115\n",
      "        27.0       0.93      0.94      0.93        81\n",
      "        28.0       0.98      0.93      0.96        60\n",
      "        29.0       0.98      0.96      0.97        45\n",
      "        30.0       0.98      0.96      0.97        48\n",
      "        31.0       0.93      0.95      0.94        65\n",
      "        32.0       1.00      0.91      0.95        11\n",
      "        33.0       0.97      0.97      0.97        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.71      0.83        14\n",
      "        61.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.94       750\n",
      "   macro avg       0.96      0.93      0.95       750\n",
      "weighted avg       0.95      0.94      0.94       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_10_age_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.92)),\n",
    "    ('clf', SVC(C=27, kernel='rbf', class_weight='balanced', probability=True))\n",
    "    ])\n",
    "\n",
    "layer_10_age_pipe.fit(l10_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_10_age_pipe.predict(l10_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_10_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94, 'layer_10_speaker_age_pipe': 0.94}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      1.00      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_10_gender_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.83)),\n",
    "    ('clf', SVC(C=23, kernel='rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_10_gender_pipe.fit(l10_train_X, train_speaker_genders)\n",
    "report = classification_report(valid_speaker_genders, layer_10_gender_pipe.predict(l10_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_gender_preds_confidences['layer_10_speaker_gender_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0, 'layer_8_speaker_gender_pipe': 1.0, 'layer_9_speaker_gender_pipe': 1.0, 'layer_10_speaker_gender_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       0.96      0.96      0.96        27\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       1.00      0.93      0.97        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.98      1.00      0.99       532\n",
      "           7       1.00      0.94      0.97        32\n",
      "           8       1.00      0.95      0.97        19\n",
      "           9       1.00      0.82      0.90        17\n",
      "          10       1.00      0.90      0.95        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       1.00      0.92      0.96        26\n",
      "          13       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.98       750\n",
      "   macro avg       1.00      0.95      0.97       750\n",
      "weighted avg       0.98      0.98      0.98       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_10_accent_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('clf', SVC(C=10, kernel='rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_10_accent_pipe.fit(l10_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_10_accent_pipe.predict(l10_valid_X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_accent_preds_confidences['layer_10_speaker_accent_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99, 'layer_8_speaker_accent_pipe': 1.0, 'layer_9_speaker_accent_pipe': 1.0, 'layer_10_speaker_accent_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031138</td>\n",
       "      <td>0.079892</td>\n",
       "      <td>0.157382</td>\n",
       "      <td>-0.014636</td>\n",
       "      <td>-0.051778</td>\n",
       "      <td>-0.021332</td>\n",
       "      <td>-0.073593</td>\n",
       "      <td>-0.005386</td>\n",
       "      <td>-0.212557</td>\n",
       "      <td>0.099683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085248</td>\n",
       "      <td>-0.096007</td>\n",
       "      <td>-0.000766</td>\n",
       "      <td>0.021399</td>\n",
       "      <td>-0.041432</td>\n",
       "      <td>0.094806</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113040</td>\n",
       "      <td>0.175731</td>\n",
       "      <td>0.217741</td>\n",
       "      <td>-0.196254</td>\n",
       "      <td>-0.010129</td>\n",
       "      <td>-0.030586</td>\n",
       "      <td>0.067114</td>\n",
       "      <td>-0.072412</td>\n",
       "      <td>-0.239192</td>\n",
       "      <td>0.104741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090283</td>\n",
       "      <td>-0.053885</td>\n",
       "      <td>-0.010967</td>\n",
       "      <td>0.062209</td>\n",
       "      <td>-0.122958</td>\n",
       "      <td>0.192949</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.048570</td>\n",
       "      <td>0.091281</td>\n",
       "      <td>0.160776</td>\n",
       "      <td>-0.150937</td>\n",
       "      <td>0.020115</td>\n",
       "      <td>0.044117</td>\n",
       "      <td>-0.050092</td>\n",
       "      <td>-0.045661</td>\n",
       "      <td>-0.155332</td>\n",
       "      <td>0.117206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021524</td>\n",
       "      <td>-0.008411</td>\n",
       "      <td>-0.006248</td>\n",
       "      <td>0.031468</td>\n",
       "      <td>-0.056915</td>\n",
       "      <td>0.154731</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.039212</td>\n",
       "      <td>0.118388</td>\n",
       "      <td>0.173831</td>\n",
       "      <td>-0.096659</td>\n",
       "      <td>-0.008702</td>\n",
       "      <td>0.061298</td>\n",
       "      <td>0.008974</td>\n",
       "      <td>-0.003277</td>\n",
       "      <td>-0.065046</td>\n",
       "      <td>0.095480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071936</td>\n",
       "      <td>-0.023120</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>-0.121892</td>\n",
       "      <td>0.072796</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.056019</td>\n",
       "      <td>0.170639</td>\n",
       "      <td>0.157917</td>\n",
       "      <td>-0.228605</td>\n",
       "      <td>-0.065965</td>\n",
       "      <td>-0.088732</td>\n",
       "      <td>-0.082243</td>\n",
       "      <td>-0.080568</td>\n",
       "      <td>-0.341500</td>\n",
       "      <td>0.142430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155621</td>\n",
       "      <td>-0.079447</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>-0.151966</td>\n",
       "      <td>0.169634</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.031138   0.079892   0.157382  -0.014636  -0.051778  -0.021332   \n",
       "1   0.113040   0.175731   0.217741  -0.196254  -0.010129  -0.030586   \n",
       "2   0.048570   0.091281   0.160776  -0.150937   0.020115   0.044117   \n",
       "3   0.039212   0.118388   0.173831  -0.096659  -0.008702   0.061298   \n",
       "4   0.056019   0.170639   0.157917  -0.228605  -0.065965  -0.088732   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0  -0.073593  -0.005386  -0.212557    0.099683  ...    -0.085248    -0.096007   \n",
       "1   0.067114  -0.072412  -0.239192    0.104741  ...    -0.090283    -0.053885   \n",
       "2  -0.050092  -0.045661  -0.155332    0.117206  ...    -0.021524    -0.008411   \n",
       "3   0.008974  -0.003277  -0.065046    0.095480  ...    -0.071936    -0.023120   \n",
       "4  -0.082243  -0.080568  -0.341500    0.142430  ...    -0.155621    -0.079447   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0    -0.000766     0.021399    -0.041432     0.094806       45      NaN   \n",
       "1    -0.010967     0.062209    -0.122958     0.192949       45      NaN   \n",
       "2    -0.006248     0.031468    -0.056915     0.154731       45      NaN   \n",
       "3    -0.007812     0.057600    -0.121892     0.072796       45      NaN   \n",
       "4     0.015316     0.127726    -0.151966     0.169634       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_train_df = pd.read_csv(layer_11_train_csv_file_path)\n",
    "l11_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l11_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l11_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l11_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "l11_train_X = l11_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "l11_valid_df = pd.read_csv(layer_11_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l11_valid_df.shape)\n",
    "print(\"null values row count: \", l11_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l11_valid_df.isnull().sum().sum() / l11_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l11_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l11_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l11_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l11_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "l11_valid_X = l11_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      1.00      0.90        13\n",
      "           2       1.00      0.89      0.94         9\n",
      "           3       0.86      1.00      0.92        12\n",
      "           4       1.00      0.88      0.93        16\n",
      "           5       1.00      0.94      0.97        18\n",
      "           6       1.00      0.89      0.94         9\n",
      "           7       0.89      0.94      0.91        17\n",
      "           8       0.86      0.86      0.86        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      0.95      0.97        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       0.94      0.88      0.91        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.87      0.93      0.90        14\n",
      "          18       1.00      0.95      0.97        19\n",
      "          19       0.78      0.93      0.85        15\n",
      "          20       0.91      0.91      0.91        11\n",
      "          21       0.88      1.00      0.93        14\n",
      "          22       1.00      0.91      0.95        11\n",
      "          23       0.83      1.00      0.91        10\n",
      "          24       0.92      1.00      0.96        12\n",
      "          25       1.00      0.91      0.95        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      0.94      0.97        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.93      0.97        15\n",
      "          30       0.88      0.88      0.88         8\n",
      "          31       0.91      0.83      0.87        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       0.92      0.85      0.88        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      0.81      0.90        16\n",
      "          38       1.00      0.91      0.95        11\n",
      "          39       0.93      1.00      0.96        13\n",
      "          40       0.92      0.92      0.92        12\n",
      "          41       0.91      1.00      0.95        10\n",
      "          42       0.86      1.00      0.92        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      0.93      0.96        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       0.93      1.00      0.96        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       0.80      1.00      0.89         8\n",
      "          52       0.92      1.00      0.96        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      1.00      1.00         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      0.90      0.95        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      0.95      0.97        20\n",
      "          59       0.83      1.00      0.91        10\n",
      "          60       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.95       750\n",
      "   macro avg       0.95      0.95      0.95       750\n",
      "weighted avg       0.95      0.95      0.95       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_11_speaker_ID_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('clf', SVC(C=100, gamma=0.001, kernel = 'rbf'))\n",
    "    ])\n",
    "\n",
    "layer_11_speaker_ID_pipe.fit(l11_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_11_speaker_ID_pipe.predict(l11_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_11_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96, 'layer_10_speaker_ID_pipe': 0.97, 'layer_11_speaker_ID_pipe': 0.95}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Speaker_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.89      0.89      0.89        36\n",
      "        23.0       0.77      0.87      0.82        71\n",
      "        24.0       0.91      0.93      0.92        46\n",
      "        25.0       0.92      0.84      0.87        79\n",
      "        26.0       0.85      0.91      0.88       115\n",
      "        27.0       0.95      0.85      0.90        81\n",
      "        28.0       0.95      0.87      0.90        60\n",
      "        29.0       0.98      0.96      0.97        45\n",
      "        30.0       0.90      0.96      0.93        48\n",
      "        31.0       0.86      0.94      0.90        65\n",
      "        32.0       1.00      0.82      0.90        11\n",
      "        33.0       0.97      0.93      0.95        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       0.92      0.86      0.89        14\n",
      "        61.0       1.00      0.95      0.97        19\n",
      "\n",
      "    accuracy                           0.90       750\n",
      "   macro avg       0.93      0.91      0.92       750\n",
      "weighted avg       0.90      0.90      0.90       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_11_age_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('clf', SVC(C=100, gamma=0.001, kernel = 'rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_11_age_pipe.fit(l11_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_11_age_pipe.predict(l11_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_11_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94, 'layer_10_speaker_age_pipe': 0.94, 'layer_11_speaker_age_pipe': 0.9}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      1.00      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_11_gender_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('clf', SVC(C=100, gamma=0.001, kernel = 'rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_11_gender_pipe.fit(l11_train_X, train_speaker_genders)\n",
    "report = classification_report(valid_speaker_genders, layer_11_gender_pipe.predict(l11_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_gender_preds_confidences['layer_11_speaker_gender_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0, 'layer_8_speaker_gender_pipe': 1.0, 'layer_9_speaker_gender_pipe': 1.0, 'layer_10_speaker_gender_pipe': 1.0, 'layer_11_speaker_gender_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        21\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       0.89      0.93      0.91        27\n",
      "           3       0.89      1.00      0.94         8\n",
      "           4       0.87      0.87      0.87        15\n",
      "           5       0.83      0.91      0.87        11\n",
      "           6       0.96      0.98      0.97       532\n",
      "           7       0.96      0.84      0.90        32\n",
      "           8       1.00      0.84      0.91        19\n",
      "           9       0.93      0.76      0.84        17\n",
      "          10       1.00      0.90      0.95        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       1.00      0.81      0.89        26\n",
      "          13       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.95       750\n",
      "   macro avg       0.95      0.90      0.92       750\n",
      "weighted avg       0.96      0.95      0.95       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_11_accent_pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('clf', SVC(C=100, gamma=0.001, kernel = 'rbf', class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "layer_11_accent_pipe.fit(l11_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_11_accent_pipe.predict(l11_valid_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_accent_preds_confidences['layer_11_speaker_accent_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99, 'layer_8_speaker_accent_pipe': 1.0, 'layer_9_speaker_accent_pipe': 1.0, 'layer_10_speaker_accent_pipe': 1.0, 'layer_11_speaker_accent_pipe': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031138</td>\n",
       "      <td>0.079892</td>\n",
       "      <td>0.157382</td>\n",
       "      <td>-0.014636</td>\n",
       "      <td>-0.051778</td>\n",
       "      <td>-0.021332</td>\n",
       "      <td>-0.073593</td>\n",
       "      <td>-0.005386</td>\n",
       "      <td>-0.212557</td>\n",
       "      <td>0.099683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085248</td>\n",
       "      <td>-0.096007</td>\n",
       "      <td>-0.000766</td>\n",
       "      <td>0.021399</td>\n",
       "      <td>-0.041432</td>\n",
       "      <td>0.094806</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113040</td>\n",
       "      <td>0.175731</td>\n",
       "      <td>0.217741</td>\n",
       "      <td>-0.196254</td>\n",
       "      <td>-0.010129</td>\n",
       "      <td>-0.030586</td>\n",
       "      <td>0.067114</td>\n",
       "      <td>-0.072412</td>\n",
       "      <td>-0.239192</td>\n",
       "      <td>0.104741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090283</td>\n",
       "      <td>-0.053885</td>\n",
       "      <td>-0.010967</td>\n",
       "      <td>0.062209</td>\n",
       "      <td>-0.122958</td>\n",
       "      <td>0.192949</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.048570</td>\n",
       "      <td>0.091281</td>\n",
       "      <td>0.160776</td>\n",
       "      <td>-0.150937</td>\n",
       "      <td>0.020115</td>\n",
       "      <td>0.044117</td>\n",
       "      <td>-0.050092</td>\n",
       "      <td>-0.045661</td>\n",
       "      <td>-0.155332</td>\n",
       "      <td>0.117206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021524</td>\n",
       "      <td>-0.008411</td>\n",
       "      <td>-0.006248</td>\n",
       "      <td>0.031468</td>\n",
       "      <td>-0.056915</td>\n",
       "      <td>0.154731</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.039212</td>\n",
       "      <td>0.118388</td>\n",
       "      <td>0.173831</td>\n",
       "      <td>-0.096659</td>\n",
       "      <td>-0.008702</td>\n",
       "      <td>0.061298</td>\n",
       "      <td>0.008974</td>\n",
       "      <td>-0.003277</td>\n",
       "      <td>-0.065046</td>\n",
       "      <td>0.095480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071936</td>\n",
       "      <td>-0.023120</td>\n",
       "      <td>-0.007812</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>-0.121892</td>\n",
       "      <td>0.072796</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.056019</td>\n",
       "      <td>0.170639</td>\n",
       "      <td>0.157917</td>\n",
       "      <td>-0.228605</td>\n",
       "      <td>-0.065965</td>\n",
       "      <td>-0.088732</td>\n",
       "      <td>-0.082243</td>\n",
       "      <td>-0.080568</td>\n",
       "      <td>-0.341500</td>\n",
       "      <td>0.142430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155621</td>\n",
       "      <td>-0.079447</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>-0.151966</td>\n",
       "      <td>0.169634</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.031138   0.079892   0.157382  -0.014636  -0.051778  -0.021332   \n",
       "1   0.113040   0.175731   0.217741  -0.196254  -0.010129  -0.030586   \n",
       "2   0.048570   0.091281   0.160776  -0.150937   0.020115   0.044117   \n",
       "3   0.039212   0.118388   0.173831  -0.096659  -0.008702   0.061298   \n",
       "4   0.056019   0.170639   0.157917  -0.228605  -0.065965  -0.088732   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0  -0.073593  -0.005386  -0.212557    0.099683  ...    -0.085248    -0.096007   \n",
       "1   0.067114  -0.072412  -0.239192    0.104741  ...    -0.090283    -0.053885   \n",
       "2  -0.050092  -0.045661  -0.155332    0.117206  ...    -0.021524    -0.008411   \n",
       "3   0.008974  -0.003277  -0.065046    0.095480  ...    -0.071936    -0.023120   \n",
       "4  -0.082243  -0.080568  -0.341500    0.142430  ...    -0.155621    -0.079447   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0    -0.000766     0.021399    -0.041432     0.094806       45      NaN   \n",
       "1    -0.010967     0.062209    -0.122958     0.192949       45      NaN   \n",
       "2    -0.006248     0.031468    -0.056915     0.154731       45      NaN   \n",
       "3    -0.007812     0.057600    -0.121892     0.072796       45      NaN   \n",
       "4     0.015316     0.127726    -0.151966     0.169634       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_train_df = pd.read_csv(layer_12_train_csv_file_path)\n",
    "l12_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null/NaN values in all columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_train_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_train_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    4762\n",
       "25.0    2849\n",
       "27.0    2846\n",
       "23.0    2842\n",
       "31.0    2385\n",
       "24.0    1906\n",
       "28.0    1899\n",
       "30.0    1894\n",
       "22.0    1432\n",
       "29.0    1424\n",
       "33.0     945\n",
       "36.0     481\n",
       "35.0     480\n",
       "34.0     478\n",
       "32.0     476\n",
       "41.0     474\n",
       "61.0     467\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l12_train_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  27.975106990014265\n",
      "rounded mean: 28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l12_train_df.label_2.mean()\n",
    "print(\"mean: \", label_2_mean)\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"rounded mean:\", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l12_train_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "l12_train_X = l12_train_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "l12_valid_df = pd.read_csv(layer_12_valid_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null/NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      False\n",
       "feature_2      False\n",
       "feature_3      False\n",
       "feature_4      False\n",
       "feature_5      False\n",
       "               ...  \n",
       "feature_768    False\n",
       "label_1        False\n",
       "label_2         True\n",
       "label_3        False\n",
       "label_4        False\n",
       "Length: 772, dtype: bool"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_valid_df.isna().any()\n",
    "\n",
    "# Based on below output we can see that there are missing values in the speaker_age column of the dataset.\n",
    "# Let's now check whether that is the only column with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_valid_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_valid_df.isnull().sum().sum()\n",
    "\n",
    "# Based on the above and below outputs, we can see that there are 480 missing values 'only' in the speaker_age column. No missing values in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset shape: (750, 772)\n",
      "null values row count:  14\n",
      "null values row count percentage:  1.866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"validation dataset shape:\", l12_valid_df.shape)\n",
    "print(\"null values row count: \", l12_valid_df.isnull().sum().sum())\n",
    "print(\"null values row count percentage: \", (l12_valid_df.isnull().sum().sum() / l12_valid_df.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "30.0     48\n",
       "28.0     46\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now check the distribution of the speaker_age column.\n",
    "l12_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Null values (Replace with Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  28\n"
     ]
    }
   ],
   "source": [
    "# Let's get the mean of the speaker_age column.\n",
    "label_2_mean = l12_valid_df.label_2.mean()\n",
    "\n",
    "# round it to nearest int\n",
    "label_2_mean = round(label_2_mean)\n",
    "print(\"mean: \", label_2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now fill the missing values with the mean value.\n",
    "l12_valid_df.label_2.fillna(label_2_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0    115\n",
       "27.0     81\n",
       "25.0     79\n",
       "23.0     71\n",
       "31.0     65\n",
       "28.0     60\n",
       "30.0     48\n",
       "24.0     46\n",
       "29.0     45\n",
       "22.0     36\n",
       "33.0     30\n",
       "61.0     19\n",
       "41.0     14\n",
       "34.0     11\n",
       "32.0     11\n",
       "35.0     11\n",
       "36.0      8\n",
       "Name: label_2, dtype: int64"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l12_valid_df.label_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "l12_valid_X = l12_valid_df.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.15      0.27        13\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.08      0.08      0.08        12\n",
      "           4       0.08      0.06      0.07        16\n",
      "           5       0.00      0.00      0.00        18\n",
      "           6       0.45      0.56      0.50         9\n",
      "           7       0.40      0.47      0.43        17\n",
      "           8       0.50      0.07      0.12        14\n",
      "           9       0.67      0.55      0.60        11\n",
      "          10       0.60      0.38      0.46         8\n",
      "          11       0.36      0.47      0.41        19\n",
      "          12       0.05      0.43      0.09         7\n",
      "          13       0.50      0.09      0.15        11\n",
      "          14       0.50      0.20      0.29        15\n",
      "          15       0.13      0.29      0.18        17\n",
      "          16       0.67      0.14      0.24        14\n",
      "          17       0.80      0.29      0.42        14\n",
      "          18       0.25      0.05      0.09        19\n",
      "          19       0.25      0.20      0.22        15\n",
      "          20       0.11      0.09      0.10        11\n",
      "          21       0.00      0.00      0.00        14\n",
      "          22       0.40      0.36      0.38        11\n",
      "          23       0.26      0.60      0.36        10\n",
      "          24       0.00      0.00      0.00        12\n",
      "          25       0.67      0.18      0.29        11\n",
      "          26       1.00      0.43      0.60         7\n",
      "          27       0.80      0.25      0.38        16\n",
      "          28       0.47      0.64      0.54        11\n",
      "          29       0.00      0.00      0.00        15\n",
      "          30       0.14      0.25      0.18         8\n",
      "          31       0.50      0.08      0.14        12\n",
      "          32       0.29      0.22      0.25         9\n",
      "          33       0.14      0.50      0.22         6\n",
      "          34       0.50      0.15      0.24        13\n",
      "          35       0.38      0.38      0.38         8\n",
      "          36       0.40      0.11      0.17        18\n",
      "          37       0.17      0.25      0.21        16\n",
      "          38       0.13      0.64      0.22        11\n",
      "          39       0.60      0.23      0.33        13\n",
      "          40       1.00      0.17      0.29        12\n",
      "          41       0.11      0.20      0.14        10\n",
      "          42       0.14      0.50      0.22        12\n",
      "          43       0.33      0.08      0.13        12\n",
      "          44       0.67      0.21      0.32        19\n",
      "          45       0.38      0.21      0.27        14\n",
      "          46       0.10      0.09      0.10        11\n",
      "          47       1.00      0.50      0.67         8\n",
      "          48       1.00      0.06      0.11        17\n",
      "          49       0.29      0.31      0.30        13\n",
      "          50       0.14      0.46      0.22        13\n",
      "          51       0.11      0.12      0.12         8\n",
      "          52       0.18      0.64      0.29        11\n",
      "          53       0.80      0.27      0.40        15\n",
      "          54       0.29      0.22      0.25         9\n",
      "          55       0.50      0.12      0.20         8\n",
      "          56       0.05      0.20      0.08        10\n",
      "          57       1.00      0.56      0.71        18\n",
      "          58       0.54      0.35      0.42        20\n",
      "          59       0.75      0.30      0.43        10\n",
      "          60       0.75      0.30      0.43        10\n",
      "\n",
      "    accuracy                           0.25       750\n",
      "   macro avg       0.41      0.26      0.26       750\n",
      "weighted avg       0.41      0.25      0.26       750\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "best_classifier__weights = 'distance' \n",
    "best_classifier__p = 2 \n",
    "best_classifier__n_neighbors = 7\n",
    "\n",
    "layer_12_speaker_ID_pipe = Pipeline([\n",
    "    ('classifier', KNeighborsClassifier(weights=best_classifier__weights, p=best_classifier__p, n_neighbors=best_classifier__n_neighbors))\n",
    "])\n",
    "\n",
    "layer_12_speaker_ID_pipe.fit(l12_train_X, train_speaker_IDs)\n",
    "report = classification_report(valid_speaker_IDs, layer_12_speaker_ID_pipe.predict(l12_valid_X))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_ID_preds_confidences['layer_12_speaker_ID_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96, 'layer_10_speaker_ID_pipe': 0.97, 'layer_11_speaker_ID_pipe': 0.95, 'layer_12_speaker_ID_pipe': 0.25}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.33      0.31      0.32        36\n",
      "        23.0       0.19      0.25      0.21        71\n",
      "        24.0       0.20      0.46      0.28        46\n",
      "        25.0       0.31      0.14      0.19        79\n",
      "        26.0       0.29      0.30      0.30       115\n",
      "        27.0       0.51      0.37      0.43        81\n",
      "        28.0       0.26      0.43      0.33        60\n",
      "        29.0       0.39      0.40      0.40        45\n",
      "        30.0       0.25      0.15      0.18        48\n",
      "        31.0       0.37      0.17      0.23        65\n",
      "        32.0       0.18      0.55      0.27        11\n",
      "        33.0       0.45      0.43      0.44        30\n",
      "        34.0       0.23      0.45      0.30        11\n",
      "        35.0       0.86      0.55      0.67        11\n",
      "        36.0       1.00      0.25      0.40         8\n",
      "        41.0       0.50      0.07      0.12        14\n",
      "        61.0       0.80      0.21      0.33        19\n",
      "\n",
      "    accuracy                           0.30       750\n",
      "   macro avg       0.42      0.32      0.32       750\n",
      "weighted avg       0.35      0.30      0.30       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_12_speaker_ages_pipe = Pipeline([\n",
    "    # ('scaler', StandardScaler()),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "layer_12_speaker_ages_pipe.fit(l12_train_X, train_speaker_ages)\n",
    "report = classification_report(valid_speaker_ages, layer_12_speaker_ages_pipe.predict(l12_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_age_preds_confidences['layer_12_speaker_age_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94, 'layer_10_speaker_age_pipe': 0.94, 'layer_11_speaker_age_pipe': 0.9, 'layer_12_speaker_age_pipe': 0.3}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.75      0.72       142\n",
      "           1       0.94      0.93      0.93       608\n",
      "\n",
      "    accuracy                           0.89       750\n",
      "   macro avg       0.82      0.84      0.83       750\n",
      "weighted avg       0.89      0.89      0.89       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_classifier__solver = 'liblinear'\n",
    "best_classifier__penalty = 'l2'\n",
    "best_classifier__C = 10\n",
    "\n",
    "layer_12_speaker_gender_pipe = Pipeline([\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', LogisticRegression(class_weight=\"balanced\", solver=best_classifier__solver, penalty=best_classifier__penalty, C=best_classifier__C))\n",
    "    ])\n",
    "\n",
    "layer_12_speaker_gender_pipe.fit(l12_train_X, train_speaker_genders)\n",
    "report = classification_report(valid_speaker_genders, layer_12_speaker_gender_pipe.predict(l12_valid_X))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_gender_preds_confidences['layer_12_speaker_gender_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0, 'layer_8_speaker_gender_pipe': 1.0, 'layer_9_speaker_gender_pipe': 1.0, 'layer_10_speaker_gender_pipe': 1.0, 'layer_11_speaker_gender_pipe': 1.0, 'layer_12_speaker_gender_pipe': 0.89}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For speaker_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.29      0.24        21\n",
      "           1       1.00      0.18      0.31        11\n",
      "           2       0.75      0.11      0.19        27\n",
      "           3       1.00      0.25      0.40         8\n",
      "           4       0.29      0.27      0.28        15\n",
      "           5       0.27      0.27      0.27        11\n",
      "           6       0.75      0.89      0.81       532\n",
      "           7       0.14      0.03      0.05        32\n",
      "           8       0.00      0.00      0.00        19\n",
      "           9       0.30      0.18      0.22        17\n",
      "          10       0.08      0.10      0.09        10\n",
      "          11       0.86      0.55      0.67        11\n",
      "          12       0.44      0.31      0.36        26\n",
      "          13       1.00      0.30      0.46        10\n",
      "\n",
      "    accuracy                           0.69       750\n",
      "   macro avg       0.51      0.27      0.31       750\n",
      "weighted avg       0.65      0.69      0.64       750\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "best_classifier__weights = 'uniform'\n",
    "best_classifier__p = 3\n",
    "best_classifier__n_neighbors = 8\n",
    "\n",
    "layer_12_speaker_accent_pipe = Pipeline([\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('classifier', KNeighborsClassifier(p=best_classifier__p, n_neighbors=best_classifier__n_neighbors, weights=best_classifier__weights))\n",
    "    ])\n",
    "\n",
    "layer_12_speaker_accent_pipe.fit(l12_train_X, train_speaker_accents)\n",
    "print(classification_report(valid_speaker_accents, layer_12_speaker_accent_pipe.predict(l12_valid_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = report.split('\\n')\n",
    "weighted_avg_line = lines[-2]\n",
    "weighted_avg_values = weighted_avg_line.split()\n",
    "f1_score = float(weighted_avg_values[3])\n",
    "speaker_accent_preds_confidences['layer_12_speaker_accent_pipe'] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99, 'layer_8_speaker_accent_pipe': 1.0, 'layer_9_speaker_accent_pipe': 1.0, 'layer_10_speaker_accent_pipe': 1.0, 'layer_11_speaker_accent_pipe': 1.0, 'layer_12_speaker_accent_pipe': 0.89}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for speaker_IDs in validation dataset from each layer model\n",
    "layer_7_speaker_ID_predictions = layer_7_speaker_ID_pipe.predict(l7_valid_X)\n",
    "layer_8_speaker_ID_predictions = layer_8_speaker_ID_pipe.predict(l8_valid_X)\n",
    "layer_9_speaker_ID_predictions = layer_9_speaker_ID_pipe.predict(l9_valid_X)\n",
    "layer_10_speaker_ID_predictions = layer_10_speaker_ID_pipe.predict(l10_valid_X)\n",
    "layer_11_speaker_ID_predictions = layer_11_speaker_ID_pipe.predict(l11_valid_X)\n",
    "layer_12_speaker_ID_predictions = layer_12_speaker_ID_pipe.predict(l12_valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions from all layers into a 2D array\n",
    "all_predictions = np.vstack([\n",
    "    layer_7_speaker_ID_predictions,\n",
    "    layer_8_speaker_ID_predictions,\n",
    "    layer_9_speaker_ID_predictions,\n",
    "    layer_10_speaker_ID_predictions,\n",
    "    layer_11_speaker_ID_predictions,\n",
    "    layer_12_speaker_ID_predictions\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_ID_pipe': 0.99, 'layer_8_speaker_ID_pipe': 0.93, 'layer_9_speaker_ID_pipe': 0.96, 'layer_10_speaker_ID_pipe': 0.97, 'layer_11_speaker_ID_pipe': 0.95, 'layer_12_speaker_ID_pipe': 0.25}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_ID_preds_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750,)\n"
     ]
    }
   ],
   "source": [
    "print(layer_7_speaker_ID_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 750)\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using just the majority prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    # print(predictions, majority_prediction)\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.94      1.00      0.97        16\n",
      "           5       1.00      0.94      0.97        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      0.94      0.97        17\n",
      "           8       1.00      0.93      0.96        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      0.95      0.97        19\n",
      "          12       0.88      1.00      0.93         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      0.88      0.94        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       1.00      1.00      1.00        14\n",
      "          18       1.00      0.95      0.97        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       0.93      1.00      0.97        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.86      1.00      0.92        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       0.94      1.00      0.97        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      0.88      0.93         8\n",
      "          31       0.85      0.92      0.88        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.85      0.92        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       0.86      1.00      0.92        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       0.93      1.00      0.97        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       0.89      1.00      0.94         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       0.91      1.00      0.95        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       0.91      1.00      0.95        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.97      0.98      0.97       750\n",
      "weighted avg       0.98      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 0.93, 0.96, 0.97, 0.95, 0.25])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ID_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 4:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       1.00      1.00      1.00        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      1.00      1.00        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      1.00      0.97        14\n",
      "          18       1.00      1.00      1.00        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       1.00      0.93      0.96        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       1.00      1.00      1.00        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      1.00      1.00        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.93      0.97        15\n",
      "          30       1.00      1.00      1.00         8\n",
      "          31       1.00      1.00      1.00        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       1.00      1.00      1.00         6\n",
      "          34       1.00      1.00      1.00        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       1.00      1.00      1.00        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       1.00      1.00      1.00        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       0.91      1.00      0.95        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       0.99      0.99      0.99       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99, 0.93, 0.96, 0.97, 0.95, 0.25]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(speaker_ID_preds_confidences.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 3:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "           3       1.00      1.00      1.00        12\n",
      "           4       0.89      1.00      0.94        16\n",
      "           5       1.00      0.94      0.97        18\n",
      "           6       1.00      1.00      1.00         9\n",
      "           7       1.00      1.00      1.00        17\n",
      "           8       1.00      1.00      1.00        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       1.00      1.00      1.00        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.93      0.97        15\n",
      "          15       1.00      1.00      1.00        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.93      1.00      0.97        14\n",
      "          18       1.00      1.00      1.00        19\n",
      "          19       0.88      0.93      0.90        15\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       1.00      0.93      0.96        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.91      1.00      0.95        10\n",
      "          24       0.92      1.00      0.96        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      1.00      1.00        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      1.00      1.00        15\n",
      "          30       1.00      1.00      1.00         8\n",
      "          31       0.92      0.92      0.92        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.86      1.00      0.92         6\n",
      "          34       1.00      0.92      0.96        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       1.00      1.00      1.00        18\n",
      "          37       1.00      1.00      1.00        16\n",
      "          38       1.00      1.00      1.00        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       1.00      0.92      0.96        12\n",
      "          41       1.00      1.00      1.00        10\n",
      "          42       1.00      1.00      1.00        12\n",
      "          43       0.92      1.00      0.96        12\n",
      "          44       1.00      0.95      0.97        19\n",
      "          45       1.00      1.00      1.00        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       0.94      0.94      0.94        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      1.00      1.00        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      1.00      1.00        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      1.00      1.00        20\n",
      "          59       1.00      1.00      1.00        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.98       750\n",
      "   macro avg       0.98      0.98      0.98       750\n",
      "weighted avg       0.99      0.98      0.98       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking based ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_7_train_speaker_ID_predictions = layer_7_speaker_ID_pipe.predict(l7_train_X)\n",
    "layer_8_train_speaker_ID_predictions = layer_8_speaker_ID_pipe.predict(l8_train_X)\n",
    "layer_9_train_speaker_ID_predictions = layer_9_speaker_ID_pipe.predict(l9_train_X)\n",
    "layer_10_train_speaker_ID_predictions = layer_10_speaker_ID_pipe.predict(l10_train_X)\n",
    "layer_11_train_speaker_ID_predictions = layer_11_speaker_ID_pipe.predict(l11_train_X)\n",
    "layer_12_train_speaker_ID_predictions = layer_12_speaker_ID_pipe.predict(l12_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_meta = [\n",
    "    layer_7_train_speaker_ID_predictions,\n",
    "    layer_8_train_speaker_ID_predictions,\n",
    "    layer_9_train_speaker_ID_predictions,\n",
    "    layer_10_train_speaker_ID_predictions,\n",
    "    layer_11_train_speaker_ID_predictions,\n",
    "    layer_12_train_speaker_ID_predictions\n",
    "]\n",
    "\n",
    "X_meta = np.vstack(X_meta).T  # Transpose the matrix to have predictions as columns\n",
    "\n",
    "X_train_meta, X_valid_meta, y_train_meta, y_valid_meta = train_test_split(X_meta, train_speaker_IDs, test_size=0.2, random_state=42)\n",
    "\n",
    "meta_model = SVC(C=40, class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28520, 6)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=40, class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=40, class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=40, class_weight='balanced')"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacked Ensemble (using hold-out method): 0.9989481065918654\n"
     ]
    }
   ],
   "source": [
    "meta_predictions = meta_model.predict(X_valid_meta)\n",
    "stacked_ensemble_accuracy = accuracy_score(y_valid_meta, meta_predictions)\n",
    "\n",
    "print(\"Accuracy of Stacked Ensemble (using hold-out method):\", stacked_ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_data_from_validation_dataset = [\n",
    "    layer_7_speaker_ID_predictions,\n",
    "    layer_8_speaker_ID_predictions,\n",
    "    layer_9_speaker_ID_predictions,\n",
    "    layer_10_speaker_ID_predictions,\n",
    "    layer_11_speaker_ID_predictions,\n",
    "    layer_12_speaker_ID_predictions\n",
    "]\n",
    "\n",
    "X_meta_data_from_validation_dataset = np.vstack(X_meta_data_from_validation_dataset).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.15      0.27        13\n",
      "           2       0.40      0.22      0.29         9\n",
      "           3       0.25      0.08      0.12        12\n",
      "           4       0.18      0.12      0.15        16\n",
      "           5       0.00      0.00      0.00        18\n",
      "           6       0.45      0.56      0.50         9\n",
      "           7       0.80      0.47      0.59        17\n",
      "           8       0.11      0.07      0.09        14\n",
      "           9       0.55      0.55      0.55        11\n",
      "          10       0.36      0.50      0.42         8\n",
      "          11       0.69      0.58      0.63        19\n",
      "          12       0.50      0.43      0.46         7\n",
      "          13       0.50      0.36      0.42        11\n",
      "          14       0.09      0.27      0.14        15\n",
      "          15       0.31      0.29      0.30        17\n",
      "          16       0.27      0.21      0.24        14\n",
      "          17       0.50      0.36      0.42        14\n",
      "          18       0.11      0.05      0.07        19\n",
      "          19       0.27      0.20      0.23        15\n",
      "          20       0.08      0.09      0.08        11\n",
      "          21       0.12      0.14      0.13        14\n",
      "          22       0.22      0.36      0.28        11\n",
      "          23       0.24      0.70      0.36        10\n",
      "          24       0.00      0.00      0.00        12\n",
      "          25       0.18      0.18      0.18        11\n",
      "          26       0.21      0.43      0.29         7\n",
      "          27       0.27      0.19      0.22        16\n",
      "          28       0.58      0.64      0.61        11\n",
      "          29       0.00      0.00      0.00        15\n",
      "          30       0.12      0.25      0.16         8\n",
      "          31       0.12      0.17      0.14        12\n",
      "          32       0.20      0.22      0.21         9\n",
      "          33       0.18      0.50      0.26         6\n",
      "          34       0.23      0.23      0.23        13\n",
      "          35       0.17      0.38      0.23         8\n",
      "          36       0.33      0.22      0.27        18\n",
      "          37       0.18      0.25      0.21        16\n",
      "          38       0.43      0.55      0.48        11\n",
      "          39       0.25      0.23      0.24        13\n",
      "          40       0.50      0.33      0.40        12\n",
      "          41       0.14      0.20      0.17        10\n",
      "          42       0.43      0.50      0.46        12\n",
      "          43       0.17      0.25      0.20        12\n",
      "          44       0.43      0.32      0.36        19\n",
      "          45       0.19      0.21      0.20        14\n",
      "          46       0.12      0.18      0.14        11\n",
      "          47       0.40      0.50      0.44         8\n",
      "          48       0.22      0.12      0.15        17\n",
      "          49       0.67      0.31      0.42        13\n",
      "          50       0.50      0.54      0.52        13\n",
      "          51       0.15      0.25      0.19         8\n",
      "          52       0.41      0.64      0.50        11\n",
      "          53       0.62      0.33      0.43        15\n",
      "          54       0.11      0.22      0.15         9\n",
      "          55       0.25      0.12      0.17         8\n",
      "          56       0.22      0.20      0.21        10\n",
      "          57       0.86      0.67      0.75        18\n",
      "          58       0.88      0.35      0.50        20\n",
      "          59       0.75      0.30      0.43        10\n",
      "          60       1.00      0.30      0.46        10\n",
      "\n",
      "    accuracy                           0.29       750\n",
      "   macro avg       0.34      0.30      0.30       750\n",
      "weighted avg       0.35      0.29      0.30       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_IDs, meta_model.predict(X_meta_data_from_validation_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 0.93, 0.96, 0.97, 0.95, 0.25])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ID_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(SVC(probability=True, class_weight=\"balanced\"), param_grid, cv=1, scoring='accuracy', verbose=3, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to your data\n",
    "grid_search.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best__C = PUT THE BEST ONE HERE\n",
    "# best__gamma = PUT THE BEST ONE HERE\n",
    "\n",
    "best_meta_model = SVC(C=best__C, kernel=best__kernel, gamma=best__gamma, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for speaker_IDs in validation dataset from each layer model\n",
    "layer_7_speaker_age_valid_predictions = layer_7_speaker_age_pipe.predict(l7_valid_X)\n",
    "layer_8_speaker_age_valid_predictions = layer_8_age_pipe.predict(l8_valid_X)\n",
    "layer_9_speaker_age_valid_predictions = layer_9_age_pipe.predict(l9_valid_X)\n",
    "layer_10_speaker_age_valid_predictions = layer_10_age_pipe.predict(l10_valid_X)\n",
    "layer_11_speaker_age_valid_predictions = layer_11_age_pipe.predict(l11_valid_X)\n",
    "layer_12_speaker_age_valid_predictions = layer_12_speaker_ages_pipe.predict(l12_valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions from all layers into a 2D array\n",
    "all_predictions = np.vstack([\n",
    "    layer_7_speaker_age_valid_predictions,\n",
    "    layer_8_speaker_age_valid_predictions,\n",
    "    layer_9_speaker_age_valid_predictions,\n",
    "    layer_10_speaker_age_valid_predictions,\n",
    "    layer_11_speaker_age_valid_predictions,\n",
    "    layer_12_speaker_age_valid_predictions\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_age_pipe': 0.97, 'layer_8_speaker_age_pipe': 0.91, 'layer_9_speaker_age_pipe': 0.94, 'layer_10_speaker_age_pipe': 0.94, 'layer_11_speaker_age_pipe': 0.9, 'layer_12_speaker_age_pipe': 0.3}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_age_preds_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750,)\n"
     ]
    }
   ],
   "source": [
    "print(layer_7_speaker_age_valid_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 750)\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using just the majority prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_age_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    # print(predictions, majority_prediction)\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.95      1.00      0.97        36\n",
      "        23.0       0.92      0.96      0.94        71\n",
      "        24.0       0.96      0.98      0.97        46\n",
      "        25.0       1.00      0.94      0.97        79\n",
      "        26.0       0.95      0.99      0.97       115\n",
      "        27.0       0.97      0.96      0.97        81\n",
      "        28.0       0.98      0.95      0.97        60\n",
      "        29.0       1.00      1.00      1.00        45\n",
      "        30.0       1.00      0.98      0.99        48\n",
      "        31.0       0.96      0.98      0.97        65\n",
      "        32.0       1.00      1.00      1.00        11\n",
      "        33.0       1.00      1.00      1.00        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.86      0.92        14\n",
      "        61.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.98      0.97      0.97       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_ages, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.97, 0.91, 0.94, 0.94, 0.9, 0.3])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_age_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_age_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 4:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.95      1.00      0.97        36\n",
      "        23.0       0.96      0.97      0.97        71\n",
      "        24.0       0.96      0.98      0.97        46\n",
      "        25.0       0.99      0.92      0.95        79\n",
      "        26.0       0.95      0.99      0.97       115\n",
      "        27.0       1.00      0.95      0.97        81\n",
      "        28.0       0.97      0.98      0.98        60\n",
      "        29.0       1.00      1.00      1.00        45\n",
      "        30.0       0.98      1.00      0.99        48\n",
      "        31.0       0.95      0.97      0.96        65\n",
      "        32.0       1.00      1.00      1.00        11\n",
      "        33.0       1.00      1.00      1.00        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.93      0.96        14\n",
      "        61.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.98      0.97      0.98       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_ages, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.97, 0.91, 0.94, 0.94, 0.9, 0.3]"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(speaker_age_preds_confidences.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_age_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 3:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       0.95      1.00      0.97        36\n",
      "        23.0       0.92      0.97      0.95        71\n",
      "        24.0       0.96      0.98      0.97        46\n",
      "        25.0       1.00      0.94      0.97        79\n",
      "        26.0       0.96      0.99      0.97       115\n",
      "        27.0       0.99      0.96      0.97        81\n",
      "        28.0       0.98      0.97      0.97        60\n",
      "        29.0       1.00      1.00      1.00        45\n",
      "        30.0       1.00      0.98      0.99        48\n",
      "        31.0       0.96      0.98      0.97        65\n",
      "        32.0       1.00      1.00      1.00        11\n",
      "        33.0       1.00      1.00      1.00        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.91      0.95        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.86      0.92        14\n",
      "        61.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.98      0.97      0.97       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_ages, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking based ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_7_train_speaker_age_predictions = layer_7_speaker_age_pipe.predict(l7_train_X)\n",
    "layer_8_train_speaker_age_predictions = layer_8_age_pipe.predict(l8_train_X)\n",
    "layer_9_train_speaker_age_predictions = layer_9_age_pipe.predict(l9_train_X)\n",
    "layer_10_train_speaker_age_predictions = layer_10_age_pipe.predict(l10_train_X)\n",
    "layer_11_train_speaker_age_predictions = layer_11_age_pipe.predict(l11_train_X)\n",
    "layer_12_train_speaker_age_predictions = layer_12_speaker_ages_pipe.predict(l12_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_meta shape: (28520, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_meta = [\n",
    "    layer_7_train_speaker_age_predictions,\n",
    "    layer_8_train_speaker_age_predictions,\n",
    "    layer_9_train_speaker_age_predictions,\n",
    "    layer_10_train_speaker_age_predictions,\n",
    "    layer_11_train_speaker_age_predictions,\n",
    "    layer_12_train_speaker_age_predictions\n",
    "]\n",
    "\n",
    "X_meta = np.vstack(X_meta).T  # Transpose the matrix to have predictions as columns\n",
    "print(\"X_meta shape:\", X_meta.shape)\n",
    "\n",
    "X_train_meta, X_valid_meta, y_train_meta, y_valid_meta = train_test_split(X_meta, train_speaker_ages, test_size=0.2, random_state=42)\n",
    "\n",
    "meta_model = SVC(class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(class_weight='balanced')"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacked Ensemble (using hold-out method): 0.9980715287517532\n"
     ]
    }
   ],
   "source": [
    "meta_predictions = meta_model.predict(X_valid_meta)\n",
    "stacked_ensemble_accuracy = accuracy_score(y_valid_meta, meta_predictions)\n",
    "\n",
    "print(\"Accuracy of Stacked Ensemble (using hold-out method):\", stacked_ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_data_from_validation_dataset = [\n",
    "    layer_7_speaker_age_valid_predictions,\n",
    "    layer_8_speaker_age_valid_predictions,\n",
    "    layer_9_speaker_age_valid_predictions,\n",
    "    layer_10_speaker_age_valid_predictions,\n",
    "    layer_11_speaker_age_valid_predictions,\n",
    "    layer_12_speaker_age_valid_predictions\n",
    "]\n",
    "\n",
    "X_meta_data_from_validation_dataset = np.vstack(X_meta_data_from_validation_dataset).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        22.0       1.00      0.83      0.91        36\n",
      "        23.0       0.90      0.90      0.90        71\n",
      "        24.0       0.84      0.91      0.87        46\n",
      "        25.0       0.87      0.86      0.87        79\n",
      "        26.0       0.92      0.95      0.93       115\n",
      "        27.0       0.84      0.84      0.84        81\n",
      "        28.0       0.76      0.87      0.81        60\n",
      "        29.0       0.80      0.89      0.84        45\n",
      "        30.0       0.91      0.88      0.89        48\n",
      "        31.0       0.97      0.91      0.94        65\n",
      "        32.0       0.73      0.73      0.73        11\n",
      "        33.0       0.93      0.87      0.90        30\n",
      "        34.0       1.00      0.91      0.95        11\n",
      "        35.0       1.00      0.73      0.84        11\n",
      "        36.0       1.00      1.00      1.00         8\n",
      "        41.0       1.00      0.71      0.83        14\n",
      "        61.0       0.90      1.00      0.95        19\n",
      "\n",
      "    accuracy                           0.88       750\n",
      "   macro avg       0.90      0.87      0.88       750\n",
      "weighted avg       0.89      0.88      0.88       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_ages, meta_model.predict(X_meta_data_from_validation_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 0.93, 0.96, 0.97, 0.95, 0.25])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ID_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(probability=True, class_weight=\"balanced\"), param_grid, cv=3, scoring='accuracy', verbose=3, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best__C = \n",
    "best__gamma =\n",
    "\n",
    "best_meta_model = SVC(C=best__C, kernel=best__kernel, gamma=best__gamma, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for speaker_IDs in validation dataset from each layer model\n",
    "layer_7_speaker_gender_valid_predictions = layer_7_speaker_gender_pipe.predict(l7_valid_X)\n",
    "layer_8_speaker_gender_valid_predictions = layer_8_gender_pipe.predict(l8_valid_X)\n",
    "layer_9_speaker_gender_valid_predictions = layer_9_gender_pipe.predict(l9_valid_X)\n",
    "layer_10_speaker_gender_valid_predictions = layer_10_gender_pipe.predict(l10_valid_X)\n",
    "layer_11_speaker_gender_valid_predictions = layer_11_gender_pipe.predict(l11_valid_X)\n",
    "layer_12_speaker_gender_valid_predictions = layer_12_speaker_gender_pipe.predict(l12_valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions from all layers into a 2D array\n",
    "all_predictions = np.vstack([\n",
    "    layer_7_speaker_gender_valid_predictions,\n",
    "    layer_8_speaker_gender_valid_predictions,\n",
    "    layer_9_speaker_gender_valid_predictions,\n",
    "    layer_10_speaker_gender_valid_predictions,\n",
    "    layer_11_speaker_gender_valid_predictions,\n",
    "    layer_12_speaker_gender_valid_predictions\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_gender_pipe': 1.0, 'layer_8_speaker_gender_pipe': 1.0, 'layer_9_speaker_gender_pipe': 1.0, 'layer_10_speaker_gender_pipe': 1.0, 'layer_11_speaker_gender_pipe': 1.0, 'layer_12_speaker_gender_pipe': 0.89}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_gender_preds_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750,)\n"
     ]
    }
   ],
   "source": [
    "print(layer_7_speaker_gender_valid_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 750)\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using just the majority prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_gender_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    # print(predictions, majority_prediction)\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      1.00      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_genders, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1.0, 1.0, 1.0, 1.0, 1.0, 0.89])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_gender_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_gender_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 4:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      1.00      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_genders, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 0.89]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(speaker_gender_preds_confidences.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_gender_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 3:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      1.00      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_genders, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking based ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_7_train_speaker_gender_predictions = layer_7_speaker_gender_pipe.predict(l7_train_X)\n",
    "layer_8_train_speaker_gender_predictions = layer_8_gender_pipe.predict(l8_train_X)\n",
    "layer_9_train_speaker_gender_predictions = layer_9_gender_pipe.predict(l9_train_X)\n",
    "layer_10_train_speaker_gender_predictions = layer_10_gender_pipe.predict(l10_train_X)\n",
    "layer_11_train_speaker_gender_predictions = layer_11_gender_pipe.predict(l11_train_X)\n",
    "layer_12_train_speaker_gender_predictions = layer_12_speaker_gender_pipe.predict(l12_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_meta shape: (28520, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_meta = [\n",
    "    layer_7_train_speaker_gender_predictions,\n",
    "    layer_8_train_speaker_gender_predictions,\n",
    "    layer_9_train_speaker_gender_predictions,\n",
    "    layer_10_train_speaker_gender_predictions,\n",
    "    layer_11_train_speaker_gender_predictions,\n",
    "    layer_12_train_speaker_gender_predictions\n",
    "]\n",
    "\n",
    "X_meta = np.vstack(X_meta).T  # Transpose the matrix to have predictions as columns\n",
    "print(\"X_meta shape:\", X_meta.shape)\n",
    "\n",
    "X_train_meta, X_valid_meta, y_train_meta, y_valid_meta = train_test_split(X_meta, train_speaker_genders, test_size=0.2, random_state=42)\n",
    "\n",
    "meta_model = SVC(class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(class_weight='balanced')"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacked Ensemble (using hold-out method): 1.0\n"
     ]
    }
   ],
   "source": [
    "meta_predictions = meta_model.predict(X_valid_meta)\n",
    "stacked_ensemble_accuracy = accuracy_score(y_valid_meta, meta_predictions)\n",
    "\n",
    "print(\"Accuracy of Stacked Ensemble (using hold-out method):\", stacked_ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_data_from_validation_dataset = [\n",
    "    layer_7_speaker_gender_valid_predictions,\n",
    "    layer_8_speaker_gender_valid_predictions,\n",
    "    layer_9_speaker_gender_valid_predictions,\n",
    "    layer_10_speaker_gender_valid_predictions,\n",
    "    layer_11_speaker_gender_valid_predictions,\n",
    "    layer_12_speaker_gender_valid_predictions\n",
    "]\n",
    "\n",
    "X_meta_data_from_validation_dataset = np.vstack(X_meta_data_from_validation_dataset).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       1.00      0.99      1.00       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_genders, meta_model.predict(X_meta_data_from_validation_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1.0, 1.0, 1.0, 1.0, 1.0, 0.89])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_gender_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(probability=True, class_weight=\"balanced\"), param_grid, cv=3, scoring='accuracy', verbose=3, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best__C = \n",
    "best__gamma =\n",
    "\n",
    "best_meta_model = SVC(C=best__C, kernel=best__kernel, gamma=best__gamma, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for speaker_IDs in validation dataset from each layer model\n",
    "layer_7_speaker_accent_valid_predictions = layer_7_speaker_accent_pipe.predict(l7_valid_X)\n",
    "layer_8_speaker_accent_valid_predictions = layer_8_accent_pipe.predict(l8_valid_X)\n",
    "layer_9_speaker_accent_valid_predictions = layer_9_accent_pipe.predict(l9_valid_X)\n",
    "layer_10_speaker_accent_valid_predictions = layer_10_accent_pipe.predict(l10_valid_X)\n",
    "layer_11_speaker_accent_valid_predictions = layer_11_accent_pipe.predict(l11_valid_X)\n",
    "layer_12_speaker_accent_valid_predictions = layer_12_speaker_accent_pipe.predict(l12_valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions from all layers into a 2D array\n",
    "all_predictions = np.vstack([\n",
    "    layer_7_speaker_accent_valid_predictions,\n",
    "    layer_8_speaker_accent_valid_predictions,\n",
    "    layer_9_speaker_accent_valid_predictions,\n",
    "    layer_10_speaker_accent_valid_predictions,\n",
    "    layer_11_speaker_accent_valid_predictions,\n",
    "    layer_12_speaker_accent_valid_predictions\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_7_speaker_accent_pipe': 0.99, 'layer_8_speaker_accent_pipe': 1.0, 'layer_9_speaker_accent_pipe': 1.0, 'layer_10_speaker_accent_pipe': 1.0, 'layer_11_speaker_accent_pipe': 1.0, 'layer_12_speaker_accent_pipe': 0.89}\n"
     ]
    }
   ],
   "source": [
    "print(speaker_accent_preds_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750,)\n"
     ]
    }
   ],
   "source": [
    "print(layer_7_speaker_accent_valid_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 750)\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using just the majority prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_accent_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    # print(predictions, majority_prediction)\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        21\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       1.00      1.00      1.00        27\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       1.00      0.93      0.97        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.98      1.00      0.99       532\n",
      "           7       1.00      0.91      0.95        32\n",
      "           8       1.00      0.95      0.97        19\n",
      "           9       1.00      0.82      0.90        17\n",
      "          10       1.00      1.00      1.00        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       1.00      0.96      0.98        26\n",
      "          13       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.98       750\n",
      "   macro avg       1.00      0.95      0.97       750\n",
      "weighted avg       0.98      0.98      0.98       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_accents, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 1.0, 1.0, 1.0, 1.0, 0.89])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_accent_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_accent_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 4:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        21\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       0.96      0.96      0.96        27\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       1.00      0.87      0.93        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.97      1.00      0.98       532\n",
      "           7       0.96      0.84      0.90        32\n",
      "           8       1.00      0.89      0.94        19\n",
      "           9       1.00      0.82      0.90        17\n",
      "          10       1.00      1.00      1.00        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       0.96      0.92      0.94        26\n",
      "          13       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.97       750\n",
      "   macro avg       0.99      0.93      0.96       750\n",
      "weighted avg       0.97      0.97      0.97       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_accents, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99, 1.0, 1.0, 1.0, 1.0, 0.89]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(speaker_accent_preds_confidences.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using majority voting with a thresold of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predictions = []\n",
    "# List of confidence values (weighted f1 scores) for each model\n",
    "confidences = list(speaker_accent_preds_confidences.values())\n",
    "\n",
    "# Iterate through each data point \n",
    "for i in range(all_predictions.shape[1]):\n",
    "    layer_7_pred = all_predictions[0][i]\n",
    "    layer_8_pred = all_predictions[1][i]\n",
    "    layer_9_pred = all_predictions[2][i]\n",
    "    layer_10_pred = all_predictions[3][i]\n",
    "    layer_11_pred = all_predictions[4][i]\n",
    "    layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "    predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "    majority_prediction = mode(predictions)\n",
    "    if predictions.count(majority_prediction) < 3:\n",
    "        # get max confidence value\n",
    "        max_confidence = max(confidences)\n",
    "        # get index of max confidence value\n",
    "        max_confidence_index = confidences.index(max_confidence)\n",
    "        # get prediction from that index\n",
    "        majority_prediction = predictions[max_confidence_index]\n",
    "\n",
    "    ensemble_predictions.append(majority_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        21\n",
      "           1       1.00      0.91      0.95        11\n",
      "           2       1.00      1.00      1.00        27\n",
      "           3       1.00      1.00      1.00         8\n",
      "           4       1.00      0.93      0.97        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.98      1.00      0.99       532\n",
      "           7       1.00      0.91      0.95        32\n",
      "           8       1.00      0.95      0.97        19\n",
      "           9       1.00      0.82      0.90        17\n",
      "          10       1.00      1.00      1.00        10\n",
      "          11       1.00      0.91      0.95        11\n",
      "          12       1.00      0.96      0.98        26\n",
      "          13       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.98       750\n",
      "   macro avg       1.00      0.95      0.97       750\n",
      "weighted avg       0.98      0.98      0.98       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_accents, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking based ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_7_train_speaker_accent_predictions = layer_7_speaker_accent_pipe.predict(l7_train_X)\n",
    "layer_8_train_speaker_accent_predictions = layer_8_accent_pipe.predict(l8_train_X)\n",
    "layer_9_train_speaker_accent_predictions = layer_9_accent_pipe.predict(l9_train_X)\n",
    "layer_10_train_speaker_accent_predictions = layer_10_accent_pipe.predict(l10_train_X)\n",
    "layer_11_train_speaker_accent_predictions = layer_11_accent_pipe.predict(l11_train_X)\n",
    "layer_12_train_speaker_accent_predictions = layer_12_speaker_accent_pipe.predict(l12_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_meta shape: (28520, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_meta = [\n",
    "    layer_7_train_speaker_accent_predictions,\n",
    "    layer_8_train_speaker_accent_predictions,\n",
    "    layer_9_train_speaker_accent_predictions,\n",
    "    layer_10_train_speaker_accent_predictions,\n",
    "    layer_11_train_speaker_accent_predictions,\n",
    "    layer_12_train_speaker_accent_predictions\n",
    "]\n",
    "\n",
    "X_meta = np.vstack(X_meta).T  # Transpose the matrix to have predictions as columns\n",
    "print(\"X_meta shape:\", X_meta.shape)\n",
    "\n",
    "X_train_meta, X_valid_meta, y_train_meta, y_valid_meta = train_test_split(X_meta, train_speaker_accents, test_size=0.2, random_state=42)\n",
    "\n",
    "meta_model = SVC(class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(class_weight='balanced')"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacked Ensemble (using hold-out method): 0.9975455820476858\n"
     ]
    }
   ],
   "source": [
    "meta_predictions = meta_model.predict(X_valid_meta)\n",
    "stacked_ensemble_accuracy = accuracy_score(y_valid_meta, meta_predictions)\n",
    "\n",
    "print(\"Accuracy of Stacked Ensemble (using hold-out method):\", stacked_ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_data_from_validation_dataset = [\n",
    "    layer_7_speaker_accent_valid_predictions,\n",
    "    layer_8_speaker_accent_valid_predictions,\n",
    "    layer_9_speaker_accent_valid_predictions,\n",
    "    layer_10_speaker_accent_valid_predictions,\n",
    "    layer_11_speaker_accent_valid_predictions,\n",
    "    layer_12_speaker_accent_valid_predictions\n",
    "]\n",
    "\n",
    "X_meta_data_from_validation_dataset = np.vstack(X_meta_data_from_validation_dataset).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        21\n",
      "           1       1.00      0.82      0.90        11\n",
      "           2       0.96      0.93      0.94        27\n",
      "           3       0.88      0.88      0.88         8\n",
      "           4       0.86      0.80      0.83        15\n",
      "           5       0.91      0.91      0.91        11\n",
      "           6       0.97      1.00      0.98       532\n",
      "           7       0.94      0.91      0.92        32\n",
      "           8       0.89      0.89      0.89        19\n",
      "           9       0.93      0.76      0.84        17\n",
      "          10       0.89      0.80      0.84        10\n",
      "          11       0.90      0.82      0.86        11\n",
      "          12       0.91      0.77      0.83        26\n",
      "          13       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.96       750\n",
      "   macro avg       0.93      0.86      0.89       750\n",
      "weighted avg       0.96      0.96      0.96       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_speaker_accents, meta_model.predict(X_meta_data_from_validation_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.99, 0.93, 0.96, 0.97, 0.95, 0.25])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_ID_preds_confidences.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\ACA semester 7\\CS4622 - Machine Learning\\ML-Project\\Combining_all_layers\\Combined_all_layers.ipynb Cell 386\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ACA%20semester%207/CS4622%20-%20Machine%20Learning/ML-Project/Combining_all_layers/Combined_all_layers.ipynb#Z1116sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m param_grid \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ACA%20semester%207/CS4622%20-%20Machine%20Learning/ML-Project/Combining_all_layers/Combined_all_layers.ipynb#Z1116sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m100\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ACA%20semester%207/CS4622%20-%20Machine%20Learning/ML-Project/Combining_all_layers/Combined_all_layers.ipynb#Z1116sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mrbf\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpoly\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ACA%20semester%207/CS4622%20-%20Machine%20Learning/ML-Project/Combining_all_layers/Combined_all_layers.ipynb#Z1116sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ACA%20semester%207/CS4622%20-%20Machine%20Learning/ML-Project/Combining_all_layers/Combined_all_layers.ipynb#Z1116sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m }\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ACA%20semester%207/CS4622%20-%20Machine%20Learning/ML-Project/Combining_all_layers/Combined_all_layers.ipynb#Z1116sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(SVC(probability\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, class_weight\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m), param_grid, cv\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/ACA%20semester%207/CS4622%20-%20Machine%20Learning/ML-Project/Combining_all_layers/Combined_all_layers.ipynb#Z1116sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train_meta, y_train_meta)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [1, 10, 100],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "    'gamma': [1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(class_weight=\"balanced\"), param_grid, cv=2, scoring='accuracy', verbose=4, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best__C = \n",
    "best__gamma =\n",
    "\n",
    "best_meta_model = SVC(C=best__C, kernel=best__kernel, gamma=best__gamma, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble_predictions = []\n",
    "# # List of confidence values (weighted f1 scores) for each model\n",
    "# confidences = list(speaker_ID_preds_confidences.values())\n",
    "\n",
    "# # Iterate through each data point \n",
    "# for i in range(all_predictions.shape[1]):\n",
    "#     layer_7_pred = all_predictions[0][i]\n",
    "#     layer_8_pred = all_predictions[1][i]\n",
    "#     layer_9_pred = all_predictions[2][i]\n",
    "#     layer_10_pred = all_predictions[3][i]\n",
    "#     layer_11_pred = all_predictions[4][i]\n",
    "#     layer_12_pred = all_predictions[5][i]\n",
    "\n",
    "#     predictions = [layer_7_pred, layer_8_pred, layer_9_pred, layer_10_pred, layer_11_pred, layer_12_pred]\n",
    "\n",
    "#     total = sum(confidences)\n",
    "#     normalized_confidences = [value / total for value in confidences]\n",
    "    \n",
    "#     weighted_mean = sum(round(prediction * n_confidence) for prediction, n_confidence in zip(predictions, normalized_confidences)) / sum(normalized_confidences)\n",
    "\n",
    "#     print(predictions, weighted_mean, valid_speaker_IDs[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
